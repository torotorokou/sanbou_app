"""Gamma Recency Model: 東京都産業廃棄物中間処理業者の月次搬入量予測

This module implements a fully working Gamma Recency forecasting model as specified.
It includes:
- Daily-to-monthly preprocessing
- GammaRecencyModel class with point prediction and gamma-based intervals
- Evaluation utilities (MAE, RMSE, MAPE, R2)
- Time series cross validation using TimeSeriesSplit
- Future forecasting helper
- Comprehensive plotting utilities
- Main entry point that runs the full workflow and saves results

Author: Auto-generated by Copilot
"""
from __future__ import annotations

import os
import json
from datetime import datetime
import warnings
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, List
import yaml

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from scipy import stats
from scipy.stats import gamma, kstest
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import RidgeCV, ElasticNetCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

try:
    from lightgbm import LGBMRegressor  # type: ignore
except Exception:  # noqa: BLE001
    LGBMRegressor = None  # type: ignore

try:
    from catboost import CatBoostRegressor  # type: ignore
except Exception:  # noqa: BLE001
    CatBoostRegressor = None  # type: ignore

try:
    import joblib  # type: ignore
except Exception:  # noqa: BLE001
    joblib = None  # type: ignore

warnings.filterwarnings("ignore")

# =============================
# 必須定数（コピペ用）
# =============================
# 季節性因子（月別）
SEASONAL_FACTORS: Dict[int, float] = {
    1: 0.8844,
    2: 0.9521,
    3: 1.2221,
    4: 1.0524,
    5: 0.9307,
    6: 0.9580,
    7: 0.9716,
    8: 0.9279,
    9: 0.9923,
    10: 1.0232,
    11: 0.9783,
    12: 1.0933,
}

# 東京都マーケットサイズ（年間、単位: トン）
TOKYO_MARKET_ANNUAL_TON: Dict[int, int] = {
    2021: 25_000_000,
    2022: 25_500_000,
    2023: 26_000_000,
    2024: 26_500_000,
    2025: 26_500_000,
    2026: 26_500_000,  # 推定値
}

# モデルパラメータ
GAMMA_SHAPE: float = 107.40
GAMMA_SCALE: float = 21.27
SHARE_MEAN_PERCENT: float = 0.1056
SHARE_MEAN_RATIO: float = SHARE_MEAN_PERCENT / 100.0
OVERALL_MEAN_TON: float = 2284.69
RECENCY_WEIGHT: float = 0.3

# 出力ディレクトリ（作成されます）
HERE = os.path.dirname(os.path.abspath(__file__))
# submission_release_20251027 を基準にする: /works/data/submission_release_20251027
SUBMISSION_ROOT = os.path.dirname(os.path.dirname(HERE))
OUTPUT_DIR = os.path.join(SUBMISSION_ROOT, "data", "output", "gamma_recency_model")
PLOTS_DIR = OUTPUT_DIR  # 同一フォルダに保存
os.makedirs(OUTPUT_DIR, exist_ok=True)


def _safe_read_csv(filepath: str) -> pd.DataFrame:
    """Try reading CSV with common encodings.

    Args:
        filepath: Path to CSV
    Returns:
        DataFrame
    Raises:
        FileNotFoundError: If file not found
        ValueError: If file cannot be parsed
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"ファイルが見つかりません: {filepath}")
    encodings = ["utf-8", "utf-8-sig", "cp932", "shift_jis"]
    last_err: Optional[Exception] = None
    for enc in encodings:
        try:
            return pd.read_csv(filepath, encoding=enc)
        except Exception as e:  # noqa: BLE001
            last_err = e
            continue
    raise ValueError(f"CSV読込に失敗: {filepath}: {last_err}")


def _year_to_market_annual_ton(year: int) -> float:
    """Get annual market size (ton) for a given year with simple extrapolation.

    If the year is beyond provided keys, extrapolate by +500,000 per year from the last key.
    If the year is before the smallest key, use the smallest key's value.
    """
    if year in TOKYO_MARKET_ANNUAL_TON:
        return float(TOKYO_MARKET_ANNUAL_TON[year])
    keys = sorted(TOKYO_MARKET_ANNUAL_TON.keys())
    min_k, max_k = keys[0], keys[-1]
    if year < min_k:
        return float(TOKYO_MARKET_ANNUAL_TON[min_k])
    # extrapolate beyond max year
    diff = year - max_k
    return float(TOKYO_MARKET_ANNUAL_TON[max_k] + 500_000 * diff)


def preprocess_daily_to_monthly(filepath: str) -> pd.DataFrame:
    """日次データを月次データに変換

    Args:
        filepath: CSVファイルパス (columns: 伝票日付, weight_t)

    Returns:
        月次データフレーム (columns: year_month, year, month, actual_weight_ton,
                          market_size_ton, recency_7)

    処理内容:
    - 日付をdatetime型に変換
    - 月次で集計
    - kg単位をトン単位に変換 (÷1000)
    - 年・月カラムを追加
    - マーケットサイズを付与
    - リーセンシー（7ヶ月移動平均）を計算
    """
    df = _safe_read_csv(filepath)

    # 標準化された列名の確認・整備
    expected_date_col = "伝票日付"
    expected_weight_col = "weight_t"
    # 別名対応
    if expected_date_col not in df.columns:
        # try English fallback
        for cand in ["date", "伝票日", "伝票", "bill_date"]:
            if cand in df.columns:
                expected_date_col = cand
                break
    if expected_weight_col not in df.columns:
        for cand in ["weight", "重量", "kg", "weight_kg"]:
            if cand in df.columns:
                expected_weight_col = cand
                break
    if expected_date_col not in df.columns or expected_weight_col not in df.columns:
        raise KeyError("必要な列が見つかりません。必要: ['伝票日付', 'weight_t']")

    # 型変換
    df[expected_date_col] = pd.to_datetime(df[expected_date_col])

    # 月初日に正規化
    df["year_month"] = df[expected_date_col].values.astype("datetime64[M]")

    # 月次集計（kg→トン）
    monthly = (
        df.groupby("year_month")[expected_weight_col]
        .sum()
        .rename("actual_weight_kg")
        .to_frame()
        .reset_index()
    )
    monthly["actual_weight_ton"] = monthly["actual_weight_kg"] / 1000.0
    monthly.drop(columns=["actual_weight_kg"], inplace=True)

    # 年・月
    monthly["year"] = monthly["year_month"].dt.year.astype(int)
    monthly["month"] = monthly["year_month"].dt.month.astype(int)

    # マーケットサイズ（月次）
    monthly["market_size_ton"] = monthly["year"].apply(_year_to_market_annual_ton) / 12.0

    # リーセンシー（直近7ヶ月の移動平均、min_periods=1）
    monthly.sort_values("year_month", inplace=True)
    monthly["recency_7"] = (
        monthly["actual_weight_ton"].rolling(window=7, min_periods=1).mean()
    )

    # 営業日/週末日のカウントと比率（祝日対応: jpholiday があれば祝日を除外）
    import calendar as _pycal
    try:
        import jpholiday as _jph
        _HAS_JPHOLIDAY_F = True
    except Exception:
        _HAS_JPHOLIDAY_F = False
    try:
        import jpholiday as _jph
        _HAS_JPHOLIDAY = True
    except Exception:
        _HAS_JPHOLIDAY = False

    def _biz_weekend_ratio(y: int, m: int) -> Tuple[int, int, int, float]:
        ndays = _pycal.monthrange(y, m)[1]
        start = pd.Timestamp(year=y, month=m, day=1)
        days = pd.date_range(start=start, periods=ndays, freq="D")
        biz = 0
        for d in days:
            dow = int(d.dayofweek)  # 0=Mon .. 6=Sun
            is_weekday = dow < 5
            if _HAS_JPHOLIDAY and is_weekday:
                if _jph.is_holiday(d.to_pydatetime().date()):
                    continue  # 祝日は除外
            if is_weekday:
                biz += 1
        business = int(biz)
        weekend = int(ndays - business)
        ratio = float(business / ndays) if ndays > 0 else 0.0
        return ndays, business, weekend, ratio

    stats_list = monthly[["year", "month"]].apply(lambda r: _biz_weekend_ratio(int(r.year), int(r.month)), axis=1)
    monthly["month_days"] = [t[0] for t in stats_list]
    monthly["business_days"] = [t[1] for t in stats_list]
    monthly["weekend_days"] = [t[2] for t in stats_list]
    monthly["business_ratio"] = [t[3] for t in stats_list]

    # year_month 表記を Period/str に
    monthly["year_month_str"] = monthly["year_month"].dt.strftime("%Y-%m")

    # 大型連休の月ダミー
    monthly["is_gw_month"] = (monthly["month"] == 5).astype(int)
    monthly["is_obon_month"] = (monthly["month"] == 8).astype(int)
    monthly["is_yearend_month"] = ((monthly["month"] == 12) | (monthly["month"] == 1)).astype(int)

    cols = [
        "year_month",
        "year_month_str",
        "year",
        "month",
        "actual_weight_ton",
        "market_size_ton",
        "recency_7",
        "month_days",
        "business_days",
        "weekend_days",
        "business_ratio",
        "is_gw_month",
        "is_obon_month",
        "is_yearend_month",
    ]
    monthly = monthly[cols]
    return monthly


class GammaRecencyModel:
    """ガンマリーセンシー予測モデル

    マーケットサイズとシェアに基づく月次搬入量予測モデル。
    季節性とリーセンシー（最近のトレンド）を考慮。

    Attributes:
        share_mean (float): 平均マーケットシェア（比率、例: 0.001056）
        gamma_shape (float): ガンマ分布のshapeパラメータ（α）
        gamma_scale (float): ガンマ分布のscaleパラメータ（θ）
        seasonal_factors (dict): 月別季節性因子の辞書
        recency_weight (float): リーセンシー調整の重み（デフォルト0.3）
        overall_mean (float): 全体平均（トン）
    """

    def __init__(
        self,
        share_mean_percent: float,
        gamma_shape: float,
        gamma_scale: float,
        recency_weight: float = 0.3,
        learn_seasonality: bool = False,
        calibr_clip: float = 0.3,
        business_beta: float = 0.0,
        calibr_method: str = "huber",  # 'median'|'huber'|'lad'
        beta_gw: float = 0.0,
        beta_obon: float = 0.0,
        beta_yearend: float = 0.0,
    ) -> None:
        """モデル初期化

        Args:
            share_mean_percent: マーケットシェア（パーセント単位、例: 0.1056）
            gamma_shape: ガンマ分布のshape
            gamma_scale: ガンマ分布のscale
            recency_weight: リーセンシー調整の重み（0.0〜1.0）
            learn_seasonality: 季節性を学習データから再推定するか
            calibr_clip: calibration scale clip (±), e.g., 0.3 -> [0.7,1.3]
        """
        if share_mean_percent <= 0:
            raise ValueError("share_mean_percent must be positive")
        if gamma_shape <= 0 or gamma_scale <= 0:
            raise ValueError("gamma parameters must be positive")
        if not (0.0 <= recency_weight <= 1.0):
            raise ValueError("recency_weight must be in [0, 1]")

        self.share_mean: float = share_mean_percent / 100.0
        self.gamma_shape: float = gamma_shape
        self.gamma_scale: float = gamma_scale
        self.recency_weight: float = recency_weight
        self.learn_seasonality: bool = learn_seasonality
        self.business_beta: float = float(business_beta)
        # 初期は与えられた定数を利用、fitで必要なら上書き
        self.seasonal_factors: Dict[int, float] = dict(SEASONAL_FACTORS)
        self.overall_mean: float = OVERALL_MEAN_TON
        # 学習時のスケーリング（単一パラメータ）
        self.calibr_scale: float = 1.0
        # calibr_scale の上下クリップ幅（±）例: 0.3 -> [0.7, 1.3]
        self.calibr_clip: float = float(calibr_clip)
        # 営業日比率の学習平均
        self.business_mean_ratio: float = 0.0
        # キャリブレーション手法
        self.calibr_method: str = str(calibr_method).lower()
        # 大型連休月の補正係数
        self.beta_gw: float = float(beta_gw)
        self.beta_obon: float = float(beta_obon)
        self.beta_yearend: float = float(beta_yearend)

    def fit(self, df: pd.DataFrame) -> None:
        """学習データから季節性因子を学習

        Args:
            df: 学習データ (必須カラム: month, actual_weight_ton)

        処理内容:
        - 月別の平均搬入量を計算
        - 全体平均搬入量を計算
        - 季節性因子 = 月別平均 / 全体平均
        """
        required_cols = {"month", "actual_weight_ton"}
        if not required_cols.issubset(df.columns):
            missing = required_cols - set(df.columns)
            raise KeyError(f"fit用の必須カラムが不足: {missing}")

        # overall_meanは学習データから更新
        overall = float(df["actual_weight_ton"].mean())
        self.overall_mean = overall if np.isfinite(overall) and overall > 0 else OVERALL_MEAN_TON

        # 季節性学習（オプション）
        if self.learn_seasonality:
            month_mean = df.groupby("month")["actual_weight_ton"].mean()
            learned: Dict[int, float] = {}
            for m in range(1, 13):
                if m in month_mean.index and self.overall_mean > 0:
                    learned[m] = float(month_mean[m] / self.overall_mean)
                else:
                    learned[m] = self.seasonal_factors.get(m, 1.0)
            # 正規化（平均がほぼ1になるように調整）
            mean_factor = float(np.mean(list(learned.values())))
            if mean_factor > 0:
                learned = {k: v / mean_factor for k, v in learned.items()}
            self.seasonal_factors = learned

        # スケーリングの一括キャリブレーション（train上の予測→実測への倍率）
        try:
            X_cols = ["month", "market_size_ton", "recency_7", "business_ratio"]
            use_cols = [c for c in X_cols if c in df.columns]
            if len(use_cols) >= 3:  # 最低限の特徴量がある
                preds = self.predict(df[use_cols], use_recency=True, use_seasonality=True)
                y = df["actual_weight_ton"].astype(float).values
                preds = np.asarray(preds, dtype=float)
                from scipy.optimize import minimize_scalar

                low = 1.0 - float(self.calibr_clip)
                high = 1.0 + float(self.calibr_clip)

                # 初期値: メディアン比
                denom = np.maximum(preds, 1e-6)
                ratios = np.clip(y / denom, low, high)
                s0 = float(np.median(ratios))

                method = getattr(self, "calibr_method", "huber")
                if method == "median":
                    self.calibr_scale = s0
                elif method == "lad":
                    def objective_lad(s: float) -> float:
                        res = y - s * preds
                        return float(np.sum(np.abs(res)))

                    try:
                        opt = minimize_scalar(objective_lad, bounds=(low, high), method="bounded")
                        self.calibr_scale = float(opt.x if (opt.success and np.isfinite(opt.x)) else s0)
                    except Exception:
                        self.calibr_scale = s0
                else:  # huber
                    # 初期残差からdeltaを推定（MAD）
                    r0 = y - s0 * preds
                    med_r0 = float(np.median(r0))
                    mad = float(np.median(np.abs(r0 - med_r0))) + 1e-6
                    delta = 1.345 * mad

                    def huber_loss(res: np.ndarray, d: float) -> np.ndarray:
                        a = np.abs(res)
                        quad = 0.5 * (a ** 2)
                        lin = d * (a - 0.5 * d)
                        return np.where(a <= d, quad, lin)

                    def objective(s: float) -> float:
                        res = y - s * preds
                        return float(np.sum(huber_loss(res, delta)))

                    try:
                        opt = minimize_scalar(objective, bounds=(low, high), method="bounded")
                        self.calibr_scale = float(opt.x if (opt.success and np.isfinite(opt.x)) else s0)
                    except Exception:
                        self.calibr_scale = s0
            else:
                self.calibr_scale = 1.0
        except Exception:
            self.calibr_scale = 1.0

        # 営業日比率の平均（学習データ）
        if "business_ratio" in df.columns and len(df) > 0:
            self.business_mean_ratio = float(np.mean(df["business_ratio"].astype(float).values))
        else:
            self.business_mean_ratio = 0.0

    def predict(
        self,
        df: pd.DataFrame,
        use_recency: bool = True,
        use_seasonality: bool = True,
    ) -> np.ndarray:
        """予測を実行

        Args:
            df: 予測対象データ (必須カラム: month, market_size_ton, recency_7)
            use_recency: リーセンシー調整を使用するか
            use_seasonality: 季節性調整を使用するか

        Returns:
            予測値の配列（単位: トン）

        計算式:
            base_pred = market_size_ton × share_mean
            seasonal_adj = base_pred × seasonal_factor[month]
            recency_factor = recency_7 / overall_mean
            final_pred = seasonal_adj × (0.7 + 0.3 × recency_factor)
        """
        required_cols = {"month", "market_size_ton", "recency_7"}
        if not required_cols.issubset(df.columns):
            missing = required_cols - set(df.columns)
            raise KeyError(f"predict用の必須カラムが不足: {missing}")

        base_pred = df["market_size_ton"].astype(float).values * float(self.share_mean)

        if use_seasonality:
            months = df["month"].astype(int).values
            seasonal = np.array([self.seasonal_factors.get(int(m), 1.0) for m in months], dtype=float)
        else:
            seasonal = np.ones_like(base_pred, dtype=float)

        if use_recency:
            recency_factor = (df["recency_7"].astype(float).values) / float(self.overall_mean)
            recency_adj = (1.0 - float(self.recency_weight)) + float(self.recency_weight) * recency_factor
        else:
            recency_adj = np.ones_like(base_pred, dtype=float)

        pred = base_pred * seasonal * recency_adj
        # 営業日補正（オプション）
        if self.business_beta != 0.0:
            if "business_ratio" in df.columns:
                br = df["business_ratio"].astype(float).values
                mean_br = self.business_mean_ratio if self.business_mean_ratio > 0 else np.mean(br)
                biz_adj = 1.0 + float(self.business_beta) * (br - mean_br)
                pred = pred * biz_adj
        # 大型連休の月ダミー補正（オプション）
        months = df["month"].astype(int).values
        if getattr(self, "beta_gw", 0.0) != 0.0:
            gw_flag = (months == 5).astype(float)
            pred = pred * (1.0 + float(self.beta_gw) * gw_flag)
        if getattr(self, "beta_obon", 0.0) != 0.0:
            obon_flag = (months == 8).astype(float)
            pred = pred * (1.0 + float(self.beta_obon) * obon_flag)
        if getattr(self, "beta_yearend", 0.0) != 0.0:
            ye_flag = ((months == 12) | (months == 1)).astype(float)
            pred = pred * (1.0 + float(self.beta_yearend) * ye_flag)
        # 学習時の一括スケール
        pred = pred * float(self.calibr_scale)
        # 物理的制約: トンは非負
        pred = np.clip(pred, a_min=0.0, a_max=None)
        return pred

    def predict_with_interval(
        self,
        df: pd.DataFrame,
        confidence: float = 0.95,
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """信頼区間付き予測

        Args:
            df: 予測対象データ
            confidence: 信頼区間（デフォルト0.95で95%信頼区間）

        Returns:
            (point_predictions, lower_interval, upper_interval)

        処理内容:
        - 点予測を実行
        - ガンマ分布のパーセンタイルから信頼区間を計算
        """
        mu = self.predict(df)
        alpha = float(self.gamma_shape)
        # ガンマ分布の平均= alpha*theta = mu → theta = mu/alpha
        with np.errstate(divide="ignore", invalid="ignore"):
            theta = np.where(alpha > 0, mu / alpha, 0.0)
        theta = np.clip(theta, a_min=1e-9, a_max=None)

        lower_q = (1.0 - confidence) / 2.0
        upper_q = 1.0 - lower_q
        lower = stats.gamma.ppf(lower_q, a=alpha, scale=theta)
        upper = stats.gamma.ppf(upper_q, a=alpha, scale=theta)
        # 数値安定化と非負制約
        lower = np.clip(np.nan_to_num(lower, nan=0.0, posinf=None, neginf=0.0), a_min=0.0, a_max=None)
        upper = np.clip(np.nan_to_num(upper, nan=0.0, posinf=None, neginf=0.0), a_min=0.0, a_max=None)
        return mu, lower, upper


def evaluate_predictions(
    y_true: np.ndarray, y_pred: np.ndarray, dataset_name: str = "Test"
) -> Dict[str, float]:
    """予測性能を評価

    Args:
        y_true: 実測値
        y_pred: 予測値
        dataset_name: データセット名（表示用）

    Returns:
        評価指標の辞書 {'MAE': float, 'RMSE': float, 'MAPE': float, 'R2': float}

    目標性能:
        MAE: < 120トン
        RMSE: < 150トン
        MAPE: < 6%
        R²: > 0.60
    """
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)

    mae = float(mean_absolute_error(y_true, y_pred))
    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))

    eps = 1e-8
    denom = np.maximum(np.abs(y_true), eps)
    mape = float(np.mean(np.abs(y_true - y_pred) / denom) * 100.0)

    r2 = float(r2_score(y_true, y_pred))

    print(f"  MAE:  {mae:8.2f} トン {'✓' if mae < 120 else '✗'}")
    print(f"  RMSE: {rmse:8.2f} トン {'✓' if rmse < 150 else '✗'}")
    print(f"  MAPE: {mape:8.2f} % {'✓' if mape < 6 else '✗'}")
    print(f"  R²:   {r2:8.2f} {'✓' if r2 > 0.60 else '✗'}")

    return {"MAE": mae, "RMSE": rmse, "MAPE": mape, "R2": r2}


def time_series_cross_validation(
    model_class, df: pd.DataFrame, n_splits: int = 5, model_kwargs: Optional[Dict] = None
) -> pd.DataFrame:
    """時系列クロスバリデーション

    Args:
        model_class: モデルクラス（GammaRecencyModel）
        df: 全データ（月次）
        n_splits: 分割数

    Returns:
        各foldの評価結果をまとめたDataFrame
    """
    tscv = TimeSeriesSplit(n_splits=n_splits)
    df_sorted = df.sort_values("year_month").reset_index(drop=True)

    rows: List[Dict[str, float]] = []
    X_cols = ["month", "market_size_ton", "recency_7", "business_ratio"]

    for i, (train_idx, test_idx) in enumerate(tscv.split(df_sorted), start=1):
        train_df = df_sorted.loc[train_idx]
        test_df = df_sorted.loc[test_idx]

        kwargs = model_kwargs.copy() if model_kwargs is not None else {}
        # デフォルトを補完
        kwargs.setdefault("share_mean_percent", SHARE_MEAN_PERCENT)
        kwargs.setdefault("gamma_shape", GAMMA_SHAPE)
        kwargs.setdefault("gamma_scale", GAMMA_SCALE)
        kwargs.setdefault("recency_weight", RECENCY_WEIGHT)
        kwargs.setdefault("learn_seasonality", False)
        model = model_class(**kwargs)
        model.fit(train_df)

        y_true = test_df["actual_weight_ton"].values
        use_cols = [c for c in X_cols if c in test_df.columns]
        y_pred = model.predict(test_df[use_cols])

        mae = float(mean_absolute_error(y_true, y_pred))
        rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
        eps = 1e-8
        mape = float(np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), eps)) * 100.0)
        r2 = float(r2_score(y_true, y_pred))

        rows.append(
            {
                "fold": i,
                "train_size": int(len(train_df)),
                "test_size": int(len(test_df)),
                "MAE": mae,
                "RMSE": rmse,
                "MAPE": mape,
                "R2": r2,
            }
        )
        print(
            f"  Fold {i}: MAE={mae:.2f}, MAPE={mape:.2f}%, R²={r2:.4f} (train={len(train_df)}, test={len(test_df)})"
        )

    cv_df = pd.DataFrame(rows)
    if not cv_df.empty:
        print()
        print(
            f"  平均: MAE={cv_df['MAE'].mean():.2f}±{cv_df['MAE'].std():.2f}, "
            f"MAPE={cv_df['MAPE'].mean():.2f}±{cv_df['MAPE'].std():.2f}%, "
            f"R²={cv_df['R2'].mean():.2f}±{cv_df['R2'].std():.2f}"
        )
    return cv_df


def forecast_future(
    model: GammaRecencyModel,
    last_actual_data: pd.DataFrame,
    start_date: str,
    periods: int = 6,
) -> pd.DataFrame:
    """将来の搬入量を予測

    Args:
        model: 学習済みGammaRecencyModel
        last_actual_data: 最新の実績データ（リーセンシー計算用, 月次）
        start_date: 予測開始月（'YYYY-MM-DD'形式、例: '2025-10-01'）
        periods: 予測期間（月数）

    Returns:
        予測結果DataFrame (columns: year_month, year, month, predicted_weight_ton,
                          seasonal_factor, market_size_ton)

    処理内容:
    - 将来の日付を生成
    - 各年のマーケットサイズを推定
    - リーセンシー（直近7ヶ月の平均）を計算（逐次、予測も含む）
    - 予測を実行
    """
    # 準備
    lad = last_actual_data.sort_values("year_month").copy()
    # 過去7ヶ月の実績または可能な範囲
    history = lad["actual_weight_ton"].values.tolist()
    # 直近7点
    window: List[float] = history[-7:]

    start = pd.to_datetime(start_date).to_period("M").to_timestamp()
    future_months = pd.date_range(start=start, periods=periods, freq="MS")

    out_rows: List[Dict[str, float]] = []

    # 祝日対応（存在すれば使用）
    try:
        import jpholiday as _jph  # type: ignore
        _HAS_JPHOLIDAY_F = True
    except Exception:
        _HAS_JPHOLIDAY_F = False

    import calendar as _pycal
    for dt in future_months:
        year = int(dt.year)
        month = int(dt.month)
        market_month = _year_to_market_annual_ton(year) / 12.0
        # リーセンシー（直近7ヶ月平均）
        recency_7 = float(np.mean(window)) if len(window) > 0 else float(model.overall_mean)

        # 未来月の営業日比率
        ndays = _pycal.monthrange(year, month)[1]
        start = pd.Timestamp(year=year, month=month, day=1)
        days = pd.date_range(start=start, periods=ndays, freq="D")
        biz = 0
        for d in days:
            dow = int(d.dayofweek)
            is_weekday = dow < 5
            if _HAS_JPHOLIDAY_F and is_weekday:
                if _jph.is_holiday(d.to_pydatetime().date()):
                    continue
            if is_weekday:
                biz += 1
        business_days = int(biz)
        weekend_days = int(ndays - business_days)
        business_ratio = float(business_days / ndays) if ndays > 0 else 0.0

        # 予測
        tmp_df = pd.DataFrame(
            {
                "month": [month],
                "market_size_ton": [market_month],
                "recency_7": [recency_7],
                "business_ratio": [business_ratio],
            }
        )
        pred = float(model.predict(tmp_df)[0])

        out_rows.append(
            {
                "year_month": dt.strftime("%Y-%m"),
                "year": year,
                "month": month,
                "predicted_weight_ton": pred,
                "seasonal_factor": model.seasonal_factors.get(month, 1.0),
                "market_size_ton": market_month,
                "recency_7": recency_7,
                "business_days": business_days,
                "weekend_days": weekend_days,
                "business_ratio": business_ratio,
                "is_gw_month": int(month == 5),
                "is_obon_month": int(month == 8),
                "is_yearend_month": int((month == 12) or (month == 1)),
            }
        )

        # 次の月のためにウィンドウを更新（予測値を含める）
        window.append(pred)
        if len(window) > 7:
            window = window[-7:]

    return pd.DataFrame(out_rows)


def perform_hyperparam_sweep(
    df: pd.DataFrame,
    recency_weights: List[float],
    recency_modes: List[str],
    calibr_clips: List[float],
    business_betas: Optional[List[float]] = None,
    holiday_betas_gw: Optional[List[float]] = None,
    holiday_betas_obon: Optional[List[float]] = None,
    holiday_betas_yearend: Optional[List[float]] = None,
    n_splits: int = 5,
) -> Dict:
    """小さなハイパーパラメータ探索を実行

    Args:
        df: 月次データ
        recency_weights: list of recency_weight to try
        recency_modes: ['ma7', 'ewma:0.2', 'ewma:0.3'] etc.
        calibr_clips: list of calibr_clip values (e.g., 0.2, 0.3)
        n_splits: CV splits

    Returns:
        Best configuration dict
    """
    results: List[Dict] = []
    X_cols = ["month", "market_size_ton", "recency_7", "business_ratio"]
    if business_betas is None:
        business_betas = [0.0]
    # 祝日月ベータの軽量グリッド（指定なければ0周辺の小さな範囲）
    default_hgrid = [0.0, -0.05, 0.05, -0.1, 0.1, -0.2, 0.2]
    if holiday_betas_gw is None:
        holiday_betas_gw = default_hgrid
    if holiday_betas_obon is None:
        holiday_betas_obon = default_hgrid
    if holiday_betas_yearend is None:
        holiday_betas_yearend = default_hgrid

    for rw in recency_weights:
        for mode in recency_modes:
            for cc in calibr_clips:
                for bb in business_betas:
                    for bgw in holiday_betas_gw:
                        for bobon in holiday_betas_obon:
                            for byear in holiday_betas_yearend:
                                # if mode indicates EWMA, we adjust recency column before CV
                                df_mod = df.copy()
                                if mode.startswith("ewma"):
                                    try:
                                        alpha = float(mode.split(":")[1])
                                    except Exception:
                                        alpha = 0.3
                                    # compute ewma of actual_weight_ton
                                    df_mod = df_mod.sort_values("year_month").reset_index(drop=True)
                                    df_mod["recency_7"] = df_mod["actual_weight_ton"].ewm(alpha=alpha, adjust=False).mean()
                                else:
                                    # ma7 is already recency_7
                                    pass

                                # current config
                                kwargs = {
                                    "share_mean_percent": SHARE_MEAN_PERCENT,
                                    "gamma_shape": GAMMA_SHAPE,
                                    "gamma_scale": GAMMA_SCALE,
                                    "recency_weight": float(rw),
                                    "learn_seasonality": False,
                                    "calibr_clip": float(cc),
                                    "business_beta": float(bb),
                                    "beta_gw": float(bgw),
                                    "beta_obon": float(bobon),
                                    "beta_yearend": float(byear),
                                }

                                cv_df = time_series_cross_validation(GammaRecencyModel, df_mod, n_splits=n_splits, model_kwargs=kwargs)
                                mean_mae = float(cv_df["MAE"].mean()) if not cv_df.empty else float("inf")
                                res_row = {
                                    "recency_mode": mode,
                                    "recency_weight": rw,
                                    "calibr_clip": cc,
                                    "business_beta": bb,
                                    "beta_gw": bgw,
                                    "beta_obon": bobon,
                                    "beta_yearend": byear,
                                    "mean_MAE": mean_mae,
                                }
                                results.append(res_row)
                                print(
                                    f"Sweep: mode={mode} rw={rw} cc={cc} bb={bb} gw={bgw} obon={bobon} ye={byear} -> mean_MAE={mean_mae:.2f}"
                                )

    res_df = pd.DataFrame(results)
    best = res_df.sort_values("mean_MAE").iloc[0].to_dict()
    print("Best config:", best)
    return best


def plot_comprehensive_analysis(
    df: pd.DataFrame,
    predictions: np.ndarray,
    future_df: Optional[pd.DataFrame] = None,
    future_pred: Optional[np.ndarray] = None,
    save_path: str = "analysis.png",
) -> None:
    """総合分析プロット

    プロット内容:
    1. 実測値 vs 予測値（時系列）
    2. 散布図（R²表示）
    3. 残差プロット
    4. 残差分布
    5. 将来予測（オプション）

    Args:
        df: 実績データ
        predictions: 予測値
        future_df: 将来予測データ（オプション）
        future_pred: 将来予測値（オプション）
        save_path: 保存先パス
    """
    sns.set_style("whitegrid")

    y_true = df["actual_weight_ton"].values
    y_pred = np.asarray(predictions)
    resid = y_true - y_pred
    r2 = r2_score(y_true, y_pred)

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle("Gamma Recency Model Analysis", fontsize=16)

    # 1. 時系列
    ax = axes[0, 0]
    ax.plot(df["year_month"], y_true, label="Actual", marker="o")
    ax.plot(df["year_month"], y_pred, label="Predicted", marker="o")
    ax.set_title("Actual vs Predicted (Time Series)")
    ax.set_xlabel("Month")
    ax.set_ylabel("Ton")
    ax.legend()

    # 2. 散布図
    ax = axes[0, 1]
    ax.scatter(y_true, y_pred, alpha=0.8)
    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]
    ax.plot(lims, lims, "k--", alpha=0.7)
    ax.set_title(f"Scatter (R²={r2:.3f})")
    ax.set_xlabel("Actual")
    ax.set_ylabel("Predicted")

    # 3. 残差（時系列）
    ax = axes[0, 2]
    ax.plot(df["year_month"], resid, color="tab:red", marker="o")
    ax.axhline(0, color="k", linestyle="--", alpha=0.7)
    ax.set_title("Residuals over Time")

    # 4. 残差分布
    ax = axes[1, 0]
    sns.histplot(resid, kde=True, ax=ax, color="tab:purple")
    ax.set_title("Residual Distribution")

    # 5. 将来予測
    ax = axes[1, 1]
    if future_df is not None:
        ax.plot(future_df["year_month"], future_df["predicted_weight_ton"], marker="o")
    ax.set_title("Future Forecast")
    ax.set_xlabel("Month")

    # 6. 季節性因子
    ax = axes[1, 2]
    months = list(range(1, 13))
    factors = [SEASONAL_FACTORS.get(m, 1.0) for m in months]
    ax.bar(months, factors)
    ax.set_xticks(months)
    ax.set_title("Seasonal Factors (Default)")

    fig.autofmt_xdate(rotation=45)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(PLOTS_DIR, save_path), dpi=150)
    plt.close(fig)


def plot_seasonality_analysis(
    seasonal_factors: Dict[int, float],
    monthly_data: pd.DataFrame,
    save_path: str = "seasonality.png",
) -> None:
    """季節性分析プロット

    プロット内容:
    1. 月別季節性因子（棒グラフ）
    2. 月別平均搬入量（箱ひげ図）

    Args:
        seasonal_factors: 季節性因子の辞書
        monthly_data: 月次データ
        save_path: 保存先パス
    """
    sns.set_style("whitegrid")

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # 季節性因子（学習後）
    months = list(range(1, 13))
    factors = [seasonal_factors.get(m, 1.0) for m in months]
    axes[0].bar(months, factors, color="tab:blue")
    axes[0].set_xticks(months)
    axes[0].set_title("Learned Seasonal Factors")

    # 月別実績の箱ひげ
    axes[1].set_title("Monthly Actual Distribution")
    sns.boxplot(
        x="month",
        y="actual_weight_ton",
        data=monthly_data,
        ax=axes[1],
        palette="Blues",
    )

    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, save_path), dpi=150)
    plt.close(fig)


# 追加: ガンマ分布適合度検証

def verify_gamma_fit(data: np.ndarray, shape: float, scale: float) -> Tuple[float, float]:
    """ガンマ分布適合度検定（Kolmogorov-Smirnov）

    期待結果: p値 > 0.05 で適合
    例: KS統計量=0.1215, p値=0.3618
    """
    ks_stat, p_value = kstest(data, lambda x: gamma.cdf(x, shape, scale=scale))
    print(f"KS統計量: {ks_stat:.4f}")
    print(f"p値: {p_value:.4f}")
    print(f"判定: {'✓ ガンマ分布に適合' if p_value > 0.05 else '✗ 不適合'}")
    return ks_stat, p_value


def _print_header() -> None:
    print("=" * 80)
    print(" " * 20 + "ガンマリーセンシーモデル実行")
    print("=" * 80)


def main() -> None:
    """メイン実行フロー

    実行内容:
    1. データ読み込みと前処理
    2. 訓練/テスト分割（80/20）
    3. モデル訓練
    4. クロスバリデーション
    5. テストデータ評価
    6. 将来予測（2025-10〜2026-03）
    7. 可視化
    8. 結果保存
    """
    _print_header()

    # 1. データ読み込み
    print("\n[1/8] データ読み込み中...")
    # submission フォルダ内の data/input を優先的に利用
    default_path = os.path.join(SUBMISSION_ROOT, "data", "input", "01_daily_clean.csv")
    alt_path = os.path.join(SUBMISSION_ROOT, "data", "input", "01_daily_clean_with_date.csv")
    fallback_root_path = os.path.join(os.path.dirname(os.path.dirname(SUBMISSION_ROOT)), "input", "01_daily_clean.csv")
    if os.path.exists(default_path):
        data_path = default_path
    elif os.path.exists(alt_path):
        data_path = alt_path
    elif os.path.exists(fallback_root_path):
        # 旧来の /works/data/input を最後の手段として参照
        data_path = fallback_root_path
    else:
        raise FileNotFoundError(
            "日次入力CSVが見つかりません。submission_release_20251027/data/input に "
            "01_daily_clean.csv か 01_daily_clean_with_date.csv を配置してください。"
        )
    monthly = preprocess_daily_to_monthly(data_path)
    print(f"  ✓ {len(_safe_read_csv(data_path)):,}行の日次データを読み込み")
    print(f"  ✓ {len(monthly):,}ヶ月の月次データに変換")

    # 2. 訓練/テスト分割
    print("\n[2/8] 訓練/テスト分割...")
    monthly_sorted = monthly.sort_values("year_month").reset_index(drop=True)
    n = len(monthly_sorted)
    train_n = int(np.floor(n * 0.8))
    train_df = monthly_sorted.iloc[:train_n].copy()
    test_df = monthly_sorted.iloc[train_n:].copy()
    if not train_df.empty:
        print(
            f"  訓練データ: {len(train_df)}ヶ月 ({train_df['year_month'].min().strftime('%Y-%m')} ~ {train_df['year_month'].max().strftime('%Y-%m')})"
        )
    if not test_df.empty:
        print(
            f"  テストデータ: {len(test_df)}ヶ月 ({test_df['year_month'].min().strftime('%Y-%m')} ~ {test_df['year_month'].max().strftime('%Y-%m')})"
        )

    # 3. 設定読み込み（存在すれば）とハイパーパラメータ探索
    print("\n[3/8] 設定読込とハイパーパラメータ探索実行中...")
    # submission フォルダ内の models/gamma_recency_model/config.yaml を参照
    config_path = os.path.join(SUBMISSION_ROOT, "models", "gamma_recency_model", "config.yaml")
    cfg = {}
    if os.path.exists(config_path):
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f) or {}
            print(f"  ✓ 設定ファイル読込: {config_path}")
        except Exception as e:
            print(f"  設定ファイル読込に失敗: {e}")

    recency_weights = cfg.get("grid", {}).get("recency_weights", [0.42, 0.44, 0.46, 0.48])
    ewma_alphas = cfg.get("grid", {}).get("ewma_alphas", [0.12, 0.14, 0.16, 0.18])
    recency_modes = [f"ewma:{a}" for a in ewma_alphas]
    calibr_clips = cfg.get("calibr", {}).get("clips", [0.2])
    business_betas = cfg.get("grid", {}).get("business_betas", [0.1, 0.2, 0.3])
    calibr_method = cfg.get("calibr", {}).get("method", "huber")
    # holiday beta sweep grids (optional)
    hgrid = cfg.get("grid", {}).get("holiday_betas", {})
    h_gw = hgrid.get("gw")
    h_obon = hgrid.get("obon")
    h_yearend = hgrid.get("yearend")
    # defaults applied inside perform_hyperparam_sweep if None
    best_cfg = perform_hyperparam_sweep(
        monthly_sorted,
        recency_weights,
        recency_modes,
        calibr_clips,
        business_betas=business_betas,
        holiday_betas_gw=h_gw,
        holiday_betas_obon=h_obon,
        holiday_betas_yearend=h_yearend,
        n_splits=5,
    )

    print("\n[4/8] 選択されたハイパラで学習とクロスバリデーションを実行...")
    # 選ばれた設定で再実行
    model_kwargs = {
        "share_mean_percent": SHARE_MEAN_PERCENT,
        "gamma_shape": GAMMA_SHAPE,
        "gamma_scale": GAMMA_SCALE,
        "recency_weight": float(best_cfg["recency_weight"]),
        "learn_seasonality": False,
        "calibr_clip": float(best_cfg["calibr_clip"]),
        # 営業日補正（グリッド探索で選択）
        "business_beta": float(best_cfg.get("business_beta", 0.2)),
        # キャリブレーション手法
        "calibr_method": calibr_method,
        # 大型連休月の補正（必要に応じて設定）
        "beta_gw": float(best_cfg.get("beta_gw", 0.0)),
        "beta_obon": float(best_cfg.get("beta_obon", 0.0)),
        "beta_yearend": float(best_cfg.get("beta_yearend", 0.0)),
    }
    # クロスバリデーションの詳細表示
    cv_df = time_series_cross_validation(GammaRecencyModel, monthly_sorted, n_splits=5, model_kwargs=model_kwargs)

    # モデルを訓練（訓練データ全体）
    model = GammaRecencyModel(**model_kwargs)
    model.fit(train_df)
    print("  ✓ 季節性因子を学習（または既定値適用）完了")

    # 5. テストデータ評価
    print("\n[5/8] テストデータで評価中...")
    X_cols = ["month", "market_size_ton", "recency_7", "business_ratio"]
    y_true_test = test_df["actual_weight_ton"].values
    use_cols_test = [c for c in X_cols if c in test_df.columns]
    y_pred_test, lb95, ub95 = model.predict_with_interval(test_df[use_cols_test])
    # 1-sigma bounds (Gamma): sigma = mu / sqrt(alpha)
    with np.errstate(divide="ignore", invalid="ignore"):
        _sigma_test = np.where(model.gamma_shape > 0, y_pred_test / np.sqrt(model.gamma_shape), 0.0)
    lower_1s_test = np.clip(y_pred_test - _sigma_test, a_min=0.0, a_max=None)
    upper_1s_test = y_pred_test + _sigma_test
    test_metrics = evaluate_predictions(y_true_test, y_pred_test, dataset_name="Test")

    # 6. 将来予測
    print("\n[6/8] 将来予測（6ヶ月）実行中...")
    
    # 学習データの最終月を取得し、その翌月を予測開始日とする
    last_date = monthly_sorted["year_month"].max()
    next_month = last_date + pd.DateOffset(months=1)
    start_date_str = next_month.strftime("%Y-%m-%d")
    print(f"  予測開始月: {start_date_str} (学習データ末月: {last_date.strftime('%Y-%m')})")

    future = forecast_future(
        model=model,
        last_actual_data=monthly_sorted,
        start_date=start_date_str,
        periods=int(cfg.get("future", {}).get("periods", 6)),
    )
    # ログ表示
    for _, r in future.iterrows():
        print(f"  {r['year_month']}:  {r['predicted_weight_ton']:7.0f} トン")
    print(
        f"  合計: {future['predicted_weight_ton'].sum():,.0f} トン (月平均: {future['predicted_weight_ton'].mean():,.0f} トン)"
    )

    # 7. 可視化
    print("\n[7/8] 可視化作成中...")
    use_cols_all = [c for c in X_cols if c in monthly_sorted.columns]
    plot_comprehensive_analysis(
        df=monthly_sorted,
        predictions=model.predict(monthly_sorted[use_cols_all]),
        future_df=future,
        save_path="analysis.png",
    )
    plot_seasonality_analysis(model.seasonal_factors, monthly_sorted, save_path="seasonality.png")
    print("  ✓ analysis.png 保存完了")
    print("  ✓ seasonality.png 保存完了")

    # ブレンド（RidgeCV）を既定で実行（安全にオン/オフ可）
    print("\n[7.5/8] ブレンド（RidgeCV）学習・評価中...")
    blend_cfg = cfg.get("blend", {})
    blend_enable = bool(blend_cfg.get("enable", True))
    ridge_alphas = blend_cfg.get("ridge_alphas", [0.01, 0.1, 1.0, 10.0, 100.0, 300.0])
    blend_cv_splits = int(blend_cfg.get("cv_splits", 5))

    def _build_blend_features(df_: pd.DataFrame, gamma_pred_: Optional[np.ndarray] = None) -> pd.DataFrame:
        """ブレンド用特徴量生成（リーク防止のためシフトを使用、NAsは後段で処理）"""
        feats = pd.DataFrame(index=df_.index)

        # ベース特徴
        for col in ["market_size_ton", "recency_7", "business_ratio"]:
            if col in df_.columns:
                feats[col] = df_[col].astype(float)

        # 季節性（円関数）
        if "month" in df_.columns:
            m = df_["month"].astype(float)
            feats["month_sin"] = np.sin(2 * np.pi * m / 12.0)
            feats["month_cos"] = np.cos(2 * np.pi * m / 12.0)
            # 月ダミー（線形モデル強化）
            for mm in range(1, 13):
                feats[f"m_{mm:02d}"] = (df_["month"] == mm).astype(float)

        # 祝月フラグ
        for hcol in ["is_gw_month", "is_obon_month", "is_yearend_month"]:
            if hcol in df_.columns:
                feats[hcol] = df_[hcol].astype(float)

        # 時間トレンド（単調増加インデックス）
        feats["t_index"] = np.arange(len(feats), dtype=float)

        # ガンマ予測由来の特徴
        if gamma_pred_ is not None:
            # 直値
            feats["gamma_pred"] = np.asarray(gamma_pred_, dtype=float)
            # 相互作用
            if "business_ratio" in feats.columns:
                feats["gamma_x_business"] = feats["gamma_pred"] * feats["business_ratio"]
            # ラグと移動平均、増減率
            feats["gamma_lag1"] = pd.Series(feats["gamma_pred"]).shift(1)
            feats["gamma_lag2"] = pd.Series(feats["gamma_pred"]).shift(2)
            feats["gamma_ma3"] = pd.Series(feats["gamma_pred"]).rolling(3).mean()
            with np.errstate(divide="ignore", invalid="ignore"):
                feats["gamma_pct_chg1"] = feats["gamma_pred"] / feats["gamma_lag1"] - 1.0
            # 交互作用
            if "month_sin" in feats.columns:
                feats["gamma_x_msin"] = feats["gamma_pred"] * feats["month_sin"]
            if "month_cos" in feats.columns:
                feats["gamma_x_mcos"] = feats["gamma_pred"] * feats["month_cos"]
            for hcol in ["is_gw_month", "is_obon_month", "is_yearend_month"]:
                if hcol in feats.columns:
                    feats[f"gamma_x_{hcol}"] = feats["gamma_pred"] * feats[hcol]

        # recencyのラグ（安定性のため1本）
        if "recency_7" in feats.columns:
            feats["recency_lag1"] = feats["recency_7"].shift(1)
        # 非線形変換
        if "business_ratio" in feats.columns:
            feats["business_ratio_sq"] = feats["business_ratio"] ** 2
        feats["t_index_sq"] = feats["t_index"] ** 2

        return feats

    blended_pred_df = None
    future_blended_df = None
    if blend_enable:
        # gamma予測を特徴量として付与（履歴+将来を連結してラグを安定生成）
        use_cols_all = [c for c in ["month", "market_size_ton", "recency_7", "business_ratio"] if c in monthly_sorted.columns]
        monthly_sorted["gamma_pred_base"] = model.predict(monthly_sorted[use_cols_all])
        fut_use_cols = [c for c in ["month", "market_size_ton", "recency_7", "business_ratio"] if c in future.columns]
        future_gamma = model.predict(future[fut_use_cols])

        df_all_concat = pd.concat([monthly_sorted, future], ignore_index=True)
        gamma_all = np.concatenate([monthly_sorted["gamma_pred_base"].values, future_gamma])
        X_all_blend = _build_blend_features(df_all_concat, gamma_pred_=gamma_all).fillna(0.0)

        # 分割
        n_hist = len(monthly_sorted)
        X_full_blend = X_all_blend.iloc[:n_hist]
        X_future_b = X_all_blend.iloc[n_hist:]

        X_train_b = X_full_blend.iloc[:train_n]
        y_train_b = monthly_sorted["actual_weight_ton"].values[:train_n]
        X_test_b = X_full_blend.iloc[train_n:]
        y_test_b = monthly_sorted["actual_weight_ton"].values[train_n:]

        # 残差学習用のターゲット（線形系で有効なことが多い）
        gamma_train_base = monthly_sorted["gamma_pred_base"].values[:train_n]
        gamma_test_base = monthly_sorted["gamma_pred_base"].values[train_n:]
        y_train_resid = y_train_b - gamma_train_base
        # y_test_resid は評価時に使用（学習には不要）
        y_test_resid = y_test_b - gamma_test_base

        # 対数残差（乗法誤差の線形化）
        with np.errstate(divide="ignore", invalid="ignore"):
            lg_y_train = np.log1p(y_train_b)
            lg_gamma_train = np.log1p(gamma_train_base)
            y_train_logresid = lg_y_train - lg_gamma_train
            lg_gamma_test = np.log1p(gamma_test_base)

        tscv_blend = TimeSeriesSplit(n_splits=max(2, min(blend_cv_splits, max(2, len(X_train_b) - 1))))

        # 候補モデルを比較
        candidates: List[Tuple[str, object, str]] = []  # (name, model, target_mode['direct'|'residual'])
        # RidgeCV（標準化込み）
        candidates.append((
            "ridge_resid",
            Pipeline([
                ("scaler", StandardScaler(with_mean=True, with_std=True)),
                ("model", RidgeCV(alphas=ridge_alphas, cv=tscv_blend)),
            ]),
            "residual",
        ))
        # ElasticNetCV（標準化込み）
        candidates.append((
            "elasticnet_resid",
            Pipeline([
                ("scaler", StandardScaler(with_mean=True, with_std=True)),
                ("model", ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], alphas=np.logspace(-3, 3, 13), cv=tscv_blend, max_iter=5000)),
            ]),
            "residual",
        ))
        # Ridge（log残差）
        candidates.append((
            "ridge_logresid",
            Pipeline([
                ("scaler", StandardScaler(with_mean=True, with_std=True)),
                ("model", RidgeCV(alphas=ridge_alphas, cv=tscv_blend)),
            ]),
            "logresid",
        ))
        # ElasticNet（log残差）
        candidates.append((
            "elasticnet_logresid",
            Pipeline([
                ("scaler", StandardScaler(with_mean=True, with_std=True)),
                ("model", ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], alphas=np.logspace(-3, 3, 13), cv=tscv_blend, max_iter=5000)),
            ]),
            "logresid",
        ))
        # LightGBM（存在すれば）
        if LGBMRegressor is not None:
            candidates.append((
                "lgbm",
                LGBMRegressor(
                    n_estimators=800,
                    learning_rate=0.03,
                    num_leaves=31,
                    subsample=0.9,
                    colsample_bytree=0.8,
                    min_child_samples=5,
                    reg_alpha=0.1,
                    reg_lambda=0.3,
                    random_state=42,
                ),
                "direct",
            ))
        if CatBoostRegressor is not None:
            candidates.append((
                "catboost",
                CatBoostRegressor(
                    iterations=1500,
                    depth=5,
                    learning_rate=0.03,
                    l2_leaf_reg=4.0,
                    subsample=0.9,
                    random_state=42,
                    verbose=False,
                ),
                "direct",
            ))

        results = []
        for name, mdl, mode_ in candidates:
            try:
                # 学習ターゲット選択
                if mode_ == "residual":
                    mdl.fit(X_train_b, y_train_resid)
                    y_hat = gamma_test_base + mdl.predict(X_test_b)
                elif mode_ == "logresid":
                    mdl.fit(X_train_b, y_train_logresid)
                    y_hat = np.expm1(lg_gamma_test + mdl.predict(X_test_b))
                else:
                    mdl.fit(X_train_b, y_train_b)
                    y_hat = mdl.predict(X_test_b)
                print(f"  [{name}] テスト評価:")
                metrics_ = evaluate_predictions(y_test_b, y_hat, dataset_name=f"Blend-{name}(Test)")
                results.append((name, mdl, mode_, y_hat, metrics_))
            except Exception as e:  # noqa: BLE001
                print(f"  [{name}] 学習/評価失敗: {e}")

        # 最良モデル選択（R²最大）
        if len(results) == 0:
            raise RuntimeError("ブレンド候補モデルの学習に失敗しました")
        results.sort(key=lambda x: x[4].get("R2", -1.0), reverse=True)
        best_name, best_model, best_mode, y_pred_blend_test, best_metrics = results[0]
        print(f"  → 選択モデル: {best_name}  R²(Test)={best_metrics.get('R2'):.3f}")

        # 1-sigma（学習残差から推定）
        try:
            if best_mode == "residual":
                _train_pred = gamma_train_base + best_model.predict(X_train_b)
            else:
                _train_pred = best_model.predict(X_train_b)
            _train_resid = y_train_b - _train_pred
            blend_sigma = float(np.nanstd(_train_resid, ddof=1))
            if not np.isfinite(blend_sigma) or blend_sigma < 0:
                blend_sigma = float(np.nanstd(_train_resid, ddof=0))
        except Exception:
            blend_sigma = 0.0

        # テスト予測の保存用DF
        blended_pred_df = test_df[["year_month", "year", "month", "actual_weight_ton"]].copy()
        blended_pred_df["gamma_pred"] = monthly_sorted.loc[train_n:, "gamma_pred_base"].values
        blended_pred_df["blended_pred"] = y_pred_blend_test
        blended_pred_df["residual"] = blended_pred_df["actual_weight_ton"] - blended_pred_df["blended_pred"]
        blended_pred_df["abs_error"] = blended_pred_df["residual"].abs()
        blended_pred_df["pct_error"] = (blended_pred_df["abs_error"] / np.maximum(blended_pred_df["actual_weight_ton"].abs(), 1e-8)) * 100.0
        blended_pred_df["year_month"] = pd.to_datetime(blended_pred_df["year_month"]).dt.strftime("%Y-%m")
        blended_pred_df["lower_1sigma"] = np.clip(blended_pred_df["blended_pred"].values - blend_sigma, a_min=0.0, a_max=None)
        blended_pred_df["upper_1sigma"] = blended_pred_df["blended_pred"].values + blend_sigma

        # 将来ブレンド（X_future_bは履歴と連結した上でシフト済み特徴）
        if best_mode == "residual":
            future_blend_pred = future_gamma + best_model.predict(X_future_b)
        else:
            future_blend_pred = best_model.predict(X_future_b)
        future_blended_df = future[["year_month", "year", "month"]].copy()
        future_blended_df["gamma_pred"] = future_gamma
        future_blended_df["blended_pred"] = future_blend_pred
        future_blended_df["lower_1sigma"] = np.clip(future_blended_df["blended_pred"].values - blend_sigma, a_min=0.0, a_max=None)
        future_blended_df["upper_1sigma"] = future_blended_df["blended_pred"].values + blend_sigma

        # ブレンド版のプロットも出力（analysis_blended.png）。
        if best_mode == "residual":
            full_blended_pred = monthly_sorted["gamma_pred_base"].values + best_model.predict(X_full_blend)
        else:
            full_blended_pred = best_model.predict(X_full_blend)
        future_for_plot = future.copy()
        future_for_plot = future_for_plot.drop(columns=["predicted_weight_ton"], errors="ignore")
        future_for_plot["predicted_weight_ton"] = future_blend_pred
        plot_comprehensive_analysis(
            df=monthly_sorted,
            predictions=full_blended_pred,
            future_df=future_for_plot,
            save_path="analysis_blended.png",
        )
        print("  ✓ analysis_blended.png 保存完了")

        # 学習済みブレンドモデルの保存
        try:
            models_dir = os.path.join(OUTPUT_DIR, "models")
            os.makedirs(models_dir, exist_ok=True)
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            blend_path = os.path.join(models_dir, f"blend_{best_name}_{ts}.pkl")
            if joblib is not None:
                joblib.dump(best_model, blend_path)
                # latestコピー
                joblib.dump(best_model, os.path.join(models_dir, f"blend_{best_name}_latest.pkl"))
            meta = {
                "model_type": best_name,
                "timestamp": ts,
                "features": list(X_full_blend.columns),
                "r2_test": float(best_metrics.get("R2", float("nan"))),
                "mae_test": float(best_metrics.get("MAE", float("nan"))),
                "rmse_test": float(best_metrics.get("RMSE", float("nan"))),
                "mape_test": float(best_metrics.get("MAPE", float("nan"))),
                "blend_sigma": float(blend_sigma),
                "target_mode": best_mode,
            }
            with open(os.path.join(models_dir, f"blend_meta_{ts}.json"), "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
            with open(os.path.join(models_dir, "blend_meta_latest.json"), "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
        except Exception as e:  # noqa: BLE001
            print(f"  ブレンドモデル保存に失敗: {e}")

    # 8. 結果保存
    print("\n[8/8] 結果をCSVに保存中...")
    # 予測結果（テスト期間）
    pred_df = test_df[["year_month", "year", "month", "actual_weight_ton"]].copy()
    pred_df["predicted_weight_ton"] = y_pred_test
    pred_df["lower_bound_95"] = lb95
    pred_df["upper_bound_95"] = ub95
    pred_df["residual"] = pred_df["actual_weight_ton"] - pred_df["predicted_weight_ton"]
    pred_df["abs_error"] = pred_df["residual"].abs()
    eps = 1e-8
    pred_df["pct_error"] = (
        (pred_df["abs_error"] / np.maximum(pred_df["actual_weight_ton"].abs(), eps)) * 100.0
    )
    pred_df["lower_1sigma"] = lower_1s_test
    pred_df["upper_1sigma"] = upper_1s_test
    # year_month as YYYY-MM
    pred_df["year_month"] = pred_df["year_month"].dt.strftime("%Y-%m")

    prediction_results_path = os.path.join(OUTPUT_DIR, "prediction_results.csv")
    pred_df[[
        "year_month",
        "actual_weight_ton",
        "predicted_weight_ton",
        "lower_bound_95",
        "upper_bound_95",
        "lower_1sigma",
        "upper_1sigma",
        "residual",
        "abs_error",
        "pct_error",
    ]].to_csv(prediction_results_path, index=False)

    # Add 1-sigma bounds for future using Gamma params
    with np.errstate(divide="ignore", invalid="ignore"):
        _sigma_future = np.where(model.gamma_shape > 0, future["predicted_weight_ton"].values / np.sqrt(model.gamma_shape), 0.0)
    future["lower_1sigma"] = np.clip(future["predicted_weight_ton"].values - _sigma_future, a_min=0.0, a_max=None)
    future["upper_1sigma"] = future["predicted_weight_ton"].values + _sigma_future

    future_path = os.path.join(OUTPUT_DIR, "future_forecast.csv")
    future[[
        "year_month",
        "year",
        "month",
        "predicted_weight_ton",
        "lower_1sigma",
        "upper_1sigma",
        "seasonal_factor",
        "market_size_ton",
        "business_days",
        "weekend_days",
        "business_ratio",
        "is_gw_month",
        "is_obon_month",
        "is_yearend_month",
    ]].to_csv(future_path, index=False)

    cv_path = os.path.join(OUTPUT_DIR, "cv_results.csv")
    cv_df.to_csv(cv_path, index=False)

    # 追加の監視用集計
    try:
        # 月別残差統計
        rbm = (
            pred_df.assign(residual=lambda d: d["residual"], abs_error=lambda d: d["abs_error"]).groupby("month")
            .agg(count=("residual", "size"), mae=("abs_error", "mean"), bias=("residual", "mean"))
            .reset_index()
        )
        rbm_path = os.path.join(OUTPUT_DIR, "residual_by_month.csv")
        rbm.to_csv(rbm_path, index=False)

        # 営業日比率デシル別
        if "business_ratio" in test_df.columns:
            tmp = test_df[["business_ratio"]].copy()
            tmp["decile"] = pd.qcut(tmp["business_ratio"], 10, labels=False, duplicates="drop")
            pr = pred_df.copy()
            pr["decile"] = tmp["decile"].values
            rbd = pr.groupby("decile").agg(
                count=("residual", "size"), mae=("abs_error", "mean"), bias=("residual", "mean")
            ).reset_index()
            rbd_path = os.path.join(OUTPUT_DIR, "residual_by_business_ratio_decile.csv")
            rbd.to_csv(rbd_path, index=False)

        # ブレンド版のモニタリング出力
        if blended_pred_df is not None:
            rbm_b = (
                blended_pred_df.assign(residual=lambda d: d["residual"], abs_error=lambda d: d["abs_error"]).groupby("month")
                .agg(count=("residual", "size"), mae=("abs_error", "mean"), bias=("residual", "mean"))
                .reset_index()
            )
            rbm_b_path = os.path.join(OUTPUT_DIR, "residual_by_month_blended.csv")
            rbm_b.to_csv(rbm_b_path, index=False)
            if "business_ratio" in test_df.columns:
                tmpb = test_df[["business_ratio"]].copy()
                tmpb["decile"] = pd.qcut(tmpb["business_ratio"], 10, labels=False, duplicates="drop")
                prb = blended_pred_df.copy()
                prb["decile"] = tmpb["decile"].values
                rbd_b = prb.groupby("decile").agg(
                    count=("residual", "size"), mae=("abs_error", "mean"), bias=("residual", "mean")
                ).reset_index()
                rbd_b_path = os.path.join(OUTPUT_DIR, "residual_by_business_ratio_decile_blended.csv")
                rbd_b.to_csv(rbd_b_path, index=False)
    except Exception as _e:  # noqa: F841
        pass

    print("  ✓ prediction_results.csv")
    print("  ✓ future_forecast.csv")
    print("  ✓ cv_results.csv")
    if blended_pred_df is not None:
        bpred_path = os.path.join(OUTPUT_DIR, "blended_prediction_results.csv")
        blended_pred_df[[
            "year_month",
            "actual_weight_ton",
            "gamma_pred",
            "blended_pred",
            "lower_1sigma",
            "upper_1sigma",
            "residual",
            "abs_error",
            "pct_error",
        ]].to_csv(bpred_path, index=False)
        bfut_path = os.path.join(OUTPUT_DIR, "blended_future_forecast.csv")
        future_blended_df.to_csv(bfut_path, index=False)
        print("  ✓ blended_prediction_results.csv")
        print("  ✓ blended_future_forecast.csv")

    # Gammaモデルの保存（パラメータ込み）
    try:
        models_dir = os.path.join(OUTPUT_DIR, "models")
        os.makedirs(models_dir, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        gamma_path = os.path.join(models_dir, f"gamma_model_{ts}.pkl")
        if joblib is not None:
            joblib.dump(model, gamma_path)
            joblib.dump(model, os.path.join(models_dir, "gamma_model_latest.pkl"))
        # メタ情報
        gmeta = {
            "timestamp": ts,
            "gamma_shape": float(getattr(model, "gamma_shape", float("nan"))),
            "gamma_scale": float(getattr(model, "gamma_scale", float("nan"))),
            "recency_weight": float(getattr(model, "recency_weight", float("nan"))),
            "business_beta": float(getattr(model, "business_beta", float("nan"))),
            "beta_gw": float(getattr(model, "beta_gw", 0.0)),
            "beta_obon": float(getattr(model, "beta_obon", 0.0)),
            "beta_yearend": float(getattr(model, "beta_yearend", 0.0)),
        }
        with open(os.path.join(models_dir, f"gamma_meta_{ts}.json"), "w", encoding="utf-8") as f:
            json.dump(gmeta, f, ensure_ascii=False, indent=2)
        with open(os.path.join(models_dir, "gamma_meta_latest.json"), "w", encoding="utf-8") as f:
            json.dump(gmeta, f, ensure_ascii=False, indent=2)
    except Exception as e:  # noqa: BLE001
        print(f"Gammaモデル保存に失敗: {e}")

    print("\n" + "=" * 80)
    print("完了！")
    print("=" * 80)


if __name__ == "__main__":
    # 最小限動作確認（ユニット的）
    try:
        # テスト用ダミーデータ
        test_data = pd.DataFrame(
            {
                "month": [1, 2, 3, 4, 5, 6],
                "market_size_ton": [2_166_667] * 6,
                "recency_7": [2200, 2250, 2300, 2280, 2290, 2285],
                "actual_weight_ton": [2000, 2100, 2700, 2300, 2050, 2100],
            }
        )
        model = GammaRecencyModel(
            share_mean_percent=SHARE_MEAN_PERCENT,
            gamma_shape=GAMMA_SHAPE,
            gamma_scale=GAMMA_SCALE,
        )
        model.fit(test_data)
        predictions = model.predict(test_data)
        assert len(predictions) == 6, "予測数が不正"
        assert all(1500 < p < 3500 for p in predictions), "予測値が範囲外"
        assert model.seasonal_factors is not None, "季節性因子が未学習"
        print("✓ 基本動作確認OK")
    except Exception as e:  # noqa: BLE001
        print(f"基本動作確認でエラー: {e}")

    # 本処理
    main()
