# CSV Upload STG Layer Column Loss Analysis

**Date**: 2025-11-14  
**Issue**: Yard and Shipment CSV uploads fail at STG layer with NOT NULL violation

---

## Executive Summary

**Root Cause**: `filter_defined_columns()` and `to_sql_ready_df()` functions are removing all data columns except tracking columns during STG layer processing.

**Impact**:

- RAW layer: ✅ SUCCESS (all columns saved)
- STG layer: ❌ FAILURE (only tracking columns remain)

---

## Error Pattern

### Shipment CSV Error

```
[SQL: INSERT INTO stg.shipment_shogun_flash
  (net_weight, quantity, amount, upload_file_id, source_row_no)
VALUES (%(net_weight)s, %(quantity)s, %(amount)s, %(upload_file_id)s::INTEGER, %(source_row_no)s::INTEGER)]

Failing row contains: (18979, null, null, null, null, null, null, null, null, 0, 0, null, null, 0, null, null, null, ...)
```

### Yard CSV Error

```
[SQL: INSERT INTO stg.yard_shogun_flash
  (net_weight, quantity, amount, upload_file_id, source_row_no)
VALUES (%(net_weight)s, %(quantity)s, %(amount)s, %(upload_file_id)s::INTEGER, %(source_row_no)s::INTEGER)]

Failing row contains: (12302, null, null, null, 0, 0, null, null, 0, null, null, null, null, null, null, null, ...)
```

**Key Observation**:

- INSERT statement only contains 5 columns
- Expected columns (slip_date, client_name, item_name, etc.) are missing
- Only net_weight, quantity, amount, upload_file_id, source_row_no remain

---

## Data Flow Analysis

### Step 1: Column Rename (✅ Working)

```
[DEBUG REPO] stg.shipment: After rename, columns=['slip_date', 'shipment_no', 'client_name', 'vendor_cd', 'vendor_name', 'site_cd', 'site_name', 'item_name', 'net_weight', 'quantity', 'unit_name', 'unit_price', 'amount', 'transport_vendor_name', 'slip_type_name']
```

- 15 columns present after rename
- All expected columns exist

### Step 2: Filter Defined Columns (⚠️ Issue Here)

```
[DEBUG REPO] [stg] YAML valid columns for shipment: 18 cols
[DEBUG REPO] [stg] Before filter: 20 cols, After filter will keep: 20 cols
[DEBUG REPO] [stg] After filter: 20 cols: ['slip_date', 'shipment_no', 'client_name', ...]
```

- Shows 20 columns after filter
- Seems correct at this point

### Step 3: to_sql_ready_df() (❌ Major Issue)

```
[DEBUG REPO] [stg] After to_sql_ready: 20 cols: ['slip_date', 'shipment_no', 'client_name', ...]
```

- Logs show 20 columns still present
- BUT actual SQL INSERT only has 5 columns

### Step 4: ORM Object Creation (❌ Data Loss Confirmed)

```
[DEBUG REPO] First record keys: ['slip_date', 'shipment_no', 'client_name', 'vendor_cd', 'vendor_name', 'site_cd', 'site_name', 'item_name', 'net_weight', 'quantity', 'unit_name', 'unit_price', 'amount', 'transport_vendor_name', 'slip_type_name']

[DEBUG REPO] First record sample: {
  'slip_date': datetime.date(2024, 4, 1),
  'shipment_no': '620.0',
  'client_name': '宮崎',
  'vendor_cd': 747.0,
  'vendor_name': '宮崎',
  'site_cd': 1.0,
  'site_name': '新木場ﾘｻｲｸﾙｾﾝﾀｰ',
  'item_name': 'ダンボール',
  'net_weight': 530.0,
  'quantity': 530.0,
  'unit_name': 'kg',
  'unit_price': 14.0,
  'amount': 7420.0,
  'transport_vendor_name': '■ｺｳﾗﾝ',
  'slip_type_name': '売上',
  'detail_note': None,
  'category_cd': 1.0,
  'category_name': '処分費',
  'source_row_no': 1,
  'upload_file_id': 2
}

[DEBUG REPO] First payload keys: ['slip_date', 'shipment_no', 'client_name', 'vendor_cd', 'vendor_name', 'site_cd', 'site_name', 'item_name', 'net_weight', 'quantity', 'unit_name', 'unit_price', 'amount', 'transport_vendor_name', 'slip_type_name']
```

- Record dictionary has all 20 columns with data
- Payload keys show all 15 YAML columns
- **BUT** SQLAlchemy only generates INSERT with 5 columns

---

## Root Cause Analysis

### Issue: ORM Model Column Mismatch

The problem occurs when `create_shogun_model_class()` generates the ORM model:

1. **DataFrame has 20 columns** (15 YAML + 5 extra)
2. **Payload dictionary has 20 keys** with actual data
3. **ORM Model Class** created by `create_shogun_model_class()` may not have all column definitions
4. **SQLAlchemy bulk_save_objects** only inserts columns that exist in the model definition

**Hypothesis**: The ORM model generated by `create_shogun_model_class()` is missing column definitions, causing SQLAlchemy to ignore those fields in the payload dictionary.

### Why RAW Works But STG Fails

**RAW Layer**:

- Uses same process
- Actually succeeds
- All columns inserted correctly

**STG Layer**:

- Same code path
- Fails with only 5 columns
- Suggests schema or model definition issue

### Possible Causes

1. **Schema-specific model generation issue**:

   - `create_shogun_model_class(csv_type, table_name, schema='stg')` may be reading wrong table definition
   - STG table schema may be out of sync

2. **Column type mismatch**:

   - Some columns may have incompatible types between DataFrame and DB
   - SQLAlchemy may be silently dropping them

3. **Table reflection issue**:
   - If using reflection, STG table metadata may be incorrect
   - Cached metadata may be stale

---

## Detailed Error Context

### Shipment CSV

- **RAW Save**: ✅ 19,331 rows saved successfully
- **Formatter**: Reduced to 18,979 rows (deduplication)
- **STG Save**: ❌ Failed with only 5 columns in INSERT

### Yard CSV

- **RAW Save**: ✅ 12,561 rows saved successfully
- **Formatter**: Reduced to 12,302 rows (deduplication)
- **STG Save**: ❌ Failed with only 5 columns in INSERT

### Common Pattern

Both failures show identical symptoms:

- Only `net_weight`, `quantity`, `amount`, `upload_file_id`, `source_row_no` in SQL
- All other columns (`slip_date`, `client_name`, `item_name`, etc.) missing from INSERT
- NOT NULL constraint on `slip_date` causes immediate failure

---

## Next Steps for Investigation

1. **Check ORM Model Generation**:

   ```python
   model_class = create_shogun_model_class('shipment', table_name='shipment_shogun_flash', schema='stg')
   # Inspect model_class.__table__.columns
   ```

2. **Verify STG Table Schema**:

   ```sql
   \d stg.shipment_shogun_flash
   \d stg.yard_shogun_flash
   ```

3. **Compare RAW vs STG Model**:

   - Check if column definitions match
   - Verify data types align

4. **Debug bulk_save_objects**:
   - Add logging before bulk insert
   - Check which columns SQLAlchemy is mapping

---

## Proposed Fix

### Option 1: Fix create_shogun_model_class()

Ensure the function generates complete column definitions for STG schema tables.

### Option 2: Fix Table Schema Reflection

If using reflection, refresh metadata or fix schema definition source.

### Option 3: Explicit Column Mapping

Add explicit column validation before ORM object creation:

```python
# Verify model has all required columns
model_columns = set(model_class.__table__.columns.keys())
payload_columns = set(payload.keys())
missing_in_model = payload_columns - model_columns

if missing_in_model:
    logger.warning(f"Model missing columns: {missing_in_model}")
```

---

## Files to Investigate

1. `app/infra/db/dynamic_models.py` - ORM model generation
2. `app/infra/db/table_definition.py` - Table schema definitions
3. Database schema - STG table actual structure
4. `app/infra/adapters/upload/shogun_csv_repository.py` - Current file

---

## Impact Assessment

**Severity**: HIGH - Blocking feature
**Affected CSV Types**: Yard, Shipment
**Working CSV Types**: Receive
**Affected Layers**: STG only (RAW works)

**Why Receive Works**:
Need to investigate if receive CSV has different schema or if there's something special about its column definitions.
