"""
UseCase: UploadShogunCsvUseCase

å°†è»CSVï¼ˆreceive/yard/shipmentï¼‰ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’æ‹…å½“ã€‚
ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦ã€Œä½•ã‚’ãƒ»ã©ã®é †ã§ã€è¡Œã†ã‹ã‚’è¨˜è¿°ã€‚

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
  1. CSVèª­è¾¼ï¼ˆDataFrameåŒ–ï¼‰
  2. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚«ãƒ©ãƒ ã€æ—¥ä»˜ã®ä¸€è²«æ€§ï¼‰
  3. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå‹å¤‰æ›ã€æ­£è¦åŒ–ï¼‰
  4. DBä¿å­˜ï¼ˆAdapterçµŒç”±ï¼‰
  5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ

è¨­è¨ˆæ–¹é‡:
  - ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã¯ã“ã“ã«é›†ç´„ï¼ˆRouter ã«ã¯æ›¸ã‹ãªã„ï¼‰
  - å¤–éƒ¨ä¾å­˜ï¼ˆDBã€ãƒ•ã‚¡ã‚¤ãƒ«ç­‰ï¼‰ã¯ Port çµŒç”±ã§æ³¨å…¥
  - backend_shared ã®æ—¢å­˜ validator/formatter ã‚’æ´»ç”¨
  - éåŒæœŸã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ: start_async_upload ã§å—ä»˜ã®ã¿è¡Œã„ã€é‡ã„å‡¦ç†ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¸
"""
import logging
from typing import Dict, Optional, List, Any
from fastapi import UploadFile, BackgroundTasks
from fastapi.concurrency import run_in_threadpool
import pandas as pd

from backend_shared.config.config_loader import ShogunCsvConfigLoader
from backend_shared.core.usecases.csv_validator.csv_upload_validator_api import CSVValidationResponder
from backend_shared.core.usecases.csv_formatter.formatter_factory import CSVFormatterFactory
from backend_shared.core.usecases.csv_formatter.formatter_config import build_formatter_config
from backend_shared.infra.adapters.presentation import SuccessApiResponse, ErrorApiResponse

from app.core.ports.csv_writer_port import IShogunCsvWriter
from app.infra.adapters.upload.raw_data_repository import RawDataRepository
from app.infra.adapters.materialized_view.materialized_view_refresher import MaterializedViewRefresher

logger = logging.getLogger(__name__)


class UploadShogunCsvUseCase:
    """å°†è»CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®UseCase"""
    
    def __init__(
        self,
        raw_writer: IShogunCsvWriter,
        stg_writer: IShogunCsvWriter,
        csv_config: ShogunCsvConfigLoader,
        validator: CSVValidationResponder,
        raw_data_repo: Optional[RawDataRepository] = None,
        mv_refresher: Optional[MaterializedViewRefresher] = None,
    ):
        """
        Args:
            raw_writer: rawå±¤ã¸ã®CSVä¿å­˜Portå®Ÿè£…
            stg_writer: stgå±¤ã¸ã®CSVä¿å­˜Portå®Ÿè£…
            csv_config: CSVè¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼
            validator: CSVãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼
            raw_data_repo: log.upload_file ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ­ã‚°ä¿å­˜ç”¨ãƒªãƒã‚¸ãƒˆãƒª
            mv_refresher: ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼æ›´æ–°ç”¨ãƒªãƒã‚¸ãƒˆãƒªï¼ˆOptionalï¼‰
        """
        self.raw_writer = raw_writer
        self.stg_writer = stg_writer
        self.csv_config = csv_config
        self.validator = validator
        self.raw_data_repo = raw_data_repo
        self.mv_refresher = mv_refresher
    
    async def start_async_upload(
        self,
        background_tasks: BackgroundTasks,
        receive: Optional[UploadFile],
        yard: Optional[UploadFile],
        shipment: Optional[UploadFile],
        file_type: str = "FLASH",
        uploaded_by: Optional[str] = None,
    ) -> SuccessApiResponse | ErrorApiResponse:
        """
        éåŒæœŸã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ï¼ˆè»½é‡ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³â†’å³åº§ã«å—ä»˜å®Œäº†ãƒ¬ã‚¹ãƒãƒ³ã‚¹â†’é‡ã„å‡¦ç†ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
        
        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
          1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ‹¡å¼µå­ã€MIME typeï¼‰
          2. é‡è¤‡ãƒã‚§ãƒƒã‚¯ + log.upload_file ã¸ã®ç™»éŒ²ï¼ˆpendingçŠ¶æ…‹ï¼‰
             - ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã«ã‚ˆã‚Šã€åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®é€£ç¶šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’æ¤œçŸ¥
             - 3åˆ†ä»¥å†…ã®é‡è¤‡ã¯409ã‚¨ãƒ©ãƒ¼ã§æ‹’å¦ï¼ˆUXå‘ä¸Šï¼‰
          3. ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã«ç™»éŒ²
          4. å³åº§ã«å—ä»˜å®Œäº†ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ï¼ˆupload_file_ids ã‚’å«ã‚€ï¼‰
        
        ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†:
          - CSVèª­è¾¼ â†’ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ â†’ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ â†’ DBä¿å­˜ï¼ˆraw/stgå±¤ï¼‰
          - ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼ã®è‡ªå‹•æ›´æ–°ï¼ˆMVP: Most Valuable Processï¼‰
          - ã‚¨ãƒ©ãƒ¼æ™‚ã¯ log.upload_file.status = 'error' ã«æ›´æ–°
        
        Args:
            background_tasks: FastAPI BackgroundTasks
            receive: å—å…¥ä¸€è¦§CSV
            yard: ãƒ¤ãƒ¼ãƒ‰ä¸€è¦§CSV
            shipment: å‡ºè·ä¸€è¦§CSV
            file_type: 'FLASH' ã¾ãŸã¯ 'FINAL'ï¼ˆé€Ÿå ±å€¤ or ç¢ºå®šå€¤ï¼‰
            uploaded_by: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆãƒ­ã‚°è¨˜éŒ²ç”¨ï¼‰
        
        Returns:
            å—ä»˜æˆåŠŸ: upload_file_ids ã‚’å«ã‚€ SuccessApiResponseï¼ˆå³åº§ï¼‰
            ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: ErrorApiResponse
        """
        # é–‹å§‹ãƒ­ã‚°: æ§‹é€ åŒ–æƒ…å ±ã‚’å«ã‚ã‚‹
        logger.info(
            "CSV upload started (async)",
            extra={
                "operation": "shogun_csv_upload",
                "file_type": file_type,
                "uploaded_by": uploaded_by or "anonymous",
                "csv_types": [k for k, v in {"receive": receive, "yard": yard, "shipment": shipment}.items() if v and v.filename],
            }
        )
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´ç†
        file_inputs = {"receive": receive, "yard": yard, "shipment": shipment}
        uploaded_files = {k: v for k, v in file_inputs.items() if v and v.filename}
        
        if not uploaded_files:
            logger.warning("CSV upload rejected: no files provided")
            return ErrorApiResponse(
                code="NO_FILES",
                detail="å°‘ãªãã¨ã‚‚1ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
                status_code=400
            )
        
        # 1. è»½é‡ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ‹¡å¼µå­ã€MIME typeï¼‰
        validation_error = self._validate_file_types(uploaded_files)
        if validation_error:
            return validation_error
        
        # 2. é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‹log.upload_fileã¸ã®ç™»éŒ²ï¼ˆpendingçŠ¶æ…‹ï¼‰
        upload_file_ids: Dict[str, int] = {}
        duplicate_files: Dict[str, Dict[str, Any]] = {}
        recent_duplicate_files: Dict[str, Dict[str, Any]] = {}
        
        if self.raw_data_repo:
            for csv_type, uf in uploaded_files.items():
                try:
                    content = await uf.read()
                    file_hash = self.raw_data_repo.calculate_file_hash(content)
                    uf.file.seek(0)
                    
                    # çŸ­æ™‚é–“ã®é€£ç¶šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆUXç”¨ï¼‰
                    is_recent_dup = self.raw_data_repo.is_recent_duplicate_upload(
                        file_hash=file_hash,
                        csv_type=csv_type,
                        uploaded_by=uploaded_by or "anonymous",
                        minutes=3,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3åˆ†ä»¥å†…ã®é€£ç¶šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’æ¤œçŸ¥
                    )
                    
                    if is_recent_dup:
                        recent_duplicate_files[csv_type] = {
                            "file_name": uf.filename,
                            "file_hash": file_hash[:8] + "...",
                            "reason": "åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒç›´å‰ï¼ˆ3åˆ†ä»¥å†…ï¼‰ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã™",
                        }
                        logger.warning(f"Recent duplicate detected: {csv_type}")
                        continue
                    
                    # é‡è¤‡ã§ãªã„å ´åˆã®ã¿ãƒ­ã‚°ä½œæˆ
                    file_id = self.raw_data_repo.create_upload_file(
                        csv_type=csv_type,
                        file_name=uf.filename or f"{csv_type}.csv",
                        file_hash=file_hash,
                        file_type=file_type,
                        file_size_bytes=len(content),
                        uploaded_by=uploaded_by,
                    )
                    upload_file_ids[csv_type] = file_id
                    logger.info(f"Created upload_file entry (pending): {csv_type} id={file_id}")
                except Exception as e:
                    logger.error(f"Failed to process {csv_type}: {e}")
                    return ErrorApiResponse(
                        code="UPLOAD_PREPARATION_ERROR",
                        detail=f"{csv_type}ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                        status_code=500,
                    )
        
        # çŸ­æ™‚é–“ã®é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯409ã‚’è¿”ã™
        # UXå‘ä¸Š: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª¤ã£ã¦åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é€£ç¶šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’é˜²ã
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3åˆ†ä»¥å†…ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆraw_data_repo.is_recent_duplicate_uploadï¼‰
        if recent_duplicate_files:
            duplicate_details = []
            for csv_type, info in recent_duplicate_files.items():
                duplicate_details.append({
                    "csv_type": csv_type,
                    "file_name": info["file_name"],
                    "reason": info["reason"],
                })
            
            return ErrorApiResponse(
                code="DUPLICATE_FILE_RECENT",
                detail=f"ç›´å‰ã«åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã™: {', '.join(recent_duplicate_files.keys())}",
                status_code=409,
                result={"duplicates": duplicate_details},
                hint="é€£ç¶šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å°‘ã—æ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
            )
        
        # 3. ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã«é‡ã„å‡¦ç†ã‚’ç™»éŒ²
        #    UploadFileã¯ãƒ¡ãƒ¢ãƒªä¸Šã«ãƒãƒƒãƒ•ã‚¡ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚‚èª­ã¿å–ã‚Šå¯èƒ½
        #    ãŸã ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ bytes ã§ä¿æŒã—ã¦ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã«æ¸¡ã™
        file_contents = {}
        for csv_type, uf in uploaded_files.items():
            uf.file.seek(0)
            file_contents[csv_type] = {
                "content": await uf.read(),
                "filename": uf.filename,
            }
        
        background_tasks.add_task(
            self._process_csv_in_background,
            file_contents=file_contents,
            upload_file_ids=upload_file_ids,
            file_type=file_type,
            uploaded_by=uploaded_by,
        )
        
        # 4. å³åº§ã«å—ä»˜å®Œäº†ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        logger.info(
            "CSV upload accepted",
            extra={
                "operation": "shogun_csv_upload",
                "file_type": file_type,
                "uploaded_by": uploaded_by or "anonymous",
                "upload_file_ids": upload_file_ids,
                "csv_types": list(upload_file_ids.keys()),
            }
        )
        
        return SuccessApiResponse(
            code="UPLOAD_ACCEPTED",
            detail=f"CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸã€‚å‡¦ç†ä¸­ã§ã™ã€‚",
            result={
                "upload_file_ids": upload_file_ids,
                "status": "processing",
            },
            hint="å‡¦ç†å®Œäº†ã¾ã§æ•°ç§’ï½æ•°åç§’ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚",
        )
    
    async def _process_csv_in_background(
        self,
        file_contents: Dict[str, Dict[str, Any]],
        upload_file_ids: Dict[str, int],
        file_type: str,
        uploaded_by: Optional[str] = None,
    ) -> None:
        """
        ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã‚‹é‡ã„å‡¦ç†
        
        Args:
            file_contents: csv_type -> {"content": bytes, "filename": str}
            upload_file_ids: csv_type -> upload_file.id
            file_type: 'FLASH' or 'FINAL'
            uploaded_by: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼å
        """
        import time
        start_time = time.time()
        
        try:
            logger.info(
                "CSV background processing started",
                extra={
                    "operation": "csv_background_processing",
                    "upload_file_ids": upload_file_ids,
                    "file_type": file_type,
                    "uploaded_by": uploaded_by or "anonymous",
                }
            )
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ processing ã«æ›´æ–°
            if self.raw_data_repo:
                for csv_type, file_id in upload_file_ids.items():
                    self.raw_data_repo.update_upload_status(file_id=file_id, status="processing")
            
            # CSVèª­è¾¼
            import io
            dfs: Dict[str, pd.DataFrame] = {}
            for csv_type, file_info in file_contents.items():
                try:
                    content_io = io.BytesIO(file_info["content"])
                    df = pd.read_csv(content_io, encoding="utf-8")
                    dfs[csv_type] = df
                    logger.info(f"[BG] Loaded {csv_type}: {len(df)} rows")
                except Exception as e:
                    logger.error(f"[BG] Failed to parse {csv_type}: {e}")
                    self._mark_all_as_failed(upload_file_ids, f"CSV parse error: {csv_type}")
                    return
            
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã®ãƒ€ãƒŸãƒ¼UploadFileä½œæˆï¼‰
            dummy_files = {}
            for csv_type, file_info in file_contents.items():
                class DummyUploadFile:
                    def __init__(self, filename):
                        self.filename = filename
                dummy_files[csv_type] = DummyUploadFile(file_info["filename"])
            
            validation_error = self._validate_csv_data(dfs, dummy_files)
            if validation_error:
                logger.error(f"[BG] Validation failed: {validation_error.detail}")
                self._mark_all_as_failed(upload_file_ids, f"Validation error: {validation_error.detail}")
                return
            
            # source_row_no è¿½åŠ 
            dfs_with_row_no = self._add_source_row_numbers(dfs)
            
            # rawå±¤ä¿å­˜
            raw_cleaned_dfs = await self._clean_empty_rows(dfs_with_row_no)
            raw_result = await self._save_data(
                self.raw_writer, raw_cleaned_dfs, dummy_files, "raw", upload_file_ids
            )
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_dfs, format_error = await self._format_csv_data(dfs_with_row_no)
            if format_error:
                logger.error(f"[BG] Format failed: {format_error.detail}")
                self._mark_all_as_failed(upload_file_ids, f"Format error: {format_error.detail}")
                return
            
            # â˜… stgå±¤ä¿å­˜å‰ã«æ—¢å­˜æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã‚’è«–ç†å‰Šé™¤ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³A: åŒä¸€æ—¥ä»˜ï¼‹ç¨®åˆ¥ã¯æœ€å¾Œã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã ã‘æœ‰åŠ¹ï¼‰
            if self.raw_data_repo:
                deleted_by = uploaded_by or "system_auto_replace"
                self._soft_delete_existing_data_by_dates(formatted_dfs, file_type, deleted_by)
            
            # stgå±¤ä¿å­˜
            stg_result = await self._save_data(
                self.stg_writer, formatted_dfs, dummy_files, "stg", upload_file_ids
            )
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            self._update_upload_logs(upload_file_ids, formatted_dfs, stg_result)
            
            # å®Œäº†ãƒ­ã‚°: å‡¦ç†æ™‚é–“ã€ä»¶æ•°ã‚’è¨˜éŒ²
            duration_ms = int((time.time() - start_time) * 1000)
            total_rows = sum(len(df) for df in formatted_dfs.values())
            
            logger.info(
                "CSV background processing completed",
                extra={
                    "operation": "csv_background_processing",
                    "upload_file_ids": upload_file_ids,
                    "csv_types": list(upload_file_ids.keys()),
                    "total_rows": total_rows,
                    "duration_ms": duration_ms,
                    "file_type": file_type,
                }
            )
            
        except Exception as e:
            logger.exception(
                "CSV background processing failed",
                extra={
                    "operation": "csv_background_processing",
                    "upload_file_ids": upload_file_ids,
                    "error": str(e),
                }
            )
            self._mark_all_as_failed(upload_file_ids, f"Internal error: {str(e)}")
    
    async def execute(
        self,
        receive: Optional[UploadFile],
        yard: Optional[UploadFile],
        shipment: Optional[UploadFile],
        file_type: str = "FLASH",  # 'FLASH' or 'FINAL'
        uploaded_by: Optional[str] = None,
    ) -> SuccessApiResponse | ErrorApiResponse:
        """
        å°†è»CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’å®Ÿè¡Œ
        
        Args:
            receive: å—å…¥ä¸€è¦§CSV
            yard: ãƒ¤ãƒ¼ãƒ‰ä¸€è¦§CSV
            shipment: å‡ºè·ä¸€è¦§CSV
            file_type: 'FLASH' ã¾ãŸã¯ 'FINAL'
            uploaded_by: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼å
        
        Returns:
            SuccessApiResponse ã¾ãŸã¯ ErrorApiResponse
        """
        logger.info("Start shogun CSV upload usecase")
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´ç†
        file_inputs = {"receive": receive, "yard": yard, "shipment": shipment}
        uploaded_files = {k: v for k, v in file_inputs.items() if v and v.filename}
        
        if not uploaded_files:
            return ErrorApiResponse(
                code="NO_FILES",
                detail="å°‘ãªãã¨ã‚‚1ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
                status_code=400
            )
        
        # log.upload_file ã¸ã®ç™»éŒ²å‰ã«é‡è¤‡ãƒã‚§ãƒƒã‚¯
        duplicate_files: Dict[str, Dict[str, Any]] = {}
        if self.raw_data_repo:
            for csv_type, uf in uploaded_files.items():
                try:
                    content = await uf.read()
                    file_hash = self.raw_data_repo.calculate_file_hash(content)
                    uf.file.seek(0)  # readå¾Œã¯ã‚·ãƒ¼ã‚¯ã‚’æˆ»ã™
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæˆåŠŸæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
                    duplicate_info = self.raw_data_repo.check_duplicate_upload(
                        csv_type=csv_type,
                        file_hash=file_hash,
                        file_type=file_type,
                        file_name=uf.filename,
                        file_size_bytes=len(content),
                        # row_count ã¯ CSVèª­è¾¼å¾Œã«åˆ¤æ˜ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ None
                    )
                    
                    if duplicate_info:
                        duplicate_files[csv_type] = duplicate_info
                        logger.warning(
                            f"Duplicate file detected: {csv_type} - "
                            f"existing_id={duplicate_info['id']}, "
                            f"match_type={duplicate_info['match_type']}"
                        )
                except Exception as e:
                    logger.error(f"Failed to check duplicate for {csv_type}: {e}")
        
        # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€409 Conflict ã‚’è¿”ã™
        if duplicate_files:
            duplicate_details = []
            for csv_type, info in duplicate_files.items():
                duplicate_details.append({
                    "csv_type": csv_type,
                    "file_name": info["file_name"],
                    "uploaded_at": info["uploaded_at"].isoformat() if info.get("uploaded_at") else None,
                    "uploaded_by": info.get("uploaded_by"),
                    "match_type": info["match_type"],
                })
            
            return ErrorApiResponse(
                code="DUPLICATE_FILE",
                detail=f"åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™: {', '.join(duplicate_files.keys())}",
                status_code=409,
                result={"duplicates": duplicate_details}
            )
        
        # log.upload_file ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ­ã‚°ã‚’ä½œæˆï¼ˆcsv_type å˜ä½ï¼‰
        upload_file_ids: Dict[str, int] = {}
        if self.raw_data_repo:
            for csv_type, uf in uploaded_files.items():
                try:
                    content = await uf.read()
                    file_hash = self.raw_data_repo.calculate_file_hash(content)
                    uf.file.seek(0)  # readå¾Œã¯ã‚·ãƒ¼ã‚¯ã‚’æˆ»ã™
                    
                    file_id = self.raw_data_repo.create_upload_file(
                        csv_type=csv_type,
                        file_name=uf.filename or f"{csv_type}.csv",
                        file_hash=file_hash,
                        file_type=file_type,
                        file_size_bytes=len(content),
                        uploaded_by=uploaded_by,
                    )
                    upload_file_ids[csv_type] = file_id
                    logger.info(f"Created log.upload_file entry for {csv_type}: id={file_id}")
                except Exception as e:
                    logger.error(f"Failed to create upload log for {csv_type}: {e}")
                    # ãƒ­ã‚°ä½œæˆå¤±æ•—ã§ã‚‚å‡¦ç†ã¯ç¶™ç¶šï¼ˆãƒ­ã‚°ã¯è£œåŠ©çš„ãªã‚‚ã®ï¼‰
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—æ¤œè¨¼ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰
        validation_error = self._validate_file_types(uploaded_files)
        if validation_error:
            self._mark_all_as_failed(upload_file_ids, "File type validation failed")
            return validation_error
        
        # 2. CSVèª­è¾¼
        dfs, read_error = await self._read_csv_files(uploaded_files)
        if read_error:
            self._mark_all_as_failed(upload_file_ids, "CSV parse error")
            return read_error
        
        # 3. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        validation_error = self._validate_csv_data(dfs, uploaded_files)
        if validation_error:
            self._mark_all_as_failed(upload_file_ids, "CSV validation failed")
            return validation_error
        
        # 4. source_row_no ã®æ¡ç•ª(1-indexed)
        dfs_with_row_no = self._add_source_row_numbers(dfs)
        logger.info(f"[TRACE] dfs_with_row_no has {len(dfs_with_row_no)} items: {list(dfs_with_row_no.keys())}")
        
        # 5. rawå±¤ã¸ã®ä¿å­˜(ç”Ÿãƒ‡ãƒ¼ã‚¿ = ç©ºè¡Œå‰Šé™¤ã®ã¿ã€æ—¥æœ¬èªã‚«ãƒ©ãƒ åã®ã¾ã¾)
        raw_cleaned_dfs = await self._clean_empty_rows(dfs_with_row_no)
        logger.info(f"[TRACE] raw_cleaned_dfs has {len(raw_cleaned_dfs)} items: {list(raw_cleaned_dfs.keys())}")
        raw_result = await self._save_data(
            self.raw_writer, raw_cleaned_dfs, uploaded_files, "raw", upload_file_ids
        )
        
        # 6. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆstgå±¤ç”¨ï¼‰
        formatted_dfs, format_error = await self._format_csv_data(dfs_with_row_no)
        if format_error:
            self._mark_all_as_failed(upload_file_ids, "CSV format error")
            return format_error
        
        # 7. stgå±¤ä¿å­˜å‰ã«æ—¢å­˜æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã‚’è«–ç†å‰Šé™¤ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³A: åŒä¸€æ—¥ä»˜ï¼‹ç¨®åˆ¥ã¯æœ€å¾Œã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã ã‘æœ‰åŠ¹ï¼‰
        if self.raw_data_repo:
            deleted_by = uploaded_by or "system_auto_replace"
            self._soft_delete_existing_data_by_dates(formatted_dfs, file_type, deleted_by)
        
        # 8. stgå±¤ã¸ã®ä¿å­˜ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ = è‹±èªã‚«ãƒ©ãƒ åã€å‹å¤‰æ›æ¸ˆã¿ï¼‰
        stg_result = await self._save_data(
            self.stg_writer, formatted_dfs, uploaded_files, "stg", upload_file_ids
        )
        
        # 9. log.upload_file ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
        self._update_upload_logs(upload_file_ids, formatted_dfs, stg_result)
        
        # 10. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆï¼ˆraw + stg ä¸¡æ–¹ã®çµæœã‚’çµ±åˆï¼‰
        return self._generate_response(raw_result, stg_result)
    
    def _validate_file_types(self, uploaded_files: Dict[str, UploadFile]) -> Optional[ErrorApiResponse]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®æ¤œè¨¼ï¼ˆMIME type + æ‹¡å¼µå­ï¼‰"""
        ALLOWED_CT = {"text/csv", "application/vnd.ms-excel"}
        
        for k, f in uploaded_files.items():
            if not f.filename:
                continue
            name = f.filename.lower()
            if not (name.endswith(".csv") or (f.content_type and f.content_type in ALLOWED_CT)):
                return ErrorApiResponse(
                    code="INVALID_FILE_TYPE",
                    detail=f"{k}: CSVãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆæ‹¡å¼µå­ã¾ãŸã¯MIMEã‚¿ã‚¤ãƒ—ã‚’ç¢ºèªï¼‰",
                    status_code=400,
                )
        return None
    
    async def _read_csv_files(
        self, uploaded_files: Dict[str, UploadFile]
    ) -> tuple[Dict[str, pd.DataFrame], Optional[ErrorApiResponse]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’DataFrameã«èª­ã¿è¾¼ã¿"""
        dfs: Dict[str, pd.DataFrame] = {}
        
        async def _read_one(uf: UploadFile) -> pd.DataFrame:
            """pandas.read_csv ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼ˆasyncã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰"""
            uf.file.seek(0)
            def _read_csv(bio):
                return pd.read_csv(bio, encoding="utf-8")
            return await run_in_threadpool(_read_csv, uf.file)
        
        for csv_type, uf in uploaded_files.items():
            try:
                df = await _read_one(uf)
                dfs[csv_type] = df
                logger.info(f"Loaded {csv_type} CSV: {len(df)} rows, {len(df.columns)} cols")
            except Exception as e:
                logger.error(f"Failed to parse {csv_type} CSV: {e}")
                return {}, ErrorApiResponse(
                    code="CSV_PARSE_ERROR",
                    detail=f"{csv_type}ã®CSVã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {str(e)}",
                    status_code=400
                )
        
        return dfs, None
    
    def _add_source_row_numbers(self, dfs: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        å„DataFrameã« source_row_no ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆ1-indexedï¼‰
        
        Args:
            dfs: csv_type -> DataFrame ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            
        Returns:
            source_row_no ã‚«ãƒ©ãƒ ãŒè¿½åŠ ã•ã‚ŒãŸ DataFrame ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        """
        result_dfs = {}
        for csv_type, df in dfs.items():
            df_copy = df.copy()
            # 1-indexed ã§è¡Œç•ªå·ã‚’ä»˜ä¸ï¼ˆCSVã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®æ¬¡ãŒ1è¡Œç›®ï¼‰
            df_copy['source_row_no'] = range(1, len(df_copy) + 1)
            result_dfs[csv_type] = df_copy
            logger.debug(f"Added source_row_no to {csv_type}: 1 to {len(df_copy)}")
        
        return result_dfs
    
    def _validate_csv_data(
        self, dfs: Dict[str, pd.DataFrame], uploaded_files: Dict[str, UploadFile]
    ) -> Optional[ErrorApiResponse]:
        """CSVãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        # ã‚«ãƒ©ãƒ æ¤œè¨¼
        validation_error = self.validator.validate_columns(dfs, uploaded_files)
        if validation_error:
            return validation_error
        
        # æ—¥ä»˜å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        date_exists_error = self.validator.validate_denpyou_date_exists(dfs, uploaded_files)
        if date_exists_error:
            return date_exists_error
        
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€æ—¥ä»˜ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        if len(dfs) > 1:
            date_consistency_error = self.validator.validate_denpyou_date_consistency(dfs)
            if date_consistency_error:
                return date_consistency_error
        
        return None
    
    async def _clean_empty_rows(
        self, dfs: Dict[str, pd.DataFrame]
    ) -> Dict[str, pd.DataFrame]:
        """
        ç©ºè¡Œé™¤å»ã®ã¿ï¼ˆrawå±¤ä¿å­˜ç”¨ï¼‰
        æ—¥æœ¬èªã‚«ãƒ©ãƒ åã®ã¾ã¾ã€å‹å¤‰æ›ãªã—
        """
        cleaned_dfs: Dict[str, pd.DataFrame] = {}
        
        for csv_type, df in dfs.items():
            original_row_count = len(df)
            df_cleaned = df.dropna(how='all')
            empty_rows_removed = original_row_count - len(df_cleaned)
            
            if empty_rows_removed > 0:
                logger.info(f"[raw] Removed {empty_rows_removed} empty rows from {csv_type} CSV")
            
            cleaned_dfs[csv_type] = df_cleaned
        
        return cleaned_dfs
    
    async def _format_csv_data(
        self, dfs: Dict[str, pd.DataFrame]
    ) -> tuple[Dict[str, pd.DataFrame], Optional[ErrorApiResponse]]:
        """
        CSVãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆstgå±¤ä¿å­˜ç”¨ï¼‰
        - ç©ºè¡Œé™¤å»
        - ã‚«ãƒ©ãƒ åã‚’è‹±èªã«å¤‰æ›
        - å‹å¤‰æ›ã€æ­£è¦åŒ–
        - tracking columns (upload_file_id, source_row_no) ã‚’ä¿æŒ
        """
        formatted_dfs: Dict[str, pd.DataFrame] = {}
        
        # tracking columns ã®ã‚«ãƒ©ãƒ åå®šç¾©ï¼ˆä¿å®ˆæ€§å‘ä¸Šã®ãŸã‚å®šæ•°åŒ–ï¼‰
        TRACKING_COLUMNS = ['upload_file_id', 'source_row_no']
        
        for csv_type, df in dfs.items():
            try:
                logger.info(f"[DEBUG] Starting format for {csv_type}: {len(df)} rows, columns={list(df.columns)[:10]}...")
                
                # 1. tracking columns ã‚’ä¸€æ™‚ä¿å­˜ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
                tracking_data = {}
                for col in TRACKING_COLUMNS:
                    if col in df.columns:
                        tracking_data[col] = df[col].copy()
                        logger.info(f"[DEBUG] Preserved tracking column '{col}' for {csv_type}: sample values={df[col].head(3).tolist()}")
                
                # 2. ç©ºè¡Œé™¤å»ï¼ˆå…¨ã‚«ãƒ©ãƒ ãŒNULLã®è¡Œã‚’å‰Šé™¤ï¼‰
                original_row_count = len(df)
                df_cleaned = df.dropna(how='all')
                empty_rows_removed = original_row_count - len(df_cleaned)
                
                if empty_rows_removed > 0:
                    logger.info(f"[stg] Removed {empty_rows_removed} empty rows from {csv_type} CSV")
                    
                    # ç©ºè¡Œå‰Šé™¤å¾Œã€tracking columns ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚èª¿æ•´
                    for col, data in tracking_data.items():
                        tracking_data[col] = data.loc[df_cleaned.index]
                
                # 3. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ï¼ˆæ¥­å‹™ã‚«ãƒ©ãƒ ã®å¤‰æ›ï¼‰
                config = build_formatter_config(self.csv_config, csv_type)
                formatter = CSVFormatterFactory.get_formatter(csv_type, config)
                logger.info(f"[DEBUG] Before formatter for {csv_type}: {len(df_cleaned)} rows, columns={list(df_cleaned.columns)[:10]}...")
                # CPU-bound operation: åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
                formatted_df = await run_in_threadpool(formatter.format, df_cleaned)
                logger.info(f"[DEBUG] After formatter for {csv_type}: {len(formatted_df)} rows, columns={list(formatted_df.columns)[:10]}...")
                
                # 4. tracking columns ã‚’å¾©å…ƒï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¾Œã®DataFrameã«å†çµåˆï¼‰
                for col, data in tracking_data.items():
                    if len(formatted_df) == len(data):
                        # è¡Œæ•°ãŒä¸€è‡´ã™ã‚‹å ´åˆã¯ãã®ã¾ã¾ä»£å…¥
                        formatted_df[col] = data.values
                        logger.info(f"[DEBUG] Restored tracking column '{col}' to formatted {csv_type}: sample values={formatted_df[col].head(3).tolist()}")
                    else:
                        # è¡Œæ•°ãŒä¸ä¸€è‡´ã®å ´åˆã¯è­¦å‘Šï¼ˆé€šå¸¸ã¯ç™ºç”Ÿã—ãªã„ã¯ãšï¼‰
                        logger.error(
                            f"[DEBUG] Row count mismatch for {csv_type}: "
                            f"formatted={len(formatted_df)}, tracking={len(data)}. "
                            f"Skipping restoration of '{col}'"
                        )
                
                formatted_dfs[csv_type] = formatted_df
                logger.info(f"[stg] Formatted {csv_type}: {len(formatted_df)} rows")
                logger.info(f"[DEBUG] Final columns for {csv_type}: {list(formatted_df.columns)}")
                logger.info(f"[DEBUG] Has upload_file_id: {'upload_file_id' in formatted_df.columns}, Has source_row_no: {'source_row_no' in formatted_df.columns}")
            except Exception as e:
                logger.error(f"Failed to format {csv_type}: {e}", exc_info=True)
                return {}, ErrorApiResponse(
                    code="FORMAT_ERROR",
                    detail=f"{csv_type}ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤±æ•—: {str(e)}",
                    status_code=400,
                    hint="ã‚«ãƒ©ãƒ åã‚„å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                )
        
        return formatted_dfs, None
    
    async def _save_data(
        self, 
        writer: IShogunCsvWriter,
        formatted_dfs: Dict[str, pd.DataFrame], 
        uploaded_files: Dict[str, UploadFile],
        layer_name: str,
        upload_file_ids: Optional[Dict[str, int]] = None,
    ) -> Dict[str, dict]:
        """
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿CSVãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜ï¼ˆrawå±¤ or stgå±¤ï¼‰
        
        Args:
            writer: CSVä¿å­˜ç”¨ã®writer (raw_writer or stg_writer)
            formatted_dfs: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿DataFrame
            uploaded_files: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            layer_name: ãƒ¬ã‚¤ãƒ¤ãƒ¼å ("raw" or "stg")
            upload_file_ids: log.upload_file.id ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆcsv_type -> file_idï¼‰
        
        Returns:
            ä¿å­˜çµæœã®è¾æ›¸
        """
        result: Dict[str, dict] = {}
        
        for csv_type, df in formatted_dfs.items():
            try:
                logger.info(f"[DEBUG] Saving {csv_type} to {layer_name}: {len(df)} rows, columns={list(df.columns)[:15]}")
                
                # upload_file_id ã‚’ DataFrame ã«è¿½åŠ 
                df_to_save = df.copy()
                if upload_file_ids and csv_type in upload_file_ids:
                    file_id = upload_file_ids[csv_type]
                    df_to_save['upload_file_id'] = file_id
                    logger.info(f"[DEBUG] Added upload_file_id={file_id} to {csv_type} ({layer_name}), now columns={list(df_to_save.columns)[:15]}")
                
                logger.info(f"[DEBUG] Before save {layer_name}: upload_file_id={'upload_file_id' in df_to_save.columns}, source_row_no={'source_row_no' in df_to_save.columns}")
                logger.info(f"[DEBUG] Sample row for {csv_type}: {df_to_save.head(1).to_dict('records')}")
                
                saved_count = await run_in_threadpool(writer.save_csv_by_type, csv_type, df_to_save)
                result[csv_type] = {
                    "filename": uploaded_files[csv_type].filename,
                    "status": "success",
                    "rows_saved": saved_count,
                }
                logger.info(f"Saved {csv_type} to {layer_name} layer: {saved_count} rows")
            except Exception as e:
                logger.error(f"Failed to save {csv_type} to {layer_name} layer: {e}", exc_info=True)
                result[csv_type] = {
                    "filename": uploaded_files[csv_type].filename,
                    "status": "error",
                    "detail": str(e),
                }
        
        return result
    
    def _mark_all_as_failed(self, upload_file_ids: Dict[str, int], error_msg: str) -> None:
        """å…¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ­ã‚°ã‚’å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        if not self.raw_data_repo:
            return
        for csv_type, file_id in upload_file_ids.items():
            try:
                self.raw_data_repo.update_upload_status(
                    file_id=file_id,
                    status="failed",
                    error_message=error_msg,
                )
            except Exception as e:
                logger.error(f"Failed to update upload log for {csv_type}: {e}")
    
    def _soft_delete_existing_data_by_dates(
        self,
        formatted_dfs: Dict[str, pd.DataFrame],
        file_type: str,
        deleted_by: str,
    ) -> None:
        """
        ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‰ã«æ—¢å­˜ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã‚’è«–ç†å‰Šé™¤ã™ã‚‹ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³Aç”¨ï¼‰
        
        åŒä¸€æ—¥ä»˜ï¼‹ç¨®åˆ¥ã¯æœ€å¾Œã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã ã‘æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚ã€
        æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ã™ã‚‹å‰ã«ã€è©²å½“ã™ã‚‹æ—¥ä»˜ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã™ã¹ã¦ is_deleted=true ã«ã™ã‚‹ã€‚
        
        Args:
            formatted_dfs: csv_type -> DataFrame ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ï¼‰
            file_type: 'FLASH' or 'FINAL'
            deleted_by: å‰Šé™¤å®Ÿè¡Œè€…ï¼ˆä¾‹: 'system_auto_replace' ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼‰
        """
        if not self.raw_data_repo:
            logger.warning("[SOFT_DELETE] âš ï¸ RawDataRepository not available, skipping soft delete")
            return
        
        logger.info(f"[SOFT_DELETE] ğŸ“‹ Starting soft delete for {len(formatted_dfs)} CSV types, file_type={file_type}")
        
        # csv_type -> csv_kind ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆfile_type ã‚’è€ƒæ…®ï¼‰
        csv_type_to_kind_map = {
            "receive": f"shogun_{file_type.lower()}_receive",
            "yard": f"shogun_{file_type.lower()}_yard",
            "shipment": f"shogun_{file_type.lower()}_shipment",
        }
        
        for csv_type, df in formatted_dfs.items():
            try:
                # [FIX] YAMLè¨­å®šã‹ã‚‰æ—¥ä»˜ã‚«ãƒ©ãƒ åã‚’å‹•çš„ã«å–å¾—
                csv_config = self.csv_config.config.get(csv_type, {})
                if not csv_config:
                    logger.warning(f"[SOFT_DELETE] âš ï¸ No config found for {csv_type}, skipping soft delete")
                    continue
                
                # æ—¥æœ¬èªã‚«ãƒ©ãƒ åã¨è‹±èªã‚«ãƒ©ãƒ åã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
                date_col_ja = csv_config.get("soft_delete_date_column")  # ä¾‹: "ä¼ç¥¨æ—¥ä»˜"
                date_col_en = csv_config.get("soft_delete_date_column_en")  # ä¾‹: "slip_date"
                
                slip_date_col = None
                if date_col_en and date_col_en in df.columns:
                    slip_date_col = date_col_en
                elif date_col_ja and date_col_ja in df.columns:
                    slip_date_col = date_col_ja
                
                if slip_date_col is None:
                    logger.warning(
                        f"[SOFT_DELETE] âš ï¸ Date column not found in {csv_type}. "
                        f"Expected: {date_col_ja}/{date_col_en}, "
                        f"Available: {list(df.columns)[:10]}"
                    )
                    continue
                
                # NaT ã‚„ None ã‚’é™¤å¤–ã—ã¦æ—¥ä»˜ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
                dates = set(df[slip_date_col].dropna().dt.date.unique())
                
                if not dates:
                    logger.info(f"No valid dates found in {csv_type}, skipping soft delete")
                    continue
                
                # csv_kind ã‚’æ±ºå®š
                csv_kind = csv_type_to_kind_map.get(csv_type)
                if not csv_kind:
                    logger.warning(f"Unknown csv_type: {csv_type}, skipping soft delete")
                    continue
                
                # ãƒ‡ãƒãƒƒã‚°: dates ã®ä¸­èº«ã¨å‹ã‚’ç¢ºèª
                dates_list_for_log = sorted(list(dates))
                logger.info(
                    f"[PRE-INSERT] ğŸ“‹ About to soft delete: csv_type={csv_type}, csv_kind={csv_kind}, "
                    f"dates_count={len(dates)}, dates_type={type(dates)}, "
                    f"first_date_type={type(list(dates)[0]) if dates else 'N/A'}, "
                    f"dates_sample={dates_list_for_log[:5]}"
                )
                
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’è«–ç†å‰Šé™¤
                logger.info(
                    f"[PRE-INSERT] ğŸ”„ Calling soft_delete for {csv_kind}, "
                    f"dates={dates_list_for_log[:5]}{'...' if len(dates) > 5 else ''}"
                )
                affected_rows = self.raw_data_repo.soft_delete_scope_by_dates(
                    csv_kind=csv_kind,
                    dates=dates,
                    deleted_by=deleted_by,
                )
                
                # affected_rows ã‚’æ˜ç¢ºã«ãƒ­ã‚°å‡ºåŠ›
                if affected_rows == 0:
                    logger.warning(
                        f"[PRE-INSERT] âš ï¸ soft_delete returned affected_rows=0 for {csv_kind}. "
                        f"This means no existing data was found for these dates: {dates_list_for_log[:5]}"
                    )
                else:
                    logger.info(
                        f"[PRE-INSERT] âœ… Soft deleted {affected_rows} existing rows "
                        f"for {csv_kind} before inserting new data (dates: {len(dates)} dates)"
                    )
                
            except Exception as e:
                logger.error(
                    f"Failed to soft delete existing data for {csv_type}: {e}",
                    exc_info=True
                )
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¯ç¶™ç¶š
                # ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤å¤±æ•—ã¯ã€æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥ã‚’å¦¨ã’ãªã„ï¼‰

    
    def _update_upload_logs(
        self,
        upload_file_ids: Dict[str, int],
        formatted_dfs: Dict[str, pd.DataFrame],
        stg_result: Dict[str, dict],
    ) -> None:
        """
        log.upload_file ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°ã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼ã‚’æ›´æ–°
        
        Args:
            upload_file_ids: csv_type -> upload_file.id ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            formatted_dfs: csv_type -> DataFrame ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            stg_result: stgå±¤ã¸ã®ä¿å­˜çµæœ
        """
        if not self.raw_data_repo:
            return
        
        # ãƒãƒ†ãƒ“ãƒ¥ãƒ¼æ›´æ–°ãŒå¿…è¦ãª csv_type ã®ãƒªã‚¹ãƒˆ
        mv_refresh_needed = []
        
        for csv_type, file_id in upload_file_ids.items():
            try:
                result_info = stg_result.get(csv_type, {})
                is_success = result_info.get("status") == "success"
                
                if is_success:
                    # æˆåŠŸ: row_count ã‚’å®Ÿéš›ã®è¡Œæ•°ã§æ›´æ–°
                    row_count = result_info.get("rows_saved", len(formatted_dfs.get(csv_type, [])))
                    self.raw_data_repo.update_upload_status(
                        file_id=file_id,
                        status="success",
                        row_count=row_count,
                    )
                    logger.info(f"Marked {csv_type} upload as success: {row_count} rows")
                    
                    # â˜… å—å…¥CSVã®æˆåŠŸæ™‚ã®ã¿ãƒãƒ†ãƒ“ãƒ¥ãƒ¼æ›´æ–°ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    # ï¼ˆç¾çŠ¶ã€MVãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã®ã¯ receive ã®ã¿ï¼‰
                    if csv_type == "receive":
                        mv_refresh_needed.append(csv_type)
                else:
                    # å¤±æ•—: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²
                    error_detail = result_info.get("detail", "Unknown error")
                    self.raw_data_repo.update_upload_status(
                        file_id=file_id,
                        status="failed",
                        error_message=error_detail,
                    )
                    logger.warning(f"Marked {csv_type} upload as failed: {error_detail}")
            except Exception as e:
                logger.error(f"Failed to update upload log for {csv_type}: {e}")
        
        # â˜… ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼ã®æ›´æ–°ï¼ˆå—å…¥CSVæˆåŠŸæ™‚ã®ã¿ï¼‰
        self._refresh_materialized_views(mv_refresh_needed)
    
    def _refresh_materialized_views(self, csv_types: List[str]) -> None:
        """
        æŒ‡å®šã•ã‚ŒãŸ csv_type ã«é–¢é€£ã™ã‚‹ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼ã‚’æ›´æ–°
        
        Args:
            csv_types: æ›´æ–°å¯¾è±¡ã® csv_type ãƒªã‚¹ãƒˆï¼ˆä¾‹: ['receive', 'shipment']ï¼‰
            
        Note:
            - ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†å…¨ä½“ã¯å¤±æ•—ã•ã›ãªã„
            - ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¦å‡¦ç†ã‚’ç¶™ç¶š
        """
        if not self.mv_refresher:
            logger.debug("MaterializedViewRefresher not injected, skipping MV refresh")
            return
        
        if not csv_types:
            logger.debug("No csv_types provided for MV refresh")
            return
        
        for csv_type in csv_types:
            try:
                logger.info(f"Starting materialized view refresh for csv_type='{csv_type}'")
                self.mv_refresher.refresh_for_csv_type(csv_type)
                logger.info(f"Successfully refreshed materialized views for csv_type='{csv_type}'")
            except Exception as e:
                # ãƒãƒ†ãƒ“ãƒ¥ãƒ¼æ›´æ–°å¤±æ•—ã¯ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ãŒã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¯æˆåŠŸæ‰±ã„
                logger.error(
                    f"Failed to refresh materialized views for csv_type='{csv_type}': {e}",
                    exc_info=True
                )
                # å‘¼ã³å‡ºã—å´ã«ã¯å½±éŸ¿ã‚’ä¸ãˆãªã„ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è‡ªä½“ã¯æˆåŠŸã—ã¦ã„ã‚‹ï¼‰
    
    def _generate_response(
        self, raw_result: Dict[str, dict], stg_result: Dict[str, dict]
    ) -> SuccessApiResponse | ErrorApiResponse:
        """
        rawå±¤ã¨stgå±¤ã®ä¿å­˜çµæœã‚’çµ±åˆã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        
        Args:
            raw_result: rawå±¤ã¸ã®ä¿å­˜çµæœ
            stg_result: stgå±¤ã¸ã®ä¿å­˜çµæœ
            
        Returns:
            çµ±åˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        # stgå±¤ã®æˆåŠŸåˆ¤å®šï¼ˆä¸»è¦ãªä¿å­˜å…ˆï¼‰
        all_stg_success = all(r["status"] == "success" for r in stg_result.values())
        
        # çµ±åˆçµæœã‚’ç”Ÿæˆ
        combined_result = {}
        for csv_type in set(list(raw_result.keys()) + list(stg_result.keys())):
            combined_result[csv_type] = {
                "raw": raw_result.get(csv_type, {"status": "not_processed"}),
                "stg": stg_result.get(csv_type, {"status": "not_processed"}),
            }
        
        if all_stg_success:
            total_rows = sum(r["rows_saved"] for r in stg_result.values() if "rows_saved" in r)
            return SuccessApiResponse(
                code="UPLOAD_SUCCESS",
                detail=f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: åˆè¨ˆ {total_rows} è¡Œã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆrawå±¤ + stgå±¤ï¼‰",
                result=combined_result,
                hint="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ",
            )
        else:
            return ErrorApiResponse(
                code="PARTIAL_SAVE_ERROR",
                detail="ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ",
                result=combined_result,
                status_code=500,
            )

