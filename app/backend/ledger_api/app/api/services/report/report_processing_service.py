"""å¸³ç¥¨å‡¦ç†ã®å…±é€šã‚µãƒ¼ãƒ“ã‚¹ã€‚

è²¬å‹™:
- CSV èª­è¾¼
- ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã® validate/format/main_process å‘¼ã³å‡ºã—
- Excel/PDF ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã€ç½²åä»˜ã URL ã‚’è¿”å´
"""

from datetime import datetime, timezone
from typing import Any, Dict, Optional, Tuple

# pandas ã¯ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯æœªä½¿ç”¨
from fastapi import UploadFile
from fastapi.responses import JSONResponse, Response

from app.api.services.report.base_report_generator import BaseReportGenerator
from app.api.services.report.artifact_service import get_report_artifact_storage
from app.api.utils.pdf_conversion import PdfConversionError, convert_excel_to_pdf
from backend_shared.src.api_response.response_error import NoFilesUploadedResponse
from backend_shared.src.utils.csv_reader import read_csv_files
from backend_shared.src.utils.date_filter_utils import (
    filter_by_period_from_min_date as shared_filter_by_period_from_min_date,
)


def _ensure_bytes(value: Any, *, label: str) -> bytes:
    """Ensure the provided value is bytes.

    ğŸ‘¶ Pydantic ã‚„ BytesIO ãªã©æ§˜ã€…ãªå‹ãŒè¿”ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§çµ±ä¸€ã—ã¦ãŠãã¾ã™ã€‚
    """

    if isinstance(value, bytes):
        return value
    if isinstance(value, bytearray):
        return bytes(value)
    try:
        # BytesIO ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ©ã‚¤ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆ
        if hasattr(value, "getvalue"):
            return bytes(value.getvalue())  # type: ignore[call-overload]
        if hasattr(value, "read"):
            data = value.read()
            return bytes(data)
    except Exception as exc:  # noqa: BLE001
        raise TypeError(f"{label}: could not normalise to bytes") from exc
    raise TypeError(f"{label}: unsupported type {type(value)!r}")


class ReportProcessingService:
    """å¸³ç¥¨å‡¦ç†ã®å…±é€šã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        pass

    def _read_uploaded_files(
        self, files: Dict[str, UploadFile]
    ) -> Tuple[Optional[Dict[str, Any]], Optional[Any]]:
        """CSVèª­è¾¼ã®ã¿ã‚’æ‹…å½“ã€‚ç©ºãƒã‚§ãƒƒã‚¯ã‚‚å«ã‚€ã€‚"""
        if not files:
            print("No files uploaded.")
            return None, NoFilesUploadedResponse()

        print(f"Uploaded files: {list(files.keys())}")

        dfs, error = read_csv_files(files)
        if error:
            return None, error
        return dfs, None

    def run(
        self, generator: BaseReportGenerator, files: Dict[str, UploadFile]
    ) -> Response:
        """
        å®Œå…¨ãªå¸³ç¥¨å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œï¼ˆFactoryä¸è¦ãƒ»å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒGeneratorã‚’ç”Ÿæˆï¼‰
        """
        # Step 1: CSVèª­è¾¼
        dfs, error = self._read_uploaded_files(files)
        if error:
            return error.to_json_response()

        assert dfs is not None

        # Step 2: æ¤œè¨¼ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
        validation_error = generator.validate(dfs, files)
        if validation_error:
            print(f"Validation error: {validation_error}")
            return validation_error.to_json_response()

        # Step 2.5: å¸³ç°¿ã”ã¨ã®æœŸé–“æŒ‡å®šãŒã‚ã‚Œã°ã€æœ€å°ä¼ç¥¨æ—¥ä»˜ã‹ã‚‰æ—¥/é€±/æœˆã§ãƒ•ã‚£ãƒ«ã‚¿
        period_type = getattr(generator, "period_type", None)
        if period_type:
            print(
                "\n==================== CSVæ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ ãƒ‡ãƒãƒƒã‚°é–‹å§‹ ===================="
            )
            # ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚£ãƒ«ã‚¿å‰ã®å„DataFrameã®shapeã¨ã‚«ãƒ©ãƒ åã‚’è¡¨ç¤º
            print("[DEBUG] DataFrame shapes BEFORE filtering:")
            for csv_type, df in dfs.items():
                try:
                    shape = getattr(df, "shape", None)
                    print(f"[DEBUG] Original {csv_type}: shape={shape}")
                    print(f"[DEBUG] Columns in {csv_type}: {list(df.columns)}")
                    # æ—¥ä»˜å€™è£œã‚«ãƒ©ãƒ ã®æœ‰ç„¡ã‚’è¡¨ç¤º
                    candidates = ["ä¼ç¥¨æ—¥ä»˜", "æ—¥ä»˜", "date", "Date"]
                    found = [c for c in candidates if c in df.columns]
                    print(
                        f"[DEBUG] Candidate date columns found in {csv_type}: {found}"
                    )
                    # ã‚µãƒ³ãƒ—ãƒ«å€¤è¡¨ç¤º
                    for col in found:
                        vals = df[col].head(3).tolist()
                        print(f"[DEBUG] Sample values for {col} in {csv_type}: {vals}")
                except Exception as ex:
                    print(
                        f"[DEBUG] Original {csv_type}: shape=Unknown (not a DataFrame), error={ex}"
                    )

            try:
                # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¸å§”è­²
                dfs = shared_filter_by_period_from_min_date(dfs, period_type)
                print(f"Applied date filtering by period: {period_type}")
                # ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®å„DataFrameã®shapeã‚’è¡¨ç¤º
                print("[DEBUG] DataFrame shapes AFTER filtering:")
                for csv_type, df in dfs.items():
                    try:
                        shape = getattr(df, "shape", None)
                        print(f"[DEBUG] Filtered {csv_type}: shape={shape}")
                    except Exception:
                        print(
                            f"[DEBUG] Filtered {csv_type}: shape=Unknown (not a DataFrame)"
                        )
            except Exception as e:
                print(f"[WARN] Date filtering skipped due to error: {e}")
            print(
                "==================== CSVæ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ ãƒ‡ãƒãƒƒã‚°çµ‚äº† ====================\n"
            )

        # Step 3: æ•´å½¢ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
        print("Formatting DataFrames...")
        df_formatted = generator.format(dfs)
        for csv_type, df in df_formatted.items():
            try:
                shape = getattr(df, "shape", None)
                print(f"Formatted {csv_type}: shape={shape}")
            except Exception:
                pass

        # Step 4: ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
        print("Running main_process...")
        df_result = generator.main_process(df_formatted)

        # Step 5: å¸³ç¥¨æ—¥ä»˜ä½œæˆï¼ˆå…±é€š: æ•´å½¢å¾Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
        print("Making report date...")
        report_date = generator.make_report_date(df_formatted)

        # Step 6: Excel/PDF ã‚’ä¿å­˜ã— JSON ã§ URL ã‚’è¿”ã™
        return self.create_response(generator, df_result, report_date)

    # ---------- æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿é–¢é€£ï¼ˆå…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰ ----------
    # å…±é€šåŒ–: æ—§ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè£…ã¯ date_filter_utils ã«ç§»å‹•
    # def filter_by_period_from_min_date(...): pass
    # def _find_date_column(...): pass

    def create_response(
        self,
        generator: BaseReportGenerator,
        df_result: Any,
        report_date: str,
        *,
        extra_payload: Optional[Dict[str, Any]] = None,
    ) -> JSONResponse:
        """Excel/PDF ã‚’ä¿å­˜ã—ã€ç½²åä»˜ã URL ã‚’å«ã‚€ JSON ã‚’è¿”å´ã™ã‚‹ã€‚"""
        try:
            excel_bytes_raw = generator.generate_excel_bytes(df_result, report_date)
            excel_bytes = _ensure_bytes(excel_bytes_raw, label="excel_bytes")
            storage = get_report_artifact_storage()
            location = storage.allocate(generator.report_key, report_date)

            excel_path = storage.save_excel(location, excel_bytes)

            pdf_exists = True
            pdf_error: Optional[str] = None
            try:
                pdf_bytes = convert_excel_to_pdf(
                    excel_path,
                    output_dir=location.directory,
                    profile_dir=location.directory / "lo_profile",
                )
                storage.save_pdf(location, pdf_bytes)
            except PdfConversionError as exc:
                pdf_exists = False
                pdf_error = str(exc)

            artifact_payload = storage.build_payload(location, excel_exists=True, pdf_exists=pdf_exists)
            metadata: Dict[str, Any] = {
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "pdf_status": "available" if pdf_exists else "unavailable",
            }
            if pdf_error:
                metadata["pdf_error"] = pdf_error

            response_body: Dict[str, Any] = {
                "status": "success",
                "report_key": generator.report_key,
                "report_date": report_date,
                "artifact": artifact_payload,
                "metadata": metadata,
            }

            if extra_payload:
                extra = extra_payload.copy()
                extra.pop("status", None)
                response_body.update(extra)

            return JSONResponse(status_code=200, content=response_body)

        except Exception as e:
            print(f"[ERROR] Excel/PDF artifact generation failed: {e}")
            raise

    # æ—§APIã¯æ’¤å»ƒï¼ˆFactoryå»ƒæ­¢ã«ä¼´ã„ä½¿ç”¨ä¸å¯ï¼‰
