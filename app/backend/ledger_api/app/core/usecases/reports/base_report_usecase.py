"""
Base Report UseCase.

å…¨ãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆUseCaseã®å…±é€šå‡¦ç†ã‚’æä¾›ã™ã‚‹åŸºåº•ã‚¯ãƒ©ã‚¹ã€‚
å„ãƒ¬ãƒãƒ¼ãƒˆå›ºæœ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦å®šç¾©ã—ã€ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ã™ã‚‹ã€‚

ğŸ”„ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°: ExcelåŒæœŸ + PDFéåŒæœŸã®2æ®µéšæ§‹æˆã«å¯¾å¿œ
"""

import time
from abc import ABC, abstractmethod
from datetime import date
from io import BytesIO
from typing import Any

from fastapi import BackgroundTasks, UploadFile
from fastapi.responses import JSONResponse

from app.application.usecases.reports.report_generation_utils import (
    generate_pdf_from_excel,
)
from app.core.ports.inbound import CsvGateway, ReportRepository
from backend_shared.application.logging import get_module_logger
from backend_shared.infra.adapters.fastapi.error_handlers import DomainError
from backend_shared.utils.date_filter_utils import (
    filter_by_period_from_max_date as shared_filter_by_period_from_max_date,
)


logger = get_module_logger(__name__)


class BaseReportUseCase(ABC):
    """
    ãƒ¬ãƒãƒ¼ãƒˆç”ŸæˆUseCaseã®åŸºåº•ã‚¯ãƒ©ã‚¹ã€‚

    å…±é€šå‡¦ç†ï¼ˆCSVèª­ã¿è¾¼ã¿ã€æ•´å½¢ã€PDFç”Ÿæˆã€ä¿å­˜ç­‰ï¼‰ã‚’å®Ÿè£…ã—ã€
    ãƒ¬ãƒãƒ¼ãƒˆå›ºæœ‰ã®å‡¦ç†ã¯æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã«å§”è­²ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        csv_gateway: CsvGateway,
        report_repository: ReportRepository,
    ):
        """
        åŸºåº•UseCaseã®åˆæœŸåŒ–ã€‚

        Args:
            csv_gateway: CSVèª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼ãƒ»æ•´å½¢ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
            report_repository: ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        """
        self.csv_gateway = csv_gateway
        self.report_repository = report_repository

    @property
    @abstractmethod
    def report_key(self) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚­ãƒ¼ï¼ˆä¾‹: "average_sheet"ï¼‰"""
        pass

    @property
    @abstractmethod
    def report_name(self) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆåï¼ˆä¾‹: "å˜ä¾¡å¹³å‡è¡¨"ï¼‰"""
        pass

    @abstractmethod
    def create_domain_model(self, df_formatted: dict[str, Any]) -> Any:
        """
        ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆStep 4ï¼‰ã€‚

        Args:
            df_formatted: æ•´å½¢æ¸ˆã¿DataFrameè¾æ›¸

        Returns:
            ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆreport_dateãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æŒã¤ï¼‰
        """
        pass

    @abstractmethod
    def execute_domain_logic(self, df_formatted: dict[str, Any]) -> Any:
        """
        ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆStep 5ï¼‰ã€‚

        Args:
            df_formatted: æ•´å½¢æ¸ˆã¿DataFrameè¾æ›¸

        Returns:
            å‡¦ç†çµæœDataFrame
        """
        pass

    @abstractmethod
    def generate_excel(self, result_df: Any, report_date: date) -> BytesIO:
        """
        Excelã‚’ç”Ÿæˆã™ã‚‹ï¼ˆStep 6ï¼‰ã€‚

        Args:
            result_df: å‡¦ç†çµæœDataFrame
            report_date: ãƒ¬ãƒãƒ¼ãƒˆæ—¥ä»˜

        Returns:
            Excelãƒã‚¤ãƒŠãƒªã‚¹ãƒˆãƒªãƒ¼ãƒ 
        """
        pass

    def execute(
        self,
        shipment: UploadFile | None = None,
        yard: UploadFile | None = None,
        receive: UploadFile | None = None,
        period_type: str | None = None,
        background_tasks: BackgroundTasks | None = None,
        async_pdf: bool = True,
    ) -> JSONResponse:
        """
        ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®å…±é€šå®Ÿè¡Œãƒ•ãƒ­ãƒ¼ã€‚

        Args:
            shipment: å‡ºè·ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«
            yard: ãƒ¤ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«
            receive: å—å…¥ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«
            period_type: æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ ("oneday" | "oneweek" | "onemonth")
            background_tasks: FastAPIã®BackgroundTasksï¼ˆPDFéåŒæœŸç”Ÿæˆç”¨ï¼‰
            async_pdf: True=PDFéåŒæœŸç”Ÿæˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰, False=åŒæœŸç”Ÿæˆï¼ˆå¾“æ¥äº’æ›ï¼‰

        Returns:
            JSONResponse: ç½²åä»˜ãURLã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        start_time = time.time()
        file_keys = [
            k
            for k, v in {"shipment": shipment, "yard": yard, "receive": receive}.items()
            if v is not None
        ]

        logger.info(
            f"{self.report_name}ç”Ÿæˆé–‹å§‹",
            extra={
                "usecase": self.report_key,
                "file_keys": file_keys,
                "period_type": period_type,
                "async_pdf": async_pdf,
            },
        )

        try:
            # Step 1: CSVèª­ã¿è¾¼ã¿
            dfs = self._read_csv_files(shipment, yard, receive)

            # Step 2: æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if period_type:
                dfs = self._apply_period_filter(dfs, period_type)

            # Step 3: CSVæ•´å½¢
            df_formatted = self._format_csv_data(dfs)

            # Step 4: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹å®Ÿè£…ï¼‰
            domain_model = self._create_domain_model_with_logging(df_formatted)

            # Step 5: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹å®Ÿè£…ï¼‰
            result_df = self._execute_domain_logic_with_logging(df_formatted)

            # Step 6: Excelç”Ÿæˆï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹å®Ÿè£…ï¼‰
            excel_bytes = self._generate_excel_with_logging(result_df, domain_model.report_date)

            if async_pdf and background_tasks is not None:
                # éåŒæœŸãƒ¢ãƒ¼ãƒ‰: Excelã®ã¿ä¿å­˜ã—ã€PDFã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç”Ÿæˆ
                return self._save_and_respond_async(
                    domain_model.report_date,
                    excel_bytes,
                    background_tasks,
                    start_time,
                )
            else:
                # åŒæœŸãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥äº’æ›ï¼‰: PDF ã‚‚åŒæ™‚ã«ç”Ÿæˆ
                # Step 7: PDFç”Ÿæˆ
                pdf_bytes = self._generate_pdf_with_logging(excel_bytes)

                # Step 8: ä¿å­˜ã¨URLç”Ÿæˆ
                artifact_urls = self._save_report_with_logging(
                    domain_model.report_date,
                    excel_bytes,
                    pdf_bytes,
                )

                # Step 9: ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿”å´
                return self._create_response(artifact_urls, domain_model.report_date, start_time)

        except DomainError:
            raise
        except Exception as ex:
            logger.exception(
                f"{self.report_name}ç”Ÿæˆå¤±æ•—",
                extra={
                    "usecase": self.report_key,
                    "error": str(ex),
                    "elapsed_seconds": round(time.time() - start_time, 3),
                },
                exc_info=True,
            )
            raise DomainError(
                code="INTERNAL_ERROR",
                status=500,
                user_message=f"{self.report_name}ã®ç”Ÿæˆä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                title="å†…éƒ¨ã‚¨ãƒ©ãƒ¼",
            ) from ex

    def _read_csv_files(
        self,
        shipment: UploadFile | None,
        yard: UploadFile | None,
        receive: UploadFile | None,
    ) -> dict[str, Any]:
        """Step 1: CSVèª­ã¿è¾¼ã¿"""
        step_start = time.time()
        logger.debug("Step 1: CSVèª­ã¿è¾¼ã¿é–‹å§‹")

        files = {
            k: v
            for k, v in {"shipment": shipment, "yard": yard, "receive": receive}.items()
            if v is not None
        }

        dfs, error = self.csv_gateway.read_csv_files(files)
        if error:
            logger.warning(
                "Step 1: CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼",
                extra={"error_type": type(error).__name__},
            )
            raise error

        assert dfs is not None
        logger.debug(
            "Step 1: CSVèª­ã¿è¾¼ã¿å®Œäº†",
            extra={"elapsed_seconds": round(time.time() - step_start, 3)},
        )
        return dfs

    def _apply_period_filter(
        self,
        dfs: dict[str, Any],
        period_type: str,
    ) -> dict[str, Any]:
        """Step 2: æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨"""
        step_start = time.time()
        logger.debug(
            "Step 2: æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨é–‹å§‹",
            extra={"period_type": period_type},
        )

        try:
            filtered_dfs = shared_filter_by_period_from_max_date(dfs, period_type)
            logger.debug(
                "Step 2: æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å®Œäº†",
                extra={
                    "period_type": period_type,
                    "elapsed_seconds": round(time.time() - step_start, 3),
                },
            )
            return filtered_dfs
        except Exception as e:
            logger.warning(
                "Step 2: æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰",
                extra={"exception": str(e)},
            )
            return dfs

    def _format_csv_data(self, dfs: dict[str, Any]) -> dict[str, Any]:
        """Step 3: CSVæ•´å½¢"""
        step_start = time.time()
        logger.debug("Step 3: CSVæ•´å½¢é–‹å§‹")

        df_formatted = self.csv_gateway.format_csv_data(dfs)
        logger.debug(
            "Step 3: CSVæ•´å½¢å®Œäº†",
            extra={"elapsed_seconds": round(time.time() - step_start, 3)},
        )
        return df_formatted

    def _create_domain_model_with_logging(self, df_formatted: dict[str, Any]) -> Any:
        """Step 4: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆï¼ˆãƒ­ã‚°ä»˜ãï¼‰"""
        step_start = time.time()
        logger.debug("Step 4: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆé–‹å§‹")

        domain_model = self.create_domain_model(df_formatted)

        logger.debug(
            "Step 4: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆå®Œäº†",
            extra={
                "report_date": domain_model.report_date.isoformat(),
                "elapsed_seconds": round(time.time() - step_start, 3),
            },
        )
        return domain_model

    def _execute_domain_logic_with_logging(self, df_formatted: dict[str, Any]) -> Any:
        """Step 5: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œï¼ˆãƒ­ã‚°ä»˜ãï¼‰"""
        step_start = time.time()
        logger.debug("Step 5: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œé–‹å§‹")

        result_df = self.execute_domain_logic(df_formatted)

        logger.debug(
            "Step 5: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œå®Œäº†",
            extra={"elapsed_seconds": round(time.time() - step_start, 3)},
        )
        return result_df

    def _generate_excel_with_logging(self, result_df: Any, report_date: date) -> BytesIO:
        """Step 6: Excelç”Ÿæˆï¼ˆãƒ­ã‚°ä»˜ãï¼‰"""
        step_start = time.time()
        logger.debug("Step 6: Excelç”Ÿæˆé–‹å§‹")

        excel_bytes = self.generate_excel(result_df, report_date)

        logger.debug(
            "Step 6: Excelç”Ÿæˆå®Œäº†",
            extra={
                "size_bytes": len(excel_bytes.getvalue()),
                "elapsed_seconds": round(time.time() - step_start, 3),
            },
        )
        return excel_bytes

    def _generate_pdf_with_logging(self, excel_bytes: BytesIO) -> BytesIO:
        """Step 7: PDFç”Ÿæˆ"""
        step_start = time.time()
        logger.debug("Step 7: PDFç”Ÿæˆé–‹å§‹")

        pdf_bytes = generate_pdf_from_excel(excel_bytes)

        logger.debug(
            "Step 7: PDFç”Ÿæˆå®Œäº†",
            extra={
                "size_bytes": len(pdf_bytes.getvalue()),
                "elapsed_seconds": round(time.time() - step_start, 3),
            },
        )
        return pdf_bytes

    def _save_report_with_logging(
        self,
        report_date: date,
        excel_bytes: BytesIO,
        pdf_bytes: BytesIO,
    ):
        """Step 8: ä¿å­˜ã¨URLç”Ÿæˆ"""
        step_start = time.time()
        logger.debug("Step 8: ä¿å­˜ã¨URLç”Ÿæˆé–‹å§‹")

        artifact_urls = self.report_repository.save_report(
            report_key=self.report_key,
            report_date=report_date,
            excel_bytes=excel_bytes,
            pdf_bytes=pdf_bytes,
        )

        logger.debug(
            "Step 8: ä¿å­˜ã¨URLç”Ÿæˆå®Œäº†",
            extra={"elapsed_seconds": round(time.time() - step_start, 3)},
        )
        return artifact_urls

    def _create_response(
        self,
        artifact_urls,
        report_date: date,
        start_time: float,
    ) -> JSONResponse:
        """Step 9: ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿”å´"""
        total_elapsed = time.time() - start_time
        logger.info(
            f"{self.report_name}ç”Ÿæˆå®Œäº†",
            extra={
                "usecase": self.report_key,
                "report_date": report_date.isoformat(),
                "total_elapsed_seconds": round(total_elapsed, 3),
            },
        )

        # BFFäº’æ›ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
        artifact_dict = artifact_urls.to_dict()
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "message": f"{self.report_name}ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ",
                "report_key": self.report_key,
                "report_date": report_date.isoformat(),
                "artifact": {
                    "excel_download_url": artifact_dict["excel_url"],
                    "pdf_preview_url": artifact_dict["pdf_url"],
                },
            },
        )

    def _save_and_respond_async(
        self,
        report_date: date,
        excel_bytes: BytesIO,
        background_tasks: BackgroundTasks,
        start_time: float,
    ) -> JSONResponse:
        """ExcelåŒæœŸä¿å­˜ + PDFéåŒæœŸç”Ÿæˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†ã€‚

        Args:
            report_date: ãƒ¬ãƒãƒ¼ãƒˆæ—¥ä»˜
            excel_bytes: Excelãƒã‚¤ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ 
            background_tasks: FastAPIã®BackgroundTasks
            start_time: å‡¦ç†é–‹å§‹æ™‚é–“

        Returns:
            JSONResponse: Excel URL + pdf_status="pending" ã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        from app.infra.adapters.artifact_storage.artifact_builder import (
            generate_pdf_background,
        )
        from app.infra.adapters.artifact_storage.artifact_service import (
            get_report_artifact_storage,
        )

        step_start = time.time()
        logger.debug("Step 7-8 (async): Excelä¿å­˜é–‹å§‹")

        # Excelã‚’ä¿å­˜
        storage = get_report_artifact_storage()
        location = storage.allocate(self.report_key, report_date.isoformat())

        excel_path = storage.save_excel(location, excel_bytes.getvalue())

        # Excel URLã‚’ç”Ÿæˆ
        excel_filename = f"{location.file_base}.xlsx"
        excel_url = storage.signer.create_url(
            location.relative_path(excel_filename),
            disposition="attachment",
        )

        logger.debug(
            "Step 7-8 (async): Excelä¿å­˜å®Œäº†",
            extra={
                "elapsed_seconds": round(time.time() - step_start, 3),
                "report_token": location.token,
            },
        )

        # PDFç”Ÿæˆã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã«ç™»éŒ²
        background_tasks.add_task(
            generate_pdf_background,
            report_key=self.report_key,
            report_date=report_date.isoformat(),
            report_token=location.token,
            excel_path_str=str(excel_path),
        )

        logger.info(
            "PDFç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã«ç™»éŒ²",
            extra={
                "report_key": self.report_key,
                "report_date": report_date.isoformat(),
                "report_token": location.token,
            },
        )

        total_elapsed = time.time() - start_time
        logger.info(
            f"{self.report_name}ç”Ÿæˆå®Œäº†ï¼ˆPDFéåŒæœŸï¼‰",
            extra={
                "usecase": self.report_key,
                "report_date": report_date.isoformat(),
                "total_elapsed_seconds": round(total_elapsed, 3),
                "pdf_status": "pending",
            },
        )

        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "message": f"{self.report_name}ã®Excelç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚PDFã¯ç”Ÿæˆä¸­ã§ã™ã€‚",
                "report_key": self.report_key,
                "report_date": report_date.isoformat(),
                "artifact": {
                    "excel_download_url": excel_url,
                    "pdf_preview_url": None,  # éåŒæœŸç”Ÿæˆä¸­ã®ãŸã‚æœªå®š
                    "report_token": location.token,
                },
                "metadata": {
                    "pdf_status": "pending",
                },
            },
        )
