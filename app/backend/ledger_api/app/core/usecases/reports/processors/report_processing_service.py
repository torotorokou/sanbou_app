"""å¸³ç¥¨å‡¦ç†ã®å…±é€šã‚µãƒ¼ãƒ“ã‚¹:
- CSV èª­è¾¼
- ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã® validate/format/main_process å‘¼ã³å‡ºã—
- Excel/PDF ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã€ç½²åä»˜ã URL ã‚’è¿”å´

ğŸ”„ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°: ExcelåŒæœŸ + PDFéåŒæœŸã®2æ®µéšæ§‹æˆã«å¯¾å¿œ
"""

import traceback
from typing import Any

from backend_shared.application.logging import get_module_logger

# pandas ã¯ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯æœªä½¿ç”¨
from fastapi import BackgroundTasks, UploadFile
from fastapi.responses import JSONResponse, Response

logger = get_module_logger(__name__)

from app.core.usecases.reports.base_generators import BaseReportGenerator
from backend_shared.infra.adapters.fastapi.error_handlers import DomainError
from backend_shared.infra.adapters.presentation.response_error import (
    NoFilesUploadedResponse,
)
from backend_shared.utils.csv_reader import read_csv_files
from backend_shared.utils.date_filter_utils import (
    filter_by_period_from_max_date as shared_filter_by_period_from_max_date,
)


def _ensure_bytes(value: Any, *, label: str) -> bytes:
    """Ensure the provided value is bytes.

    ğŸ‘¶ Pydantic ã‚„ BytesIO ãªã©æ§˜ã€…ãªå‹ãŒè¿”ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§çµ±ä¸€ã—ã¦ãŠãã¾ã™ã€‚
    """

    if isinstance(value, bytes):
        return value
    if isinstance(value, bytearray):
        return bytes(value)
    try:
        # BytesIO ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ©ã‚¤ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆ
        if hasattr(value, "getvalue"):
            return bytes(value.getvalue())  # type: ignore[call-overload]
        if hasattr(value, "read"):
            data = value.read()
            return bytes(data)
    except Exception as exc:  # noqa: BLE001
        raise TypeError(f"{label}: could not normalise to bytes") from exc
    raise TypeError(f"{label}: unsupported type {type(value)!r}")


class ReportProcessingService:
    """å¸³ç¥¨å‡¦ç†ã®å…±é€šã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        pass

    def _read_uploaded_files(
        self, files: dict[str, UploadFile]
    ) -> tuple[dict[str, Any] | None, Any | None]:
        """CSVèª­è¾¼ã®ã¿ã‚’æ‹…å½“ã€‚ç©ºãƒã‚§ãƒƒã‚¯ã‚‚å«ã‚€ã€‚"""
        if not files:
            logger.warning("No files uploaded")
            return None, NoFilesUploadedResponse()

        logger.debug(
            "Processing uploaded files", extra={"file_keys": list(files.keys())}
        )

        dfs, error = read_csv_files(files)
        if error:
            return None, error
        return dfs, None

    def run(
        self,
        generator: BaseReportGenerator,
        files: dict[str, UploadFile],
        background_tasks: BackgroundTasks | None = None,
        async_pdf: bool = True,
    ) -> Response:
        """
        å®Œå…¨ãªå¸³ç¥¨å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œï¼ˆFactoryä¸è¦ãƒ»å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒGeneratorã‚’ç”Ÿæˆï¼‰

        Args:
            generator: ãƒ¬ãƒãƒ¼ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
            files: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«
            background_tasks: FastAPIã®BackgroundTasksï¼ˆPDFéåŒæœŸç”Ÿæˆç”¨ï¼‰
            async_pdf: True=PDFéåŒæœŸç”Ÿæˆ, False=åŒæœŸç”Ÿæˆï¼ˆå¾“æ¥äº’æ›ï¼‰
        """
        try:
            # Step 1: CSVèª­è¾¼
            dfs, error = self._read_uploaded_files(files)
            if error:
                return error.to_json_response()

            assert dfs is not None

            # Step 2: æ¤œè¨¼ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
            validation_error = generator.validate(dfs, files)
            if validation_error:
                logger.warning(
                    "Validation failed", extra={"error": str(validation_error)}
                )
                return validation_error.to_json_response()

            # Step 2.5: å¸³ç°¿ã”ã¨ã®æœŸé–“æŒ‡å®šãŒã‚ã‚Œã°ã€æœ€å°ä¼ç¥¨æ—¥ä»˜ã‹ã‚‰æ—¥/é€±/æœˆã§ãƒ•ã‚£ãƒ«ã‚¿
            period_type = getattr(generator, "period_type", None)
            if period_type:
                logger.debug(
                    "Starting CSV date filtering", extra={"period_type": period_type}
                )
                logger.debug("DataFrame shapes BEFORE filtering")
                for csv_type, df in dfs.items():
                    try:
                        shape = getattr(df, "shape", None)
                        columns = list(df.columns)
                        candidates = ["ä¼ç¥¨æ—¥ä»˜", "æ—¥ä»˜", "date", "Date"]
                        found = [c for c in candidates if c in df.columns]
                        samples = {col: df[col].head(3).tolist() for col in found}
                        logger.debug(
                            "DataFrame info before filtering",
                            extra={
                                "csv_type": csv_type,
                                "shape": shape,
                                "columns": columns,
                                "date_columns_found": found,
                                "sample_values": samples,
                            },
                        )
                    except Exception as ex:
                        logger.debug(
                            "DataFrame info unavailable",
                            extra={"csv_type": csv_type, "error": str(ex)},
                        )

                try:
                    dfs = shared_filter_by_period_from_max_date(dfs, period_type)
                    logger.info(
                        "Applied date filtering", extra={"period_type": period_type}
                    )
                    logger.debug("DataFrame shapes AFTER filtering")
                    for csv_type, df in dfs.items():
                        try:
                            shape = getattr(df, "shape", None)
                            logger.debug(
                                "DataFrame shape after filtering",
                                extra={"csv_type": csv_type, "shape": shape},
                            )
                        except Exception:
                            logger.debug(
                                "DataFrame shape unavailable after filtering",
                                extra={"csv_type": csv_type},
                            )
                except Exception as e:
                    logger.warning(
                        "Date filtering skipped due to error",
                        extra={"error": str(e)},
                        exc_info=True,
                    )
                logger.debug("Completed CSV date filtering")

            # Step 3: æ•´å½¢ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
            logger.debug("Formatting DataFrames")
            try:
                df_formatted = generator.format(dfs)
            except DomainError:
                # æ—¢ã«DomainErrorã®å ´åˆã¯ãã®ã¾ã¾å†raise
                raise
            except Exception as ex:
                logger.error("format() failed", extra={"error": str(ex)}, exc_info=True)
                raise DomainError(
                    code="REPORT_FORMAT_ERROR",
                    status=500,
                    user_message=f"å¸³ç¥¨ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(ex)}",
                    title="ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã‚¨ãƒ©ãƒ¼",
                ) from ex

            for csv_type, df in df_formatted.items():
                try:
                    shape = getattr(df, "shape", None)
                    logger.debug(
                        "Formatted DataFrame",
                        extra={"csv_type": csv_type, "shape": shape},
                    )
                except Exception:
                    pass

            # Step 4: ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
            logger.debug("Running main_process")
            try:
                df_result = generator.main_process(df_formatted)
            except DomainError:
                # æ—¢ã«DomainErrorã®å ´åˆã¯ãã®ã¾ã¾å†raise
                raise
            except Exception as ex:
                logger.error(
                    "main_process raised an exception",
                    extra={
                        "exception_type": type(ex).__name__,
                        "message": str(ex),
                        "traceback": traceback.format_exc(),
                    },
                    exc_info=True,
                )
                # DomainErrorã«å¤‰æ›ã—ã¦è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æä¾›
                raise DomainError(
                    code="REPORT_PROCESSING_ERROR",
                    status=500,
                    user_message=f"å¸³ç¥¨ã®è¨ˆç®—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(ex)}",
                    title="å¸³ç¥¨å‡¦ç†ã‚¨ãƒ©ãƒ¼",
                ) from ex

            # Step 5: å¸³ç¥¨æ—¥ä»˜ä½œæˆï¼ˆå…±é€š: æ•´å½¢å¾Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
            logger.debug("Making report date")
            report_date = generator.make_report_date(df_formatted)

            # Step 6: Excel/PDF ã‚’ä¿å­˜ã— JSON ã§ URL ã‚’è¿”ã™
            return self.create_response(
                generator,
                df_result,
                report_date,
                background_tasks=background_tasks,
                async_pdf=async_pdf,
            )

        except DomainError:
            # DomainErrorã¯ãã®ã¾ã¾å†raiseã—ã¦FastAPIã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ã«ä»»ã›ã‚‹
            raise
        except Exception as e:  # äºˆæœŸã›ã¬ä¾‹å¤–ã‚’DomainErrorã«å¤‰æ›
            logger.error(
                "Report processing failed",
                extra={"error": str(e), "traceback": traceback.format_exc()},
                exc_info=True,
            )

            # DomainErrorã¨ã—ã¦å†raiseã—ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ã§ProblemDetailsåŒ–
            raise DomainError(
                code="REPORT_GENERATION_ERROR",
                status=500,
                user_message=f"å¸³ç¥¨ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                title="å¸³ç¥¨ç”Ÿæˆã‚¨ãƒ©ãƒ¼",
            ) from e

    # ---------- æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿é–¢é€£ï¼ˆå…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰ ----------
    # å…±é€šåŒ–: æ—§ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè£…ã¯ date_filter_utils ã«ç§»å‹•
    # def filter_by_period_from_min_date(...): pass
    # def _find_date_column(...): pass

    def create_response(
        self,
        generator: BaseReportGenerator,
        df_result: Any,
        report_date: str,
        *,
        extra_payload: dict[str, Any] | None = None,
        background_tasks: BackgroundTasks | None = None,
        async_pdf: bool = True,
    ) -> JSONResponse:
        """Excel/PDF ã‚’ä¿å­˜ã—ã€ç½²åä»˜ã URL ã‚’å«ã‚€ JSON ã‚’è¿”å´ã™ã‚‹ã€‚

        Args:
            generator: ãƒ¬ãƒãƒ¼ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
            df_result: å‡¦ç†çµæœDataFrame
            report_date: ãƒ¬ãƒãƒ¼ãƒˆæ—¥ä»˜
            extra_payload: è¿½åŠ ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
            background_tasks: FastAPIã®BackgroundTasksï¼ˆPDFéåŒæœŸç”Ÿæˆç”¨ï¼‰
            async_pdf: True=PDFéåŒæœŸç”Ÿæˆ, False=åŒæœŸç”Ÿæˆ
        """
        from app.infra.adapters.artifact_storage import ArtifactResponseBuilder
        from app.infra.adapters.artifact_storage.artifact_builder import (
            generate_pdf_background,
        )

        builder = ArtifactResponseBuilder()
        response = builder.build(
            generator,
            df_result,
            report_date,
            extra_payload=extra_payload,
            async_pdf=async_pdf,
        )

        # PDFéåŒæœŸç”Ÿæˆã®å ´åˆã€BackgroundTasksã«ã‚¿ã‚¹ã‚¯ã‚’ç™»éŒ²
        if async_pdf and background_tasks is not None:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            import json

            response_body = json.loads(response.body.decode())
            metadata = response_body.get("metadata", {})
            excel_path = metadata.get("excel_path")
            artifact = response_body.get("artifact", {})
            report_token = artifact.get("report_token")
            report_key = response_body.get("report_key")

            if excel_path and report_token:
                background_tasks.add_task(
                    generate_pdf_background,
                    report_key=report_key,
                    report_date=report_date,
                    report_token=report_token,
                    excel_path_str=excel_path,
                )
                logger.info(
                    "PDFç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã«ç™»éŒ²",
                    extra={
                        "report_key": report_key,
                        "report_date": report_date,
                        "report_token": report_token,
                    },
                )

        return response

    # æ—§APIã¯æ’¤å»ƒï¼ˆFactoryå»ƒæ­¢ã«ä¼´ã„ä½¿ç”¨ä¸å¯ï¼‰
