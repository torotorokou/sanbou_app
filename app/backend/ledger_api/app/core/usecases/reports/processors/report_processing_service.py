"""å¸³ç¥¨å‡¦ç†ã®å…±é€šã‚µãƒ¼ãƒ“ã‚¹:
- CSV èª­è¾¼
- ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã® validate/format/main_process å‘¼ã³å‡ºã—
- Excel/PDF ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã€ç½²åä»˜ã URL ã‚’è¿”å´
"""

from typing import Any, Dict, Optional, Tuple
import traceback

# pandas ã¯ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯æœªä½¿ç”¨
from fastapi import UploadFile
from fastapi.responses import JSONResponse, Response
from backend_shared.application.logging import get_module_logger

logger = get_module_logger(__name__)

from app.core.usecases.reports.base_generators import BaseReportGenerator
from backend_shared.infra.adapters.presentation.response_error import NoFilesUploadedResponse
from backend_shared.infra.adapters.fastapi.error_handlers import DomainError
from backend_shared.utils.csv_reader import read_csv_files
from backend_shared.utils.date_filter_utils import (
    filter_by_period_from_max_date as shared_filter_by_period_from_max_date,
)


def _ensure_bytes(value: Any, *, label: str) -> bytes:
    """Ensure the provided value is bytes.

    ğŸ‘¶ Pydantic ã‚„ BytesIO ãªã©æ§˜ã€…ãªå‹ãŒè¿”ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§çµ±ä¸€ã—ã¦ãŠãã¾ã™ã€‚
    """

    if isinstance(value, bytes):
        return value
    if isinstance(value, bytearray):
        return bytes(value)
    try:
        # BytesIO ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ©ã‚¤ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆ
        if hasattr(value, "getvalue"):
            return bytes(value.getvalue())  # type: ignore[call-overload]
        if hasattr(value, "read"):
            data = value.read()
            return bytes(data)
    except Exception as exc:  # noqa: BLE001
        raise TypeError(f"{label}: could not normalise to bytes") from exc
    raise TypeError(f"{label}: unsupported type {type(value)!r}")


class ReportProcessingService:
    """å¸³ç¥¨å‡¦ç†ã®å…±é€šã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        pass

    def _read_uploaded_files(
        self, files: Dict[str, UploadFile]
    ) -> Tuple[Optional[Dict[str, Any]], Optional[Any]]:
        """CSVèª­è¾¼ã®ã¿ã‚’æ‹…å½“ã€‚ç©ºãƒã‚§ãƒƒã‚¯ã‚‚å«ã‚€ã€‚"""
        if not files:
            logger.warning("No files uploaded")
            return None, NoFilesUploadedResponse()

        logger.debug("Processing uploaded files", extra={"file_keys": list(files.keys())})

        dfs, error = read_csv_files(files)
        if error:
            return None, error
        return dfs, None

    def run(
        self, generator: BaseReportGenerator, files: Dict[str, UploadFile]
    ) -> Response:
        """
        å®Œå…¨ãªå¸³ç¥¨å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œï¼ˆFactoryä¸è¦ãƒ»å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒGeneratorã‚’ç”Ÿæˆï¼‰
        """
        try:
            # Step 1: CSVèª­è¾¼
            dfs, error = self._read_uploaded_files(files)
            if error:
                return error.to_json_response()

            assert dfs is not None

            # Step 2: æ¤œè¨¼ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
            validation_error = generator.validate(dfs, files)
            if validation_error:
                logger.warning("Validation failed", extra={"error": str(validation_error)})
                return validation_error.to_json_response()

            # Step 2.5: å¸³ç°¿ã”ã¨ã®æœŸé–“æŒ‡å®šãŒã‚ã‚Œã°ã€æœ€å°ä¼ç¥¨æ—¥ä»˜ã‹ã‚‰æ—¥/é€±/æœˆã§ãƒ•ã‚£ãƒ«ã‚¿
            period_type = getattr(generator, "period_type", None)
            if period_type:
                logger.debug("Starting CSV date filtering", extra={"period_type": period_type})
                logger.debug("DataFrame shapes BEFORE filtering")
                for csv_type, df in dfs.items():
                    try:
                        shape = getattr(df, "shape", None)
                        columns = list(df.columns)
                        candidates = ["ä¼ç¥¨æ—¥ä»˜", "æ—¥ä»˜", "date", "Date"]
                        found = [c for c in candidates if c in df.columns]
                        samples = {col: df[col].head(3).tolist() for col in found}
                        logger.debug(
                            "DataFrame info before filtering",
                            extra={
                                "csv_type": csv_type,
                                "shape": shape,
                                "columns": columns,
                                "date_columns_found": found,
                                "sample_values": samples
                            }
                        )
                    except Exception as ex:
                        logger.debug(
                            "DataFrame info unavailable",
                            extra={"csv_type": csv_type, "error": str(ex)}
                        )

                try:
                    dfs = shared_filter_by_period_from_max_date(dfs, period_type)
                    logger.info("Applied date filtering", extra={"period_type": period_type})
                    logger.debug("DataFrame shapes AFTER filtering")
                    for csv_type, df in dfs.items():
                        try:
                            shape = getattr(df, "shape", None)
                            logger.debug(
                                "DataFrame shape after filtering",
                                extra={"csv_type": csv_type, "shape": shape}
                            )
                        except Exception:
                            logger.debug(
                                "DataFrame shape unavailable after filtering",
                                extra={"csv_type": csv_type}
                            )
                except Exception as e:
                    logger.warning(
                        "Date filtering skipped due to error",
                        extra={"error": str(e)}, exc_info=True
                    )
                logger.debug("Completed CSV date filtering")

            # Step 3: æ•´å½¢ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
            logger.debug("Formatting DataFrames")
            try:
                df_formatted = generator.format(dfs)
            except DomainError:
                # æ—¢ã«DomainErrorã®å ´åˆã¯ãã®ã¾ã¾å†raise
                raise
            except Exception as ex:
                logger.error("format() failed", extra={"error": str(ex)}, exc_info=True)
                raise DomainError(
                    code="REPORT_FORMAT_ERROR",
                    status=500,
                    user_message=f"å¸³ç¥¨ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(ex)}",
                    title="ãƒ‡ãƒ¼ã‚¿æ•´å½¢ã‚¨ãƒ©ãƒ¼"
                ) from ex
            
            for csv_type, df in df_formatted.items():
                try:
                    shape = getattr(df, "shape", None)
                    logger.debug(
                        "Formatted DataFrame",
                        extra={"csv_type": csv_type, "shape": shape}
                    )
                except Exception:
                    pass

            # Step 4: ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å®šç¾©ï¼‰
            logger.debug("Running main_process")
            try:
                df_result = generator.main_process(df_formatted)
            except DomainError:
                # æ—¢ã«DomainErrorã®å ´åˆã¯ãã®ã¾ã¾å†raise
                raise
            except Exception as ex:
                logger.error(
                    "main_process raised an exception",
                    extra={
                        "exception_type": type(ex).__name__,
                        "message": str(ex),
                        "traceback": traceback.format_exc()
                    },
                    exc_info=True
                )
                # DomainErrorã«å¤‰æ›ã—ã¦è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æä¾›
                raise DomainError(
                    code="REPORT_PROCESSING_ERROR",
                    status=500,
                    user_message=f"å¸³ç¥¨ã®è¨ˆç®—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(ex)}",
                    title="å¸³ç¥¨å‡¦ç†ã‚¨ãƒ©ãƒ¼"
                ) from ex

            # Step 5: å¸³ç¥¨æ—¥ä»˜ä½œæˆï¼ˆå…±é€š: æ•´å½¢å¾Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
            logger.debug("Making report date")
            report_date = generator.make_report_date(df_formatted)

            # Step 6: Excel/PDF ã‚’ä¿å­˜ã— JSON ã§ URL ã‚’è¿”ã™
            return self.create_response(generator, df_result, report_date)

        except DomainError:
            # DomainErrorã¯ãã®ã¾ã¾å†raiseã—ã¦FastAPIã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ã«ä»»ã›ã‚‹
            raise
        except Exception as e:  # äºˆæœŸã›ã¬ä¾‹å¤–ã‚’DomainErrorã«å¤‰æ›
            logger.error(
                "Report processing failed",
                extra={"error": str(e), "traceback": traceback.format_exc()},
                exc_info=True
            )
            
            # DomainErrorã¨ã—ã¦å†raiseã—ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ã§ProblemDetailsåŒ–
            raise DomainError(
                code="REPORT_GENERATION_ERROR",
                status=500,
                user_message=f"å¸³ç¥¨ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                title="å¸³ç¥¨ç”Ÿæˆã‚¨ãƒ©ãƒ¼"
            ) from e

    # ---------- æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿é–¢é€£ï¼ˆå…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰ ----------
    # å…±é€šåŒ–: æ—§ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè£…ã¯ date_filter_utils ã«ç§»å‹•
    # def filter_by_period_from_min_date(...): pass
    # def _find_date_column(...): pass

    def create_response(
        self,
        generator: BaseReportGenerator,
        df_result: Any,
        report_date: str,
        *,
        extra_payload: Optional[Dict[str, Any]] = None,
    ) -> JSONResponse:
        """Excel/PDF ã‚’ä¿å­˜ã—ã€ç½²åä»˜ã URL ã‚’å«ã‚€ JSON ã‚’è¿”å´ã™ã‚‹ã€‚"""
        from app.infra.adapters.artifact_storage import ArtifactResponseBuilder
        
        builder = ArtifactResponseBuilder()
        return builder.build(
            generator,
            df_result,
            report_date,
            extra_payload=extra_payload,
        )

    # æ—§APIã¯æ’¤å»ƒï¼ˆFactoryå»ƒæ­¢ã«ä¼´ã„ä½¿ç”¨ä¸å¯ï¼‰
