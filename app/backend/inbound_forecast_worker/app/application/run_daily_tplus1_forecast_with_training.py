"""
RunDailyTplus1ForecastWithTrainingUseCase: Êó•Ê¨°t+1‰∫àÊ∏¨ÔºàÂ≠¶ÁøíËæº„ÅøÔºâ
=============================================
ÊâãÈ†Ü:
1. workspace‰ΩúÊàêÔºà/tmp/forecast_jobs/{job_id}/Ôºâ
2. DB„Åã„ÇâÂÆüÁ∏æ„Éá„Éº„ÇøÂèñÂæóÔºàÂìÅÁõÆÂà•„ÄÅÈÅéÂéª365Êó•Ôºâ‚Üí raw.csv
3. DB„Åã„Çâ‰∫àÁ¥Ñ„Éá„Éº„ÇøÂèñÂæóÔºàÈÅéÂéª60Êó•+Êú™Êù•7Êó•Ôºâ‚Üí reserve.csv
4. retrain_and_eval.py --quick „ÅßÂ≠¶Áøí‚Üí‰∫àÊ∏¨
5. ÁµêÊûúCSVË™≠„ÅøËæº„Åø‚ÜíDB„Å´‰øùÂ≠ò
6. workspace‰øùÊåÅÔºà„Éá„Éê„ÉÉ„Ç∞Áî®Ôºâ

Phase 4ÂÆüË£Ö: DB‚ÜíÂ≠¶Áøí‚Üí‰∫àÊ∏¨„ÅÆE2E
- retrain_and_eval.py „Çí --quick „ÅßÂÆüË°å
- ÂìÅÁõÆÂà•„Éá„Éº„Çø„Çí stg.shogun_final_receive „Åã„ÇâÂèñÂæó
- ‰∫àÁ¥Ñ„Éá„Éº„Çø„Çí mart.v_reserve_daily_for_forecast „Åã„ÇâÂèñÂæó
"""
from __future__ import annotations

import os
import subprocess
from datetime import date, timedelta
from pathlib import Path
from typing import Optional
from uuid import UUID

import pandas as pd
from sqlalchemy.orm import Session

from backend_shared.application.logging import get_module_logger

logger = get_module_logger(__name__)


class RunDailyTplus1ForecastWithTrainingUseCase:
    """Êó•Ê¨°t+1‰∫àÊ∏¨ÔºàÂ≠¶ÁøíËæº„ÅøÔºâUseCase"""
    
    def __init__(
        self,
        db_session: Session,
        inbound_actuals_exporter,  # InboundActualsExportPort
        reserve_exporter,  # ReserveExportPort
        forecast_result_repo,  # DailyForecastResultRepositoryPort
        retrain_script_path: Path,
        timeout: int = 1800,
    ):
        self._db = db_session
        self._inbound_actuals_exporter = inbound_actuals_exporter
        self._reserve_exporter = reserve_exporter
        self._forecast_result_repo = forecast_result_repo
        self._retrain_script_path = retrain_script_path
        self._timeout = timeout
    
    def execute(
        self,
        target_date: date,
        job_id: UUID,
    ) -> None:
        """
        Êó•Ê¨°t+1‰∫àÊ∏¨ÔºàÂ≠¶ÁøíËæº„ÅøÔºâ„ÇíÂÆüË°å
        
        Args:
            target_date: ‰∫àÊ∏¨ÂØæË±°Êó•ÔºàÊòéÊó•Ôºâ
            job_id: „Ç∏„Éß„ÉñID
        
        Raises:
            Exception: „Éá„Éº„ÇøÂèñÂæó„Ç®„É©„Éº„ÄÅÂ≠¶Áøí„Ç®„É©„Éº„ÄÅ‰∫àÊ∏¨„Ç®„É©„ÉºÁ≠â
        """
        logger.info(
            f"üöÄ Starting daily t+1 forecast with training",
            extra={
                "target_date": str(target_date),
                "job_id": str(job_id)
            }
        )
        
        # 1. workspace‰ΩúÊàê
        workspace = Path(f"/tmp/forecast_jobs/{job_id}")
        workspace.mkdir(parents=True, exist_ok=True)
        out_dir = workspace / "out"
        out_dir.mkdir(exist_ok=True)
        
        logger.info(f"üìÅ Created workspace: {workspace}")
        
        try:
            # 2. DB„Åã„ÇâÂÆüÁ∏æ„Éá„Éº„ÇøÂèñÂæóÔºàÂìÅÁõÆÂà•„ÄÅÈÅéÂéª365Êó•Ôºâ
            actuals_start = target_date - timedelta(days=365)
            actuals_end = target_date - timedelta(days=1)  # Êò®Êó•„Åæ„Åß
            
            logger.info(
                f"üìä Exporting actuals: {actuals_start} to {actuals_end}"
            )
            
            actuals_df = self._inbound_actuals_exporter.export_item_level_actuals(
                start_date=actuals_start,
                end_date=actuals_end
            )
            
            if actuals_df.empty:
                raise ValueError(
                    f"No actuals found between {actuals_start} and {actuals_end}"
                )
            
            # raw.csv‰øùÂ≠òÔºàÊó•Êú¨Ë™û„Éò„ÉÉ„ÉÄÔºâ
            raw_csv_path = workspace / "raw.csv"
            actuals_df.to_csv(raw_csv_path, index=False, encoding="utf-8")
            
            logger.info(
                f"‚úÖ Exported {len(actuals_df)} actuals to {raw_csv_path}",
                extra={
                    "actuals_count": len(actuals_df),
                    "actuals_max_date": str(actuals_df["‰ºùÁ•®Êó•‰ªò"].max())
                }
            )
            
            # 3. DB„Åã„Çâ‰∫àÁ¥Ñ„Éá„Éº„ÇøÂèñÂæóÔºàÈÅéÂéª60Êó•+Êú™Êù•7Êó•Ôºâ
            reserve_start = target_date - timedelta(days=60)
            reserve_end = target_date + timedelta(days=7)
            
            logger.info(
                f"üìÖ Exporting reserve: {reserve_start} to {reserve_end}"
            )
            
            reserve_df = self._reserve_exporter.export_daily_reserve(
                start_date=reserve_start,
                end_date=reserve_end
            )
            
            # reserve.csv‰øùÂ≠òÔºàÊó•Êú¨Ë™û„Éò„ÉÉ„ÉÄÔºâ
            reserve_csv_path = workspace / "reserve.csv"
            reserve_df.to_csv(reserve_csv_path, index=False, encoding="utf-8")
            
            logger.info(
                f"‚úÖ Exported {len(reserve_df)} reserve records to {reserve_csv_path}",
                extra={"reserve_count": len(reserve_df)}
            )
            
            # 4. retrain_and_eval.py --quick „ÅßÂ≠¶Áøí‚Üí‰∫àÊ∏¨
            pred_out_csv = workspace / "tplus1_pred.csv"
            log_file = workspace / "run.log"
            
            cmd = [
                "python3",
                str(self._retrain_script_path),
                "--quick",
                "--raw-csv", str(raw_csv_path),
                "--reserve-csv", str(reserve_csv_path),
                "--out-dir", str(out_dir),
                "--pred-out-csv", str(pred_out_csv),
                "--start-date", str(target_date),
                "--log", str(log_file),
            ]
            
            logger.info(
                f"üîÑ Running retrain_and_eval: {' '.join(cmd[:5])}...",
                extra={"full_command": ' '.join(cmd)}
            )
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self._timeout
            )
            
            if result.returncode != 0:
                # „É≠„Ç∞„Éï„Ç°„Ç§„É´Êú´Â∞æ„ÇíÂèñÂæó
                error_detail = ""
                if log_file.exists():
                    with open(log_file, "r") as f:
                        lines = f.readlines()
                        error_detail = "".join(lines[-50:])  # Êú´Â∞æ50Ë°å
                
                raise RuntimeError(
                    f"retrain_and_eval.py failed with rc={result.returncode}\n"
                    f"stdout: {result.stdout}\n"
                    f"stderr: {result.stderr}\n"
                    f"log tail:\n{error_detail}"
                )
            
            logger.info(
                f"‚úÖ retrain_and_eval completed successfully",
                extra={"returncode": result.returncode}
            )
            
            # 5. ÁµêÊûúCSVË™≠„ÅøËæº„Åø‚ÜíDB„Å´‰øùÂ≠ò
            if not pred_out_csv.exists():
                raise FileNotFoundError(
                    f"Prediction output not found: {pred_out_csv}"
                )
            
            pred_df = pd.read_csv(pred_out_csv)
            
            if pred_df.empty:
                raise ValueError("Prediction CSV is empty")
            
            # CSV„Åã„Çâ‰∫àÊ∏¨ÂÄ§„ÇíÂèñÂæóÔºàretrain_and_eval„ÅÆÂá∫ÂäõÂΩ¢Âºè„Å´‰æùÂ≠òÔºâ
            # ÊÉ≥ÂÆö: date, y_pred Á≠â„ÅÆ„Ç´„É©„É†
            # „Å®„Çä„ÅÇ„Åà„ÅöÊúÄÂàù„ÅÆË°å„ÇíÂèñÂæó
            p50 = float(pred_df.iloc[0].get("y_pred", pred_df.iloc[0].iloc[-1]))
            p10 = None  # retrain_and_eval„ÅåÂå∫Èñì‰∫àÊ∏¨„ÇíÂá∫Âäõ„Åó„Å¶„ÅÑ„Çå„Å∞ÂèñÂæó
            p90 = None
            
            logger.info(
                f"üìà Prediction result: p50={p50:.3f}",
                extra={"p50": p50, "p10": p10, "p90": p90}
            )
            
            # input_snapshot‰ΩúÊàê
            input_snapshot = {
                "actuals_start_date": str(actuals_start),
                "actuals_end_date": str(actuals_end),
                "actuals_count": len(actuals_df),
                "reserve_exists": len(reserve_df) > 0,
                "reserve_count": len(reserve_df),
                "model_version": "final_fast_balanced",
                "training_mode": "quick",
                "workspace": str(workspace),
            }
            
            # DB„Å´‰øùÂ≠ò
            self._forecast_result_repo.save_result(
                target_date=target_date,
                job_id=job_id,
                p50=p50,
                p10=p10,
                p90=p90,
                unit="ton",
                input_snapshot=input_snapshot
            )
            
            logger.info(
                f"‚úÖ Saved prediction result to DB",
                extra={
                    "target_date": str(target_date),
                    "job_id": str(job_id),
                    "p50": p50
                }
            )
        
        except Exception as e:
            logger.error(
                f"‚ùå Forecast with training failed",
                exc_info=True,
                extra={
                    "target_date": str(target_date),
                    "job_id": str(job_id),
                    "workspace": str(workspace),
                    "error": str(e)
                }
            )
            raise
