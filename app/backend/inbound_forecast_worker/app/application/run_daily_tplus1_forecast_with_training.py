"""
RunDailyTplus1ForecastWithTrainingUseCase: æ—¥æ¬¡t+1äºˆæ¸¬ï¼ˆå­¦ç¿’è¾¼ã¿ï¼‰
=============================================
æ‰‹é †:
1. workspaceä½œæˆï¼ˆ/tmp/forecast_jobs/{job_id}/ï¼‰
2. DBã‹ã‚‰å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå“ç›®åˆ¥ã€éå»365æ—¥ï¼‰â†’ raw.csv
3. DBã‹ã‚‰äºˆç´„ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéå»60æ—¥+æœªæ¥7æ—¥ï¼‰â†’ reserve.csv
4. retrain_and_eval.py --quick ã§å­¦ç¿’â†’äºˆæ¸¬
5. çµæœCSVèª­ã¿è¾¼ã¿â†’DBã«ä¿å­˜
6. workspaceä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰

Phase 4å®Ÿè£…: DBâ†’å­¦ç¿’â†’äºˆæ¸¬ã®E2E
- retrain_and_eval.py ã‚’ --quick ã§å®Ÿè¡Œ
- å“ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’ stg.shogun_final_receive ã‹ã‚‰å–å¾—
- äºˆç´„ãƒ‡ãƒ¼ã‚¿ã‚’ mart.v_reserve_daily_for_forecast ã‹ã‚‰å–å¾—
"""
from __future__ import annotations

import os
import subprocess
from datetime import date, timedelta
from pathlib import Path
from typing import Optional
from uuid import UUID

import pandas as pd
from sqlalchemy.orm import Session

from backend_shared.application.logging import get_module_logger

logger = get_module_logger(__name__)


class RunDailyTplus1ForecastWithTrainingUseCase:
    """æ—¥æ¬¡t+1äºˆæ¸¬ï¼ˆå­¦ç¿’è¾¼ã¿ï¼‰UseCase"""
    
    def __init__(
        self,
        db_session: Session,
        inbound_actuals_exporter,  # InboundActualsExportPort
        reserve_exporter,  # ReserveExportPort
        forecast_result_repo,  # DailyForecastResultRepositoryPort
        model_metrics_repo,  # ModelMetricsRepositoryPort
        retrain_script_path: Path,
        timeout: int = 1800,
        actuals_lookback_days: int = 540,
    ):
        self._db = db_session
        self._inbound_actuals_exporter = inbound_actuals_exporter
        self._reserve_exporter = reserve_exporter
        self._forecast_result_repo = forecast_result_repo
        self._model_metrics_repo = model_metrics_repo
        self._retrain_script_path = retrain_script_path
        self._timeout = timeout
        self._actuals_lookback_days = actuals_lookback_days
    
    def execute(
        self,
        target_date: date,
        job_id: UUID,
    ) -> None:
        """
        æ—¥æ¬¡t+1äºˆæ¸¬ï¼ˆå­¦ç¿’è¾¼ã¿ï¼‰ã‚’å®Ÿè¡Œ
        
        Args:
            target_date: äºˆæ¸¬å¯¾è±¡æ—¥ï¼ˆæ˜æ—¥ï¼‰
            job_id: ã‚¸ãƒ§ãƒ–ID
        
        Raises:
            Exception: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ã€å­¦ç¿’ã‚¨ãƒ©ãƒ¼ã€äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ç­‰
        """
        logger.info(
            f"ğŸš€ Starting daily t+1 forecast with training",
            extra={
                "target_date": str(target_date),
                "job_id": str(job_id)
            }
        )
        
        # 1. workspaceä½œæˆ
        workspace = Path(f"/tmp/forecast_jobs/{job_id}")
        workspace.mkdir(parents=True, exist_ok=True)
        out_dir = workspace / "out"
        out_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ“ Created workspace: {workspace}")
        
        try:
            # 2. DBã‹ã‚‰å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå“ç›®åˆ¥ï¼‰
            actuals_start = target_date - timedelta(days=self._actuals_lookback_days)
            actuals_end = target_date - timedelta(days=1)  # æ˜¨æ—¥ã¾ã§
            
            logger.info(
                f"ğŸ“Š Exporting actuals: {actuals_start} to {actuals_end}"
            )
            
            actuals_df = self._inbound_actuals_exporter.export_item_level_actuals(
                start_date=actuals_start,
                end_date=actuals_end
            )
            
            if actuals_df.empty:
                raise ValueError(
                    f"No actuals found between {actuals_start} and {actuals_end}"
                )
            
            # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            actuals_max_date = actuals_df["ä¼ç¥¨æ—¥ä»˜"].max()
            avg_weight = actuals_df["æ­£å‘³é‡é‡"].mean()
            
            if actuals_max_date != actuals_end:
                logger.warning(
                    f"âš ï¸ Actuals max date mismatch: expected {actuals_end}, got {actuals_max_date}"
                )
            
            # kgå˜ä½ã®æ¤œè¨¼ï¼ˆ10 kg ï½ 50,000 kgï¼‰
            if avg_weight < 10 or avg_weight > 50000:
                logger.error(
                    f"âŒ Suspicious average weight: {avg_weight:.3f} kg (expected 300ï½1000 kg)"
                )
                raise ValueError(f"Invalid average weight: {avg_weight:.3f} kg")
            
            logger.info(
                f"âœ… Actuals data prepared: {len(actuals_df)} records",
                extra={
                    "actuals_count": len(actuals_df),
                    "actuals_max_date": str(actuals_max_date),
                    "avg_weight_kg": round(avg_weight, 3)
                }
            )
            
            # 3. DBã‹ã‚‰äºˆç´„ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéå»360æ—¥ã€target_dateå½“æ—¥ã¾ã§ï¼‰
            # æ³¨æ„: å­¦ç¿’ã«å¿…è¦ãªæœŸé–“ã‚’ç¢ºä¿ï¼ˆtrain_daily_model.pyã§ä½¿ç”¨ï¼‰
            reserve_start = target_date - timedelta(days=360)
            reserve_end = target_date  # target_dateå½“æ—¥ã¾ã§ï¼ˆäºˆæ¸¬ã«ä½¿ç”¨ï¼‰
            
            logger.info(
                f"ğŸ“… Preparing reserve data range: {reserve_start} to {reserve_end}"
            )
            
            # äºˆç´„ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ãƒ­ã‚°ã®ã¿å‡ºåŠ›ï¼ˆCSVä¿å­˜ã¯å»ƒæ­¢ï¼‰
            reserve_df = self._reserve_exporter.export_daily_reserve(
                start_date=reserve_start,
                end_date=reserve_end
            )
            
            if not reserve_df.empty:
                reserve_dates = pd.to_datetime(reserve_df["äºˆç´„æ—¥"]).dt.date
                target_date_exists = target_date in reserve_dates.values
                if not target_date_exists:
                    logger.warning(
                        f"âš ï¸ Reserve data for target_date {target_date} does not exist. "
                        f"Max reserve date: {reserve_dates.max()}"
                    )
            else:
                logger.warning("âš ï¸ Reserve data is empty")
            
            logger.info(
                f"âœ… Reserve data prepared: {len(reserve_df)} records",
                extra={
                    "reserve_count": len(reserve_df),
                    "reserve_max_date": str(reserve_df["äºˆç´„æ—¥"].max()) if not reserve_df.empty else "N/A"
                }
            )
            
            # 4. retrain_and_eval.py --quick ã§å­¦ç¿’â†’äºˆæ¸¬ï¼ˆDBç›´æ¥å–å¾—ãƒ¢ãƒ¼ãƒ‰ï¼‰
            pred_out_csv = workspace / "tplus1_pred.csv"
            log_file = workspace / "run.log"
            
            # DBæ¥ç¶šæ–‡å­—åˆ—ã®å–å¾—ï¼ˆbackend_sharedã®url_builderã‚’ä½¿ç”¨ï¼‰
            from backend_shared.db.url_builder import build_database_url
            db_url = build_database_url(driver="psycopg", raise_on_missing=True)
            
            cmd = [
                "python3",
                str(self._retrain_script_path),
                "--quick",
                "--use-db",  # CSVå»ƒæ­¢ï¼šDBç›´æ¥å–å¾—ãƒ¢ãƒ¼ãƒ‰
                "--db-connection-string", db_url,
                "--actuals-start-date", str(actuals_start),
                "--actuals-end-date", str(actuals_end),
                "--reserve-start-date", str(reserve_start),
                "--reserve-end-date", str(reserve_end),
                "--out-dir", str(out_dir),
                "--pred-out-csv", str(pred_out_csv),
                "--start-date", str(target_date),
                "--end-date", str(target_date),  # 1æ—¥ã®ã¿äºˆæ¸¬ï¼ˆå¿…é ˆï¼‰
                "--log", str(log_file),
            ]
            
            logger.info(
                f"ğŸ”„ Running retrain_and_eval: {' '.join(cmd[:5])}...",
                extra={"full_command": ' '.join(cmd)}
            )
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self._timeout
            )
            
            if result.returncode != 0:
                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã‚’å–å¾—
                error_detail = ""
                if log_file.exists():
                    with open(log_file, "r") as f:
                        lines = f.readlines()
                        error_detail = "".join(lines[-50:])  # æœ«å°¾50è¡Œ
                
                raise RuntimeError(
                    f"retrain_and_eval.py failed with rc={result.returncode}\n"
                    f"stdout: {result.stdout}\n"
                    f"stderr: {result.stderr}\n"
                    f"log tail:\n{error_detail}"
                )
            
            logger.info(
                f"âœ… retrain_and_eval completed successfully",
                extra={"returncode": result.returncode}
            )
            
            # 5. çµæœCSVèª­ã¿è¾¼ã¿â†’DBã«ä¿å­˜
            if not pred_out_csv.exists():
                raise FileNotFoundError(
                    f"Prediction output not found: {pred_out_csv}"
                )
            
            pred_df = pd.read_csv(pred_out_csv)
            
            if pred_df.empty:
                raise ValueError("Prediction CSV is empty")
            
            # CSVã‹ã‚‰äºˆæ¸¬å€¤ã‚’å–å¾—ï¼ˆp50åˆ—ã‚’å„ªå…ˆã€ãªã‘ã‚Œã°total_predï¼‰
            first_row = pred_df.iloc[0]
            if "p50" in pred_df.columns:
                p50 = float(first_row["p50"])
            elif "total_pred" in pred_df.columns:
                p50 = float(first_row["total_pred"])
            else:
                raise ValueError(f"Required column 'p50' or 'total_pred' not found. Columns: {pred_df.columns.tolist()}")
            
            # p10/p90ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            # çµ±è¨ˆçš„å®šç¾©ï¼ˆPhase 2ï¼‰:
            #   - "p50" â†’ median: 50%åˆ†ä½ç‚¹ï¼ˆQuantileå›å¸° alpha=0.5ï¼‰
            #   - "p90" â†’ upper_quantile_90: 90%åˆ†ä½ç‚¹ï¼ˆQuantileå›å¸° alpha=0.9ï¼‰
            #   - "p10" â†’ lower_1sigma: median - 1.28Ïƒï¼ˆæ­£è¦åˆ†å¸ƒä»®å®šã€çœŸã®10%åˆ†ä½ç‚¹ã§ã¯ãªã„ï¼‰
            # CSVã®"total_pred_low_1sigma", "total_pred_high_1sigma"ã‚‚åŒã˜æ„å‘³ï¼ˆtotal_pred Â± 1Ïƒï¼‰
            p10 = None
            p90 = None
            
            # quantileå›å¸°ã®å€¤ã‚’å„ªå…ˆä½¿ç”¨
            if "p50" in pred_df.columns and "p90" in pred_df.columns:
                # p90ï¼ˆupper_quantile_90ï¼‰ã‹ã‚‰Ïƒã‚’é€†ç®—ã—ã¦p10ï¼ˆlower_1sigmaï¼‰ã‚’æ¨å®š
                # è¨ˆç®—å¼: p90 = p50 + 1.28Ïƒ â†’ Ïƒ = (p90 - p50) / 1.28 â†’ p10 = p50 - 1.28Ïƒ
                p90_raw = float(first_row["p90"])
                if p90_raw > p50:
                    z90 = 1.2815515655446004  # æ­£è¦åˆ†å¸ƒã®80%ç‚¹ï¼ˆç‰‡å´ï¼‰ã®zå€¤
                    sigma = (p90_raw - p50) / z90
                    z10 = -1.2815515655446004  # æ­£è¦åˆ†å¸ƒã®10%ç‚¹ï¼ˆç‰‡å´ï¼‰ã®zå€¤
                    p10 = max(0.0, p50 + z10 * sigma)  # lower_1sigmaï¼ˆéè² åˆ¶ç´„ï¼‰
                    p90 = p90_raw  # upper_quantile_90
                else:
                    # p90ãŒp50ä»¥ä¸‹ã®å ´åˆ (zero_capç­‰ã§ã‚­ãƒ£ãƒƒãƒ—ã•ã‚ŒãŸå ´åˆ)
                    # Ïƒãƒ™ãƒ¼ã‚¹ã®å€¤ã‚’ä½¿ç”¨
                    if "total_pred_low_1sigma" in pred_df.columns and "total_pred_high_1sigma" in pred_df.columns:
                        p10 = float(first_row["total_pred_low_1sigma"])
                        p90 = float(first_row["total_pred_high_1sigma"])
            elif "total_pred_low_1sigma" in pred_df.columns and "total_pred_high_1sigma" in pred_df.columns:
                # quantileå€¤ãŒãªã„å ´åˆã¯Ïƒãƒ™ãƒ¼ã‚¹ã®å€¤ã‚’ä½¿ç”¨ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
                p10 = float(first_row["total_pred_low_1sigma"])
                p90 = float(first_row["total_pred_high_1sigma"])
            
            # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆkgå˜ä½ï¼‰
            if p50 < 1000.0 or p50 > 200000.0:
                logger.warning(
                    f"âš ï¸ Prediction value out of reasonable range: p50={p50:.3f} kg",
                    extra={"p50": p50, "min_expected": 1000.0, "max_expected": 200000.0}
                )
            
            logger.info(
                f"ğŸ“ˆ Prediction result: p50={p50:.3f} kg",
                extra={"p50": p50, "p10": p10, "p90": p90}
            )
            
            # input_snapshotä½œæˆ
            input_snapshot = {
                "actuals_start_date": str(actuals_start),
                "actuals_end_date": str(actuals_end),
                "actuals_count": len(actuals_df),
                "reserve_exists": len(reserve_df) > 0,
                "reserve_count": len(reserve_df),
                "model_version": "final_fast_balanced",
                "training_mode": "quick",
                "workspace": str(workspace),
            }
            
            # DBã«ä¿å­˜
            self._forecast_result_repo.save_result(
                target_date=target_date,
                job_id=job_id,
                p50=p50,
                p10=p10,
                p90=p90,
                unit="kg",
                input_snapshot=input_snapshot
            )
            
            logger.info(
                f"âœ… Saved prediction result to DB",
                extra={
                    "target_date": str(target_date),
                    "job_id": str(job_id),
                    "p50": p50
                }
            )
            
            # 6. ãƒ¢ãƒ‡ãƒ«ç²¾åº¦æŒ‡æ¨™ã‚’DBã«ä¿å­˜
            # train_daily_model.py ãŒ scores_walkforward.json ã‚’å‡ºåŠ›ã—ã¦ã„ã‚‹ãŸã‚èª­ã¿å–ã‚Š
            scores_file = out_dir / "scores_walkforward.json"
            if scores_file.exists():
                self._save_model_metrics(
                    job_id=job_id,
                    scores_file=scores_file,
                    actuals_start=actuals_start,
                    actuals_end=actuals_end
                )
            else:
                logger.warning(
                    f"âš ï¸ Model metrics file not found: {scores_file}. Skipping metrics save."
                )
        
        except Exception as e:
            logger.error(
                f"âŒ Forecast with training failed",
                exc_info=True,
                extra={
                    "target_date": str(target_date),
                    "job_id": str(job_id),
                    "workspace": str(workspace),
                    "error": str(e)
                }
            )
            raise
    
    def _save_model_metrics(
        self,
        job_id: UUID,
        scores_file: Path,
        actuals_start: date,
        actuals_end: date
    ) -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ç²¾åº¦æŒ‡æ¨™ã‚’DBã«ä¿å­˜
        
        Args:
            job_id: äºˆæ¸¬ã‚¸ãƒ§ãƒ–ID
            scores_file: scores_walkforward.json ã®ãƒ‘ã‚¹
            actuals_start: å­¦ç¿’é–‹å§‹æ—¥
            actuals_end: å­¦ç¿’çµ‚äº†æ—¥
        """
        import json
        from app.ports.model_metrics_repository import ModelMetrics
        
        try:
            with open(scores_file, "r") as f:
                scores = json.load(f)
            
            # train_daily_model.py L773-783 ã§å‡ºåŠ›ã•ã‚Œã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            # {"R2_total": 0.605, "MAE_total": 13.56, "R2_sum_only": 0.611, "MAE_sum_only": 13.44, 
            #  "n_days": 245, "config": {...}}
            
            metrics = ModelMetrics(
                job_id=job_id,
                model_name="daily_tplus1",
                model_version="final_fast_balanced",
                train_window_start=actuals_start,
                train_window_end=actuals_end,
                eval_method="walk_forward",
                mae=scores.get("MAE_total"),
                r2=scores.get("R2_total"),
                n_samples=scores.get("n_days", 0),
                rmse=None,  # train_daily_model.py ã§ã¯è¨ˆç®—ã—ã¦ã„ãªã„
                mape=None,  # train_daily_model.py ã§ã¯è¨ˆç®—ã—ã¦ã„ãªã„
                mae_sum_only=scores.get("MAE_sum_only"),
                r2_sum_only=scores.get("R2_sum_only"),
                unit="kg",
                metadata=scores.get("config")
            )
            
            metrics_id = self._model_metrics_repo.save_metrics(metrics)
            
            logger.info(
                f"âœ… Saved model metrics to DB",
                extra={
                    "metrics_id": str(metrics_id),
                    "job_id": str(job_id),
                    "mae": metrics.mae,
                    "r2": metrics.r2,
                    "n_samples": metrics.n_samples
                }
            )
        except Exception as e:
            logger.error(
                f"âŒ Failed to save model metrics",
                exc_info=True,
                extra={
                    "job_id": str(job_id),
                    "scores_file": str(scores_file),
                    "error": str(e)
                }
            )
