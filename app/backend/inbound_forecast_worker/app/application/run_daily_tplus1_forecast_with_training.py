"""
RunDailyTplus1ForecastWithTrainingUseCase: æ—¥æ¬¡t+1äºˆæ¸¬ï¼ˆå­¦ç¿’è¾¼ã¿ï¼‰
=============================================
æ‰‹é †:
1. workspaceä½œæˆï¼ˆ/tmp/forecast_jobs/{job_id}/ï¼‰
2. DBã‹ã‚‰å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå“ç›®åˆ¥ã€éå»365æ—¥ï¼‰â†’ raw.csv
3. DBã‹ã‚‰äºˆç´„ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéå»60æ—¥+æœªæ¥7æ—¥ï¼‰â†’ reserve.csv
4. retrain_and_eval.py --quick ã§å­¦ç¿’â†’äºˆæ¸¬
5. çµæœCSVèª­ã¿è¾¼ã¿â†’DBã«ä¿å­˜
6. workspaceä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰

Phase 4å®Ÿè£…: DBâ†’å­¦ç¿’â†’äºˆæ¸¬ã®E2E
- retrain_and_eval.py ã‚’ --quick ã§å®Ÿè¡Œ
- å“ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’ stg.shogun_final_receive ã‹ã‚‰å–å¾—
- äºˆç´„ãƒ‡ãƒ¼ã‚¿ã‚’ mart.v_reserve_daily_for_forecast ã‹ã‚‰å–å¾—
"""
from __future__ import annotations

import os
import subprocess
from datetime import date, timedelta
from pathlib import Path
from typing import Optional
from uuid import UUID

import pandas as pd
from sqlalchemy.orm import Session

from backend_shared.application.logging import get_module_logger

logger = get_module_logger(__name__)


class RunDailyTplus1ForecastWithTrainingUseCase:
    """æ—¥æ¬¡t+1äºˆæ¸¬ï¼ˆå­¦ç¿’è¾¼ã¿ï¼‰UseCase"""
    
    def __init__(
        self,
        db_session: Session,
        inbound_actuals_exporter,  # InboundActualsExportPort
        reserve_exporter,  # ReserveExportPort
        forecast_result_repo,  # DailyForecastResultRepositoryPort
        retrain_script_path: Path,
        timeout: int = 1800,
    ):
        self._db = db_session
        self._inbound_actuals_exporter = inbound_actuals_exporter
        self._reserve_exporter = reserve_exporter
        self._forecast_result_repo = forecast_result_repo
        self._retrain_script_path = retrain_script_path
        self._timeout = timeout
    
    def execute(
        self,
        target_date: date,
        job_id: UUID,
    ) -> None:
        """
        æ—¥æ¬¡t+1äºˆæ¸¬ï¼ˆå­¦ç¿’è¾¼ã¿ï¼‰ã‚’å®Ÿè¡Œ
        
        Args:
            target_date: äºˆæ¸¬å¯¾è±¡æ—¥ï¼ˆæ˜æ—¥ï¼‰
            job_id: ã‚¸ãƒ§ãƒ–ID
        
        Raises:
            Exception: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ã€å­¦ç¿’ã‚¨ãƒ©ãƒ¼ã€äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ç­‰
        """
        logger.info(
            f"ğŸš€ Starting daily t+1 forecast with training",
            extra={
                "target_date": str(target_date),
                "job_id": str(job_id)
            }
        )
        
        # 1. workspaceä½œæˆ
        workspace = Path(f"/tmp/forecast_jobs/{job_id}")
        workspace.mkdir(parents=True, exist_ok=True)
        out_dir = workspace / "out"
        out_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ“ Created workspace: {workspace}")
        
        try:
            # 2. DBã‹ã‚‰å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå“ç›®åˆ¥ã€éå»365æ—¥ï¼‰
            actuals_start = target_date - timedelta(days=365)
            actuals_end = target_date - timedelta(days=1)  # æ˜¨æ—¥ã¾ã§
            
            logger.info(
                f"ğŸ“Š Exporting actuals: {actuals_start} to {actuals_end}"
            )
            
            actuals_df = self._inbound_actuals_exporter.export_item_level_actuals(
                start_date=actuals_start,
                end_date=actuals_end
            )
            
            if actuals_df.empty:
                raise ValueError(
                    f"No actuals found between {actuals_start} and {actuals_end}"
                )
            
            # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            actuals_max_date = actuals_df["ä¼ç¥¨æ—¥ä»˜"].max()
            avg_weight = actuals_df["æ­£å‘³é‡é‡"].mean()
            
            if actuals_max_date != actuals_end:
                logger.warning(
                    f"âš ï¸ Actuals max date mismatch: expected {actuals_end}, got {actuals_max_date}"
                )
            
            if avg_weight < 0.01 or avg_weight > 50:
                logger.error(
                    f"âŒ Suspicious average weight: {avg_weight:.3f} ton (expected 0.5ï½5.0)"
                )
                raise ValueError(f"Invalid average weight: {avg_weight:.3f} ton")
            
            logger.info(
                f"âœ… Actuals data prepared: {len(actuals_df)} records",
                extra={
                    "actuals_count": len(actuals_df),
                    "actuals_max_date": str(actuals_max_date),
                    "avg_weight_ton": round(avg_weight, 3)
                }
            )
            
            # 3. DBã‹ã‚‰äºˆç´„ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéå»360æ—¥ã€target_dateå½“æ—¥ã¾ã§ï¼‰
            # æ³¨æ„: å­¦ç¿’ã«å¿…è¦ãªæœŸé–“ã‚’ç¢ºä¿ï¼ˆtrain_daily_model.pyã§ä½¿ç”¨ï¼‰
            reserve_start = target_date - timedelta(days=360)
            reserve_end = target_date  # target_dateå½“æ—¥ã¾ã§ï¼ˆäºˆæ¸¬ã«ä½¿ç”¨ï¼‰
            
            logger.info(
                f"ğŸ“… Preparing reserve data range: {reserve_start} to {reserve_end}"
            )
            
            # äºˆç´„ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ãƒ­ã‚°ã®ã¿å‡ºåŠ›ï¼ˆCSVä¿å­˜ã¯å»ƒæ­¢ï¼‰
            reserve_df = self._reserve_exporter.export_daily_reserve(
                start_date=reserve_start,
                end_date=reserve_end
            )
            
            if not reserve_df.empty:
                reserve_dates = pd.to_datetime(reserve_df["äºˆç´„æ—¥"]).dt.date
                target_date_exists = target_date in reserve_dates.values
                if not target_date_exists:
                    logger.warning(
                        f"âš ï¸ Reserve data for target_date {target_date} does not exist. "
                        f"Max reserve date: {reserve_dates.max()}"
                    )
            else:
                logger.warning("âš ï¸ Reserve data is empty")
            
            logger.info(
                f"âœ… Reserve data prepared: {len(reserve_df)} records",
                extra={
                    "reserve_count": len(reserve_df),
                    "reserve_max_date": str(reserve_df["äºˆç´„æ—¥"].max()) if not reserve_df.empty else "N/A"
                }
            )
            
            # 4. retrain_and_eval.py --quick ã§å­¦ç¿’â†’äºˆæ¸¬ï¼ˆDBç›´æ¥å–å¾—ãƒ¢ãƒ¼ãƒ‰ï¼‰
            pred_out_csv = workspace / "tplus1_pred.csv"
            log_file = workspace / "run.log"
            
            # DBæ¥ç¶šæ–‡å­—åˆ—ã®å–å¾—ï¼ˆbackend_sharedã®url_builderã‚’ä½¿ç”¨ï¼‰
            from backend_shared.db.url_builder import build_database_url
            db_url = build_database_url(driver="psycopg", raise_on_missing=True)
            
            cmd = [
                "python3",
                str(self._retrain_script_path),
                "--quick",
                "--use-db",  # CSVå»ƒæ­¢ï¼šDBç›´æ¥å–å¾—ãƒ¢ãƒ¼ãƒ‰
                "--db-connection-string", db_url,
                "--actuals-start-date", str(actuals_start),
                "--actuals-end-date", str(actuals_end),
                "--reserve-start-date", str(reserve_start),
                "--reserve-end-date", str(reserve_end),
                "--out-dir", str(out_dir),
                "--pred-out-csv", str(pred_out_csv),
                "--start-date", str(target_date),
                "--end-date", str(target_date),  # 1æ—¥ã®ã¿äºˆæ¸¬ï¼ˆå¿…é ˆï¼‰
                "--log", str(log_file),
            ]
            
            logger.info(
                f"ğŸ”„ Running retrain_and_eval: {' '.join(cmd[:5])}...",
                extra={"full_command": ' '.join(cmd)}
            )
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self._timeout
            )
            
            if result.returncode != 0:
                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã‚’å–å¾—
                error_detail = ""
                if log_file.exists():
                    with open(log_file, "r") as f:
                        lines = f.readlines()
                        error_detail = "".join(lines[-50:])  # æœ«å°¾50è¡Œ
                
                raise RuntimeError(
                    f"retrain_and_eval.py failed with rc={result.returncode}\n"
                    f"stdout: {result.stdout}\n"
                    f"stderr: {result.stderr}\n"
                    f"log tail:\n{error_detail}"
                )
            
            logger.info(
                f"âœ… retrain_and_eval completed successfully",
                extra={"returncode": result.returncode}
            )
            
            # 5. çµæœCSVèª­ã¿è¾¼ã¿â†’DBã«ä¿å­˜
            if not pred_out_csv.exists():
                raise FileNotFoundError(
                    f"Prediction output not found: {pred_out_csv}"
                )
            
            pred_df = pd.read_csv(pred_out_csv)
            
            if pred_df.empty:
                raise ValueError("Prediction CSV is empty")
            
            # CSVã‹ã‚‰äºˆæ¸¬å€¤ã‚’å–å¾—ï¼ˆp50åˆ—ã‚’å„ªå…ˆã€ãªã‘ã‚Œã°total_predï¼‰
            first_row = pred_df.iloc[0]
            if "p50" in pred_df.columns:
                p50 = float(first_row["p50"])
            elif "total_pred" in pred_df.columns:
                p50 = float(first_row["total_pred"])
            else:
                raise ValueError(f"Required column 'p50' or 'total_pred' not found. Columns: {pred_df.columns.tolist()}")
            
            # p10/p90ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            # æ³¨æ„: CSVåˆ—ã¯ç•°ãªã‚‹æ„å‘³ã‚’æŒã¤
            #   - "p50", "p90": quantileå›å¸°ã«ã‚ˆã‚‹50%/90%åˆ†ä½ç‚¹
            #   - "total_pred_low_1sigma", "total_pred_high_1sigma": total_pred Â± 1Ïƒ
            # DBã«ã¯æœ¬æ¥ã®quantileå€¤ã‚’ä¿å­˜ã™ã¹ã
            p10 = None
            p90 = None
            
            # quantileå›å¸°ã®å€¤ã‚’å„ªå…ˆä½¿ç”¨
            if "p50" in pred_df.columns and "p90" in pred_df.columns:
                # p90ã‹ã‚‰Ïƒã‚’é€†ç®—ã—ã¦p10ã‚’æ¨å®š (p90 = p50 + 1.28Ïƒ ã¨ä»®å®š)
                p90_raw = float(first_row["p90"])
                if p90_raw > p50:
                    z90 = 1.2815515655446004  # 80%åˆ†ä½ç‚¹ã®zå€¤
                    sigma = (p90_raw - p50) / z90
                    z10 = -1.2815515655446004  # 20%åˆ†ä½ç‚¹ã®zå€¤
                    p10 = max(0.0, p50 + z10 * sigma)
                    p90 = p90_raw
                else:
                    # p90ãŒp50ä»¥ä¸‹ã®å ´åˆ (zero_capç­‰ã§ã‚­ãƒ£ãƒƒãƒ—ã•ã‚ŒãŸå ´åˆ)
                    # Ïƒãƒ™ãƒ¼ã‚¹ã®å€¤ã‚’ä½¿ç”¨
                    if "total_pred_low_1sigma" in pred_df.columns and "total_pred_high_1sigma" in pred_df.columns:
                        p10 = float(first_row["total_pred_low_1sigma"])
                        p90 = float(first_row["total_pred_high_1sigma"])
            elif "total_pred_low_1sigma" in pred_df.columns and "total_pred_high_1sigma" in pred_df.columns:
                # quantileå€¤ãŒãªã„å ´åˆã¯Ïƒãƒ™ãƒ¼ã‚¹ã®å€¤ã‚’ä½¿ç”¨ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
                p10 = float(first_row["total_pred_low_1sigma"])
                p90 = float(first_row["total_pred_high_1sigma"])
            
            # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
            if p50 < 1.0 or p50 > 200.0:
                logger.warning(
                    f"âš ï¸ Prediction value out of reasonable range: p50={p50:.3f} ton",
                    extra={"p50": p50, "min_expected": 1.0, "max_expected": 200.0}
                )
            
            logger.info(
                f"ğŸ“ˆ Prediction result: p50={p50:.3f}",
                extra={"p50": p50, "p10": p10, "p90": p90}
            )
            
            # input_snapshotä½œæˆ
            input_snapshot = {
                "actuals_start_date": str(actuals_start),
                "actuals_end_date": str(actuals_end),
                "actuals_count": len(actuals_df),
                "reserve_exists": len(reserve_df) > 0,
                "reserve_count": len(reserve_df),
                "model_version": "final_fast_balanced",
                "training_mode": "quick",
                "workspace": str(workspace),
            }
            
            # DBã«ä¿å­˜
            self._forecast_result_repo.save_result(
                target_date=target_date,
                job_id=job_id,
                p50=p50,
                p10=p10,
                p90=p90,
                unit="ton",
                input_snapshot=input_snapshot
            )
            
            logger.info(
                f"âœ… Saved prediction result to DB",
                extra={
                    "target_date": str(target_date),
                    "job_id": str(job_id),
                    "p50": p50
                }
            )
        
        except Exception as e:
            logger.error(
                f"âŒ Forecast with training failed",
                exc_info=True,
                extra={
                    "target_date": str(target_date),
                    "job_id": str(job_id),
                    "workspace": str(workspace),
                    "error": str(e)
                }
            )
            raise
