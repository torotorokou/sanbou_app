import pandas as pd
from typing import List, Tuple, Optional
from app.api.services.manage_report_processors.factory_report.utils.logger import (
    app_logger,
)
from app.api.services.manage_report_processors.factory_report.utils.summary_tools import (
    safe_merge_by_keys,
    summary_update_column_if_notna,
)


def apply_negation_filters(
    df: pd.DataFrame, match_df: pd.DataFrame, key_cols: List[str], logger=None
) -> pd.DataFrame:
    """
    match_df ã® key_cols ã« `Notå€¤` ã¾ãŸã¯ `NOTå€¤` ãŒã‚ã‚Œã°ã€ãã®å€¤ã‚’é™¤å¤–ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ã‚’ df ã«é©ç”¨ã€‚

    Parameters:
        df (pd.DataFrame): ãƒ•ã‚£ãƒ«ã‚¿å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        match_df (pd.DataFrame): ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        key_cols (List[str]): ã‚­ãƒ¼åˆ—ã®ãƒªã‚¹ãƒˆ
        logger: ãƒ­ã‚¬ãƒ¼

    Returns:
        pd.DataFrame: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    filter_conditions = {}
    for col in key_cols:
        if col not in df.columns:
            if logger:
                logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã«åˆ— '{col}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        if col in match_df.columns:
            unique_vals = match_df[col].dropna().unique()
            neg_vals = [
                v[3:]
                for v in unique_vals
                if isinstance(v, str) and v.lower().startswith("not")
            ]
            if neg_vals:
                filter_conditions[col] = neg_vals
                if logger:
                    logger.info(
                        f"ğŸš« '{col}' ã«å¯¾ã—ã¦å¦å®šãƒ•ã‚£ãƒ«ã‚¿: {', '.join(neg_vals)} ã‚’é©ç”¨ã—ã¾ã—ãŸ"
                    )

    for col, ng_values in filter_conditions.items():
        df = df[~df[col].isin(ng_values)]

    return df


def process_sheet_partition(
    master_csv: pd.DataFrame, sheet_name: str, expected_level: int, logger=None
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    æŒ‡å®šã‚·ãƒ¼ãƒˆã‹ã‚‰ key_level ä¸€è‡´è¡Œã¨ä¸ä¸€è‡´è¡Œã‚’åˆ†é›¢ã€‚

    Parameters:
        master_csv (pd.DataFrame): ãƒã‚¹ã‚¿ãƒ¼CSV
        sheet_name (str): ã‚·ãƒ¼ãƒˆå
        expected_level (int): æœŸå¾…ã™ã‚‹ã‚­ãƒ¼ãƒ¬ãƒ™ãƒ«
        logger: ãƒ­ã‚¬ãƒ¼

    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: (ä¸€è‡´è¡Œ, ä¸ä¸€è‡´è¡Œ)
    """
    sheet_df = (
        master_csv[master_csv["CSVã‚·ãƒ¼ãƒˆå"] == sheet_name].copy()
        if "CSVã‚·ãƒ¼ãƒˆå" in master_csv.columns
        else master_csv.copy()
    )

    if "key_level" in sheet_df.columns:
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¿½åŠ 
        print("ğŸ” process_sheet_partition ãƒ‡ãƒãƒƒã‚°:")
        print(f"  sheet_name: '{sheet_name}'")
        print(f"  expected_level: {expected_level} (å‹: {type(expected_level)})")
        print(f"  sheet_df ã®è¡Œæ•°: {len(sheet_df)}")
        print(f"  key_level ã‚«ãƒ©ãƒ ã®å€¤: {sheet_df['key_level'].unique()}")
        print(f"  key_level ã‚«ãƒ©ãƒ ã®å‹: {sheet_df['key_level'].dtype}")
        print(
            f"  key_level ã®ä¸€æ„ãªå€¤ã¨å‹: {[(v, type(v)) for v in sheet_df['key_level'].unique()]}"
        )

        match_df = sheet_df[sheet_df["key_level"] == expected_level].copy()
        remain_df = sheet_df[sheet_df["key_level"] != expected_level].copy()

        print(f"  match_df ã®è¡Œæ•°: {len(match_df)}")
        print(f"  remain_df ã®è¡Œæ•°: {len(remain_df)}")
    else:
        # key_levelã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯å…¨ä½“ã‚’å¯¾è±¡ã¨ã™ã‚‹
        match_df = sheet_df.copy()
        remain_df = pd.DataFrame()

    return match_df, remain_df


def summary_apply_by_sheet(
    master_csv: pd.DataFrame,
    data_df: pd.DataFrame,
    sheet_name: str,
    key_cols: List[str],
    source_col: str = "æ­£å‘³é‡é‡",
    target_col: str = "å€¤",
) -> pd.DataFrame:
    """
    æŒ‡å®šã•ã‚ŒãŸã‚·ãƒ¼ãƒˆã¨ã‚­ãƒ¼åˆ—ã«åŸºã¥ã„ã¦é›†è¨ˆå‡¦ç†ã‚’é©ç”¨

    Parameters:
        master_csv (pd.DataFrame): ãƒã‚¹ã‚¿ãƒ¼CSV
        data_df (pd.DataFrame): ãƒ‡ãƒ¼ã‚¿
        sheet_name (str): ã‚·ãƒ¼ãƒˆå
        key_cols (List[str]): ã‚­ãƒ¼åˆ—ã®ãƒªã‚¹ãƒˆ
        source_col (str): ã‚½ãƒ¼ã‚¹åˆ—å
        target_col (str): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—å

    Returns:
        pd.DataFrame: å‡¦ç†æ¸ˆã¿ã®ãƒã‚¹ã‚¿ãƒ¼CSV
    """
    logger = app_logger()
    logger.info(f"â–¶ï¸ ã‚·ãƒ¼ãƒˆ: {sheet_name}, ã‚­ãƒ¼: {key_cols}, é›†è¨ˆåˆ—: {source_col}")

    # ãƒ‡ãƒãƒƒã‚°ï¼šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ãƒ­ã‚°
    logger.info(f"ğŸ” data_df ã®ã‚«ãƒ©ãƒ : {data_df.columns.tolist()}")
    logger.info(f"ğŸ” '{source_col}' ã‚«ãƒ©ãƒ ã®å­˜åœ¨: {source_col in data_df.columns}")
    if source_col in data_df.columns:
        logger.info(
            f"ğŸ” '{source_col}' ã‚«ãƒ©ãƒ ã®ã‚µãƒ³ãƒ—ãƒ«å€¤: {data_df[source_col].head().tolist()}"
        )

    # è©²å½“ã‚·ãƒ¼ãƒˆã® key_level ãƒ•ã‚£ãƒ«ã‚¿
    expected_level = len(key_cols)
    match_df, remain_df = process_sheet_partition(
        master_csv, sheet_name, expected_level, logger
    )

    if match_df.empty:
        logger.info(
            f"âš ï¸ key_level={expected_level} ã«ä¸€è‡´ã™ã‚‹è¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        )
        return master_csv

    # notæ¤œç´¢ã‚’é©ç”¨ï¼ˆNotå€¤ã®ã‚ã‚‹è¡Œã‚’é™¤å¤–ï¼‰
    filtered_data_df = apply_negation_filters(
        data_df.copy(), match_df, key_cols, logger
    )

    # ãƒ‡ãƒãƒƒã‚°ï¼šãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ‡ãƒ¼ã‚¿
    logger.info(
        f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿å¾Œ filtered_data_df ã®ã‚«ãƒ©ãƒ : {filtered_data_df.columns.tolist()}"
    )
    logger.info(
        f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿å¾Œ '{source_col}' ã‚«ãƒ©ãƒ ã®å­˜åœ¨: {source_col in filtered_data_df.columns}"
    )

    # ãƒãƒ¼ã‚¸ç”¨ key ã‚’å†å®šç¾©ï¼ˆNotã€‡ã€‡ã‚’å«ã‚€åˆ—ã‚’é™¤å¤–ï¼‰
    merge_key_cols = []
    for col in key_cols:
        if col in match_df.columns and col in filtered_data_df.columns:
            has_neg = any(
                isinstance(val, str) and val.lower().startswith("not")
                for val in match_df[col].dropna().unique()
            )
            if not has_neg:
                merge_key_cols.append(col)
            else:
                logger.info(f"âš ï¸ '{col}' ã« 'Not' æŒ‡å®šãŒã‚ã‚‹ãŸã‚ãƒãƒ¼ã‚¸ã‚­ãƒ¼ã‹ã‚‰é™¤å¤–")

    if not merge_key_cols:
        logger.warning("âŒ æœ‰åŠ¹ãªãƒãƒ¼ã‚¸ã‚­ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return master_csv

    # é›†è¨ˆ
    if source_col in filtered_data_df.columns:
        logger.info(f"âœ… '{source_col}' ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ãŸã‚é›†è¨ˆå‡¦ç†ã‚’å®Ÿè¡Œ")
        agg_df = filtered_data_df.groupby(merge_key_cols, as_index=False)[
            [source_col]
        ].sum()

        # ãƒãƒ¼ã‚¸
        merged_df = safe_merge_by_keys(match_df, agg_df, merge_key_cols)
        merged_df = summary_update_column_if_notna(merged_df, source_col, target_col)

        # æ­£å‘³é‡é‡ã®å‰Šé™¤
        if source_col in merged_df.columns:
            merged_df.drop(columns=[source_col], inplace=True)

        # æœ€çµ‚çµåˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã®ä»–ã‚·ãƒ¼ãƒˆ + æ®‹ä½™ + ãƒãƒ¼ã‚¸çµæœï¼‰
        if "CSVã‚·ãƒ¼ãƒˆå" in master_csv.columns:
            master_others = master_csv[master_csv["CSVã‚·ãƒ¼ãƒˆå"] != sheet_name]
            final_df = pd.concat(
                [master_others, remain_df, merged_df], ignore_index=True
            )
        else:
            final_df = pd.concat([remain_df, merged_df], ignore_index=True)

        return final_df
    else:
        logger.warning(
            f"âš ï¸ '{source_col}' ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„ãŸã‚é›†è¨ˆå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™"
        )
        return master_csv
