# CSVå»ƒæ­¢â†’DBç›´çµï¼šæ—¥æ¬¡t+1äºˆæ¸¬ã®å®Ÿè£…è¨ˆç”»æ›¸

## ğŸ“Š èª¿æŸ»çµæœ

### 1. DBã‚¹ã‚­ãƒ¼ãƒ

#### 1.1 å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆå“ç›®åˆ¥ï¼‰

**ãƒ†ãƒ¼ãƒ–ãƒ«**: `stg.shogun_final_receive`

| ã‚«ãƒ©ãƒ åï¼ˆè‹±èªï¼‰ | å‹ | èª¬æ˜ | 
|--------------|------|------|
| slip_date | date | ä¼ç¥¨æ—¥ä»˜ |
| item_name | text | å“å |
| net_weight | numeric | æ­£å‘³é‡é‡ï¼ˆkgï¼‰ |
| is_deleted | boolean | å‰Šé™¤ãƒ•ãƒ©ã‚° |

**ç¾åœ¨ã®CSVåˆ—åï¼ˆæ—¥æœ¬èªï¼‰**:
- `ä¼ç¥¨æ—¥ä»˜`: slip_date
- `å“å`: item_name
- `æ­£å‘³é‡é‡`: net_weight / 1000.0 (kgâ†’tonå¤‰æ›)

#### 1.2 äºˆç´„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆæ—¥æ¬¡é›†è¨ˆï¼‰

**ãƒ“ãƒ¥ãƒ¼**: `mart.v_reserve_daily_for_forecast`

| ã‚«ãƒ©ãƒ åï¼ˆè‹±èªï¼‰ | å‹ | èª¬æ˜ |
|--------------|------|------|
| date | date | äºˆç´„æ—¥ |
| reserve_trucks | bigint | å°æ•° |
| reserve_fixed_trucks | bigint | å›ºå®šå®¢å°æ•° |
| reserve_fixed_ratio | numeric | å›ºå®šå®¢æ¯”ç‡ |
| source | text | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ |

**ç¾åœ¨ã®CSVåˆ—åï¼ˆæ—¥æœ¬èªï¼‰**:
- `äºˆç´„æ—¥`: date
- `å°æ•°`: reserve_trucks
- `å›ºå®šå®¢`: reserve_fixed_trucks

#### 1.3 å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆæ—¥æ¬¡é›†è¨ˆï¼‰â€»åˆ©ç”¨å¯èƒ½ã ãŒç¾åœ¨æœªä½¿ç”¨

**ãƒ“ãƒ¥ãƒ¼**: `mart.mv_receive_daily` (Materialized View)

| ã‚«ãƒ©ãƒ åï¼ˆè‹±èªï¼‰ | å‹ | èª¬æ˜ |
|--------------|------|------|
| ddate | date | æ—¥ä»˜ |
| receive_net_ton | numeric(18,3) | æ­£å‘³é‡é‡åˆè¨ˆï¼ˆtonï¼‰ |
| receive_vehicle_count | integer | è»Šä¸¡å°æ•° |
| avg_weight_kg_per_vehicle | numeric(18,3) | è»Šä¸¡ã‚ãŸã‚Šå¹³å‡é‡é‡ï¼ˆkgï¼‰ |

**æ³¨**: train_daily_model.py ã¯å“ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€æ—¥æ¬¡é›†è¨ˆã§ã¯ä¸ååˆ†ã€‚stg.shogun_final_receive ã‚’ç¶™ç¶šä½¿ç”¨ã™ã‚‹ã€‚

---

### 2. å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å¼•æ•°è¦ä»¶

#### 2.1 train_daily_model.py (1258è¡Œ)

**å¿…é ˆå…¥åŠ›å¼•æ•°**:
```python
--raw-csv: str  # å®Ÿç¸¾CSVï¼ˆå¿…é ˆã§ã¯ãªã„ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‹ç”¨ï¼‰
--raw-date-col: str = "ä¼ç¥¨æ—¥ä»˜"  # æ—¥ä»˜åˆ—å
--raw-item-col: str = "å“å"  # å“ç›®åˆ—å
--raw-weight-col: str = "æ­£å‘³é‡é‡"  # é‡é‡åˆ—åï¼ˆtonå˜ä½æƒ³å®šï¼‰

--reserve-csv: str (optional)  # äºˆç´„CSV
--reserve-date-col: str = "äºˆç´„æ—¥"
--reserve-count-col: str = "å°æ•°"
--reserve-fixed-col: str = "å›ºå®šå®¢"

--out-dir: str (required)  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
--save-bundle: str (optional)  # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹
```

**å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å½¢å¼**:
- å®Ÿç¸¾CSV: å“ç›®åˆ¥æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ1è¡Œ=1å“ç›®Ã—1æ—¥ï¼‰
  - åˆ—: [ä¼ç¥¨æ—¥ä»˜, å“å, æ­£å‘³é‡é‡]
  - æ­£å‘³é‡é‡: tonå˜ä½
- äºˆç´„CSV: æ—¥æ¬¡é›†è¨ˆãƒ‡ãƒ¼ã‚¿ï¼ˆ1è¡Œ=1æ—¥ï¼‰
  - åˆ—: [äºˆç´„æ—¥, å°æ•°, å›ºå®šå®¢]

**å†…éƒ¨å‡¦ç†**:
- pandas.read_csv() ã§ãƒ­ãƒ¼ãƒ‰
- åˆ—åã¯æ—¥æœ¬èªæƒ³å®šï¼ˆå¼•æ•°ã§å¤‰æ›´å¯èƒ½ï¼‰
- Stage1: å“ç›®åˆ¥OOFãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
- Stage2: åˆè¨ˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

#### 2.2 serve_predict_model_v4_2_4.py (1440è¡Œ)

**å¿…é ˆå…¥åŠ›å¼•æ•°**:
```python
--bundle: str (required)  # train_daily_model.pyãŒä¿å­˜ã—ãŸjoblibãƒ•ã‚¡ã‚¤ãƒ«
--reserve-csv: str (optional)  # äºˆæ¸¬æœŸé–“ã®äºˆç´„ãƒ‡ãƒ¼ã‚¿
--reserve-date-col: str = "äºˆç´„æ—¥"
--future-days: int (optional)  # äºˆæ¸¬æ—¥æ•°
--start-date: str (optional)  # äºˆæ¸¬é–‹å§‹æ—¥
--end-date: str (optional)  # äºˆæ¸¬çµ‚äº†æ—¥
--out-csv: str (required)  # å‡ºåŠ›CSV
```

**å‡ºåŠ›å½¢å¼**:
```csv
date, sum_items_pred, p50, p90, mean_pred, total_pred
2025-12-19, 79.5, 82.3, 95.1, 79.5, 79.5
```

**åˆ—ã®èª¬æ˜**:
- `date`: äºˆæ¸¬æ—¥
- `p50`: Stage2ãƒ¢ãƒ‡ãƒ«ã®ä¸­å¤®å€¤äºˆæ¸¬ï¼ˆä¸»è¦æŒ‡æ¨™ï¼‰
- `p90`: Stage2ãƒ¢ãƒ‡ãƒ«ã®90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«äºˆæ¸¬
- `mean_pred`: Stage2ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡äºˆæ¸¬
- `total_pred`: Stage1å“ç›®åˆè¨ˆäºˆæ¸¬ï¼ˆå‚è€ƒï¼‰

---

### 3. ç¾åœ¨ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆCSVæ–¹å¼ï¼‰

**ãƒ•ãƒ­ãƒ¼**:
```
RunDailyTplus1ForecastWithTrainingUseCase
  â”œâ”€ 1. InboundActualsExporter.export_item_level_actuals()
  â”‚     â†’ stg.shogun_final_receive â†’ raw.csv (å“ç›®åˆ¥ã€365æ—¥)
  â”‚
  â”œâ”€ 2. ReserveExporter.export_daily_reserve()
  â”‚     â†’ mart.v_reserve_daily_for_forecast â†’ reserve.csv (67æ—¥)
  â”‚
  â”œâ”€ 3. subprocess.run(retrain_and_eval.py)
  â”‚     â”œâ”€ train_daily_model.py --raw-csv raw.csv --reserve-csv reserve.csv
  â”‚     â”‚   â†’ out/bundle.joblib
  â”‚     â”‚
  â”‚     â””â”€ daily_tplus1_predict.py --bundle out/bundle.joblib
  â”‚           â†’ tplus1_pred.csv
  â”‚
  â””â”€ 4. pd.read_csv(tplus1_pred.csv) â†’ p50å–å¾— â†’ DBä¿å­˜
```

**å•é¡Œç‚¹**:
- CSVä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆãƒ»èª­ã¿è¾¼ã¿ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
- ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡æ¶ˆè²»
- æ–‡å­—ã‚³ãƒ¼ãƒ‰ãƒˆãƒ©ãƒ–ãƒ«ï¼ˆUTF-8/Shift-JISï¼‰
- ãƒ‡ãƒãƒƒã‚°æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚³ã‚¹ãƒˆ

---

### 4. ç›®æ¨™ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆDBç›´çµæ–¹å¼ï¼‰

**ãƒ•ãƒ­ãƒ¼**:
```
RunDailyTplus1ForecastWithTrainingUseCase
  â”œâ”€ 1. InboundActualsExporter.export_item_level_actuals()
  â”‚     â†’ stg.shogun_final_receive â†’ pandas DataFrame (å“ç›®åˆ¥ã€365æ—¥)
  â”‚
  â”œâ”€ 2. ReserveExporter.export_daily_reserve()
  â”‚     â†’ mart.v_reserve_daily_for_forecast â†’ pandas DataFrame (67æ—¥)
  â”‚
  â”œâ”€ 3. subprocess.run(retrain_and_eval.py --use-db)
  â”‚     â”œâ”€ train_daily_model.py --use-db
  â”‚     â”‚   â”œâ”€ load_raw_from_db() â†’ DataFrame (å†…éƒ¨ã§DBæ¥ç¶š)
  â”‚     â”‚   â”œâ”€ load_reserve_from_db() â†’ DataFrame
  â”‚     â”‚   â””â”€ æ—¢å­˜å‡¦ç†ï¼ˆç‰¹å¾´é‡ç”Ÿæˆâ†’å­¦ç¿’â†’ä¿å­˜ï¼‰
  â”‚     â”‚       â†’ out/bundle.joblib
  â”‚     â”‚
  â”‚     â””â”€ daily_tplus1_predict.py --bundle --use-db
  â”‚           â”œâ”€ load_reserve_from_db() â†’ DataFrame
  â”‚           â””â”€ äºˆæ¸¬ â†’ tplus1_pred.csv (ä¸€æ™‚çš„ã«ç”Ÿæˆã€ã¾ãŸã¯JSONè¿”å´)
  â”‚
  â””â”€ 4. äºˆæ¸¬çµæœã‚’ãƒ¡ãƒ¢ãƒªçµŒç”±ã§DBä¿å­˜ï¼ˆCSVã¯ä½¿ã‚ãªã„ï¼‰
```

**åˆ©ç‚¹**:
- CSV I/Oã®å‰Šé™¤ï¼ˆé€Ÿåº¦å‘ä¸Šï¼‰
- ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡å‰Šæ¸›
- æ–‡å­—ã‚³ãƒ¼ãƒ‰ãƒˆãƒ©ãƒ–ãƒ«è§£æ¶ˆ
- ãƒ‡ãƒ¼ã‚¿å–å¾—ç¯„å›²ã®ä¸€å…ƒç®¡ç†ï¼ˆUseCaseã§çµ±ä¸€ï¼‰

---

## ğŸ¯ å®Ÿè£…æ–¹é‡ï¼šãƒ™ã‚¤ãƒ“ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‚’å£Šã•ãªã„ï¼‰

### åŸå‰‡
1. **æ—¢å­˜CLIå¼•æ•°ã¯å‰Šé™¤ã—ãªã„** â†’ è¿½åŠ ã®ã¿
2. **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å¾“æ¥é€šã‚Š** â†’ --use-db ã¯æ˜ç¤ºçš„ã«æŒ‡å®š
3. **ç‰¹å¾´é‡ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯ä¸å¤‰** â†’ å…¥åŠ›DataFrameæ•´å½¢ã§å¯¾å¿œ
4. **æ®µéšçš„ç§»è¡Œ** â†’ CSVæ–¹å¼ã¨å…±å­˜æœŸé–“ã‚’è¨­ã‘ã‚‹

### Phase 1: train_daily_model.py ã¸ã® --use-db è¿½åŠ 

**å¤‰æ›´ç®‡æ‰€**:
```python
# å¼•æ•°è¿½åŠ 
ap.add_argument("--use-db", action="store_true",
                help="DBã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆCSVã‚’ä½¿ã‚ãªã„ï¼‰")
ap.add_argument("--db-connection-string", type=str, default=None,
                help="PostgreSQLæ¥ç¶šæ–‡å­—åˆ—ï¼ˆ--use-dbæ™‚ã«æŒ‡å®šï¼‰")

# main()å†…ã§åˆ†å²
if args.use_db:
    # DBã‹ã‚‰ç›´æ¥å–å¾—
    raw_df = load_raw_from_db(
        connection_string=args.db_connection_string,
        start_date=...,  # â† å¼•æ•°ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨ˆç®—
        end_date=...,
        date_col=args.raw_date_col,
        item_col=args.raw_item_col,
        weight_col=args.raw_weight_col,
    )
    
    if args.reserve_csv or args.use_db:
        reserve_df = load_reserve_from_db(
            connection_string=args.db_connection_string,
            start_date=...,
            end_date=...,
            date_col=args.reserve_date_col,
            count_col=args.reserve_count_col,
            fixed_col=args.reserve_fixed_col,
        )
else:
    # å¾“æ¥é€šã‚ŠCSVã‹ã‚‰èª­ã¿è¾¼ã¿
    raw_df = pd.read_csv(args.raw_csv, ...)
    if args.reserve_csv:
        reserve_df = pd.read_csv(args.reserve_csv, ...)
```

**æ–°è¦é–¢æ•°**:
```python
def load_raw_from_db(
    connection_string: str,
    start_date: date,
    end_date: date,
    date_col: str,
    item_col: str,
    weight_col: str,
) -> pd.DataFrame:
    """
    stg.shogun_final_receive ã‹ã‚‰å“ç›®åˆ¥å®Ÿç¸¾ã‚’å–å¾—
    
    Returns:
        DataFrame with columns: [date_col, item_col, weight_col]
        weight_col ã¯ ton å˜ä½
    """
    import sqlalchemy
    from sqlalchemy import text
    
    engine = sqlalchemy.create_engine(connection_string)
    
    sql = text("""
        SELECT 
            slip_date AS date_col,
            item_name AS item_col,
            net_weight / 1000.0 AS weight_col
        FROM stg.shogun_final_receive
        WHERE slip_date >= :start_date 
          AND slip_date <= :end_date
          AND is_deleted = false
          AND net_weight IS NOT NULL
          AND item_name IS NOT NULL
        ORDER BY slip_date, item_name
    """)
    
    with engine.connect() as conn:
        result = conn.execute(sql, {
            "start_date": start_date,
            "end_date": end_date
        })
        rows = result.fetchall()
    
    df = pd.DataFrame(rows, columns=[date_col, item_col, weight_col])
    
    # æ—¥ä»˜å‹å¤‰æ›
    df[date_col] = pd.to_datetime(df[date_col]).dt.normalize()
    
    return df


def load_reserve_from_db(
    connection_string: str,
    start_date: date,
    end_date: date,
    date_col: str,
    count_col: str,
    fixed_col: str,
) -> pd.DataFrame:
    """
    mart.v_reserve_daily_for_forecast ã‹ã‚‰äºˆç´„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Returns:
        DataFrame with columns: [date_col, count_col, fixed_col]
    """
    import sqlalchemy
    from sqlalchemy import text
    
    engine = sqlalchemy.create_engine(connection_string)
    
    sql = text("""
        SELECT 
            date AS date_col,
            reserve_trucks AS count_col,
            reserve_fixed_trucks AS fixed_col
        FROM mart.v_reserve_daily_for_forecast
        WHERE date >= :start_date 
          AND date <= :end_date
        ORDER BY date
    """)
    
    with engine.connect() as conn:
        result = conn.execute(sql, {
            "start_date": start_date,
            "end_date": end_date
        })
        rows = result.fetchall()
    
    df = pd.DataFrame(rows, columns=[date_col, count_col, fixed_col])
    
    # æ—¥ä»˜å‹å¤‰æ›
    df[date_col] = pd.to_datetime(df[date_col]).dt.normalize()
    
    return df
```

**æ³¨æ„**:
- æ—¢å­˜ã® `pd.read_csv()` ã®å¾Œç¶šå‡¦ç†ï¼ˆæ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã€å‹å¤‰æ›ç­‰ï¼‰ã‚’ãã®ã¾ã¾ä½¿ãˆã‚‹ã‚ˆã†ã«DataFrameã‚’æ•´å½¢
- åˆ—åã¯å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸæ—¥æœ¬èªåˆ—åã«åˆã‚ã›ã‚‹
- tonå˜ä½ã¸ã®å¤‰æ›ã¯å–å¾—æ™‚ã«å®Ÿæ–½ï¼ˆ`/ 1000.0`ï¼‰

---

### Phase 2: serve_predict_model_v4_2_4.py ã¸ã® --use-db è¿½åŠ 

**å¤‰æ›´ç®‡æ‰€**:
```python
# å¼•æ•°è¿½åŠ 
ap.add_argument("--use-db", action="store_true",
                help="DBã‹ã‚‰äºˆç´„ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥å–å¾—")
ap.add_argument("--db-connection-string", type=str, default=None,
                help="PostgreSQLæ¥ç¶šæ–‡å­—åˆ—ï¼ˆ--use-dbæ™‚ï¼‰")

# main()å†…ã§åˆ†å²
if args.use_db and args.db_connection_string:
    reserve_df = load_reserve_from_db(
        connection_string=args.db_connection_string,
        start_date=...,  # äºˆæ¸¬é–‹å§‹æ—¥
        end_date=...,  # äºˆæ¸¬çµ‚äº†æ—¥
        date_col=args.reserve_date_col,
        count_col=...,
        fixed_col=...,
    )
elif args.reserve_csv:
    reserve_df = pd.read_csv(args.reserve_csv, ...)
else:
    reserve_df = None
```

**æ³¨**:
- `load_reserve_from_db()` ã¯ train_daily_model.py ã¨å…±é€šåŒ–ï¼ˆå…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«æŠ½å‡ºï¼‰

---

### Phase 3: retrain_and_eval.py / daily_tplus1_predict.py ã¸ã®å¼•æ•°è»¢é€

**retrain_and_eval.py**:
```python
# å¼•æ•°è¿½åŠ 
ap.add_argument("--use-db", action="store_true")
ap.add_argument("--db-connection-string", type=str, default=None)

# train_daily_model.py ã¸ã®è»¢é€
if args.use_db:
    cmd_train.extend([
        "--use-db",
        "--db-connection-string", args.db_connection_string,
    ])

# daily_tplus1_predict.py ã¸ã®è»¢é€
if args.use_db:
    cmd_pred.extend([
        "--use-db",
        "--db-connection-string", args.db_connection_string,
    ])
```

**daily_tplus1_predict.py**:
```python
# å¼•æ•°è¿½åŠ 
ap.add_argument("--use-db", action="store_true")
ap.add_argument("--db-connection-string", type=str, default=None)

# serve_predict_model_v4_2_4.py ã¸ã®è»¢é€
if args.use_db:
    cmd.extend([
        "--use-db",
        "--db-connection-string", args.db_connection_string,
    ])
```

---

### Phase 4: UseCase ã®å¤‰æ›´ï¼ˆCSVå»ƒæ­¢ï¼‰

**RunDailyTplus1ForecastWithTrainingUseCase**:

**å¤‰æ›´å‰**:
```python
# CSVä¿å­˜
raw_csv_path = workspace / "raw.csv"
actuals_df.to_csv(raw_csv_path, index=False, encoding="utf-8")

reserve_csv_path = workspace / "reserve.csv"
reserve_df.to_csv(reserve_csv_path, index=False, encoding="utf-8")

# ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
cmd = [
    "python3", str(self._retrain_script_path),
    "--quick",
    "--raw-csv", str(raw_csv_path),
    "--reserve-csv", str(reserve_csv_path),
    ...
]
```

**å¤‰æ›´å¾Œ**:
```python
# CSVä¿å­˜ã¯ä¸è¦ï¼ˆ--use-dbãƒ¢ãƒ¼ãƒ‰ï¼‰

# DBæ¥ç¶šæ–‡å­—åˆ—ã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‹ã‚‰å–å¾—
db_url = os.getenv("DATABASE_URL") or self._db_url

# ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
cmd = [
    "python3", str(self._retrain_script_path),
    "--quick",
    "--use-db",
    "--db-connection-string", db_url,
    "--start-date", str(target_date),
    "--end-date", str(target_date),
    "--out-dir", str(out_dir),
    "--pred-out-csv", str(pred_out_csv),
    ...
]
```

**æ—¥ä»˜ç¯„å›²ã®çµ±ä¸€**:
```python
# å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ç¯„å›²
actuals_start = target_date - timedelta(days=360)
actuals_end = target_date - timedelta(days=1)

# äºˆç´„ãƒ‡ãƒ¼ã‚¿ç¯„å›²
reserve_start = target_date - timedelta(days=360)
reserve_end = target_date

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«æ¸¡ã™ï¼ˆå†…éƒ¨ã§DBå–å¾—æ™‚ã«ä½¿ç”¨ï¼‰
cmd.extend([
    "--actuals-start-date", str(actuals_start),
    "--actuals-end-date", str(actuals_end),
    "--reserve-start-date", str(reserve_start),
    "--reserve-end-date", str(reserve_end),
])
```

**æ³¨æ„**:
- ç¾çŠ¶ã§ã¯ retrain_and_eval.py ã¯æ—¥ä»˜ç¯„å›²ã‚’å—ã‘å–ã£ã¦ã„ãªã„ãŸã‚ã€è¿½åŠ å®Ÿè£…ãŒå¿…è¦
- ã¾ãŸã¯ã€UseCaseå´ã§DataFrameã‚’pickleåŒ–ã—ã¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«æ¸¡ã™æ–¹å¼ã‚‚æ¤œè¨ï¼ˆéæ¨å¥¨ï¼‰

---

### Phase 5: å—å…¥ãƒˆãƒ³â†’æ•°é‡å¤‰æ›ï¼ˆä»•æ§˜ç¢ºèªå¿…è¦ï¼‰

**ç¾çŠ¶**:
- `æ­£å‘³é‡é‡` åˆ—ã¯ ton å˜ä½
- train_daily_model.py ã¯ã“ã®å€¤ã‚’ç›´æ¥ä½¿ç”¨
- å¤‰æ›ã¯ä¸è¦ï¼Ÿ

**ã‚‚ã—å¤‰æ›ãŒå¿…è¦ãªå ´åˆ**:
```python
def ton_to_quantity(ton: float) -> float:
    """
    å—å…¥ãƒˆãƒ³æ•°ã‚’ã€Œæ•°é‡ã€å˜ä½ã«å¤‰æ›
    
    ä»•æ§˜:
    - 1å°ã‚ãŸã‚Šå¹³å‡é‡é‡ã‚’åŸºæº–ã«æ›ç®—
    - ã¾ãŸã¯å›ºå®šä¿‚æ•°ï¼ˆä¾‹: 1 ton = 10 quantityï¼‰
    
    TODO: æ­£ç¢ºãªå¤‰æ›ä»•æ§˜ã‚’ docs ã«è¨˜è¼‰
    """
    # æš«å®šå®Ÿè£…
    CONVERSION_FACTOR = 1.0  # 1:1å¤‰æ›ï¼ˆä»•æ§˜ç¢ºèªå¾…ã¡ï¼‰
    return ton * CONVERSION_FACTOR
```

**é©ç”¨ç®‡æ‰€**:
- `load_raw_from_db()` ã®è¿”å´DataFrame
- `æ­£å‘³é‡é‡` åˆ—ã‚’å¤‰æ›å¾Œã®å€¤ã«ç½®ãæ›ãˆ

**ãŸã ã—**:
- å­¦ç¿’å´ãŒ ton å˜ä½ã‚’æƒ³å®šã—ã¦ã„ã‚‹ãªã‚‰å¤‰æ›ä¸è¦
- å¤‰æ›ãŒå¿…è¦ã‹ã©ã†ã‹ã¯å­¦ç¿’çµæœã®å˜ä½ã¨ä¸€è‡´ã•ã›ã‚‹å¿…è¦ã‚ã‚Š
- **çµè«–**: ç¾çŠ¶ã®å®Ÿè£…ã§ã¯ ton å˜ä½ã®ã¾ã¾ä½¿ç”¨ã—ã¦ãŠã‚Šã€å¤‰æ›ã¯ä¸è¦ã¨åˆ¤æ–­

---

## ğŸ“ å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Phase 1: ã‚¹ã‚¯ãƒªãƒ—ãƒˆå±¤ã¸ã® --use-db è¿½åŠ 
- [ ] `scripts/train_daily_model.py` ã« `load_raw_from_db()` è¿½åŠ 
- [ ] `scripts/train_daily_model.py` ã« `load_reserve_from_db()` è¿½åŠ 
- [ ] `scripts/train_daily_model.py` ã« `--use-db` å¼•æ•°è¿½åŠ 
- [ ] `scripts/serve_predict_model_v4_2_4.py` ã« `load_reserve_from_db()` è¿½åŠ 
- [ ] `scripts/serve_predict_model_v4_2_4.py` ã« `--use-db` å¼•æ•°è¿½åŠ 

### Phase 2: å…±é€šåŒ–
- [ ] `scripts/db_utils.py` ä½œæˆï¼ˆload_raw_from_db, load_reserve_from_db ã‚’å…±é€šåŒ–ï¼‰
- [ ] train_daily_model.py, serve_predict_model_v4_2_4.py ã‹ã‚‰ import

### Phase 3: ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®å¼•æ•°è»¢é€
- [ ] `scripts/retrain_and_eval.py` ã« `--use-db` è¿½åŠ ãƒ»è»¢é€
- [ ] `scripts/daily_tplus1_predict.py` ã« `--use-db` è¿½åŠ ãƒ»è»¢é€

### Phase 4: UseCaseå±¤ã®å¤‰æ›´
- [ ] `RunDailyTplus1ForecastWithTrainingUseCase` ã‹ã‚‰ CSVä¿å­˜ã‚’å‰Šé™¤
- [ ] `--use-db` ãƒ•ãƒ©ã‚°ã¨ DBæ¥ç¶šæ–‡å­—åˆ—ã‚’ã‚³ãƒãƒ³ãƒ‰ã«è¿½åŠ 
- [ ] æ—¥ä»˜ç¯„å›²ã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå´ã§å—ã‘å–ã‚Šï¼‰

### Phase 5: ãƒ†ã‚¹ãƒˆã¨ç›£æŸ»
- [ ] ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§1ã‚¸ãƒ§ãƒ–å®Ÿè¡Œï¼ˆ--use-db ã‚ã‚Šï¼‰
- [ ] workspaceç¢ºèªï¼ˆCSVç”Ÿæˆã•ã‚Œã¦ã„ãªã„ã“ã¨ï¼‰
- [ ] DBä¿å­˜ç¢ºèªï¼ˆäºˆæ¸¬å€¤ãŒæ­£å¸¸ç¯„å›²ï¼‰
- [ ] audit.md ä½œæˆï¼ˆè¨¼æ‹ ä»˜ãæ¤œè¨¼ï¼‰

---

## ğŸ” æ¤œè¨¼é …ç›®ï¼ˆç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆã«è¨˜è¼‰ï¼‰

### 1. ãƒ‡ãƒ¼ã‚¿å–å¾—ç¯„å›²ã®æ¤œè¨¼

**SQLå®Ÿè¡Œä¾‹**:
```sql
-- å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ç¯„å›²
SELECT 
    MIN(slip_date) AS min_date,
    MAX(slip_date) AS max_date,
    COUNT(*) AS row_count,
    COUNT(DISTINCT item_name) AS item_count,
    AVG(net_weight / 1000.0) AS avg_weight_ton
FROM stg.shogun_final_receive
WHERE slip_date >= CURRENT_DATE - 360
  AND slip_date <= CURRENT_DATE - 1
  AND is_deleted = false
  AND net_weight IS NOT NULL
  AND item_name IS NOT NULL;

-- äºˆç´„ãƒ‡ãƒ¼ã‚¿ç¯„å›²
SELECT 
    MIN(date) AS min_date,
    MAX(date) AS max_date,
    COUNT(*) AS row_count,
    AVG(reserve_trucks) AS avg_trucks
FROM mart.v_reserve_daily_for_forecast
WHERE date >= CURRENT_DATE - 360
  AND date <= CURRENT_DATE;
```

**æœŸå¾…å€¤**:
- å®Ÿç¸¾: `[target_date - 360, target_date - 1]`
- äºˆç´„: `[target_date - 360, target_date]`

### 2. åˆ—åå¤‰æ›ã®æ¤œè¨¼

**ã‚³ãƒ¼ãƒ‰ç¢ºèªç®‡æ‰€**:
- `load_raw_from_db()` ã®è¿”å´DataFrameåˆ—å
- `load_reserve_from_db()` ã®è¿”å´DataFrameåˆ—å

**æœŸå¾…å€¤**:
- å®Ÿç¸¾: `[ä¼ç¥¨æ—¥ä»˜, å“å, æ­£å‘³é‡é‡]`
- äºˆç´„: `[äºˆç´„æ—¥, å°æ•°, å›ºå®šå®¢]`

### 3. å­¦ç¿’ãƒ»äºˆæ¸¬ã®å®Ÿè¡Œç¢ºèª

**ãƒ­ã‚°ç¢ºèª**:
```bash
docker compose logs inbound_forecast_worker | grep -E "Starting|completed|p50="
```

**æœŸå¾…å‡ºåŠ›**:
```
Starting daily t+1 forecast with training
Training completed successfully
Prediction result: p50=75.3 ton
Saved prediction result to DB
```

### 4. DBä¿å­˜ã®æ¤œè¨¼

**SQLå®Ÿè¡Œä¾‹**:
```sql
SELECT 
    target_date,
    p50,
    p10,
    p90,
    unit,
    input_snapshot->>'model_version' AS model_version,
    input_snapshot->>'training_mode' AS training_mode,
    generated_at
FROM forecast.daily_forecast_results
WHERE target_date = CURRENT_DATE
ORDER BY generated_at DESC
LIMIT 1;
```

**æœŸå¾…å€¤**:
- `p50` ãŒ 20ï½100 ton ç¯„å›²ï¼ˆç•°å¸¸å€¤ã§ãªã„ï¼‰
- `unit = 'ton'`
- `model_version = 'final_fast_balanced'`
- `training_mode = 'quick'`

### 5. CSVå»ƒæ­¢ã®ç¢ºèª

**ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ç¢ºèª**:
```bash
ls -la /tmp/forecast_jobs/{job_id}/
# æœŸå¾…: raw.csv, reserve.csv ãŒå­˜åœ¨ã—ãªã„
# å­˜åœ¨: out/, tplus1_pred.csv (ä¸€æ™‚çš„), run.log
```

**æ³¨**: tplus1_pred.csv ã¯ serve_predict_model_v4_2_4.py ã®å‡ºåŠ›ã¨ã—ã¦æ®‹ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼ˆå¾Œç¶šãƒ•ã‚§ãƒ¼ã‚ºã§å‰Šé™¤æ¤œè¨ï¼‰

---

## ğŸš€ å®Ÿè£…å„ªå…ˆé †ä½

### P0ï¼ˆå¿…é ˆï¼‰
1. train_daily_model.py ã¸ã® --use-db å®Ÿè£…
2. serve_predict_model_v4_2_4.py ã¸ã® --use-db å®Ÿè£…
3. retrain_and_eval.py / daily_tplus1_predict.py ã¸ã®å¼•æ•°è»¢é€
4. UseCase ã‹ã‚‰CSVä¿å­˜å‰Šé™¤ãƒ»--use-db æŒ‡å®š

### P1ï¼ˆæ¨å¥¨ï¼‰
5. db_utils.py ã¸ã®å…±é€šåŒ–
6. æ—¥ä»˜ç¯„å›²å¼•æ•°ã®æ˜ç¤ºåŒ–
7. ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ

### P2ï¼ˆå°†æ¥ï¼‰
8. tplus1_pred.csv ã‚‚å»ƒæ­¢ï¼ˆJSONçµŒç”±ã§çµæœã‚’è¿”ã™ï¼‰
9. ç’°å¢ƒå¤‰æ•°ã§ã®æ¥ç¶šæ–‡å­—åˆ—ç®¡ç†ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰
10. çµ±åˆãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ–

---

## âš ï¸ ãƒªã‚¹ã‚¯ã¨å¯¾ç­–

### ãƒªã‚¹ã‚¯1: DBæ¥ç¶šã‚¨ãƒ©ãƒ¼
- **å¯¾ç­–**: æ¥ç¶šæ–‡å­—åˆ—ã®æ¤œè¨¼ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: CSVæ–¹å¼ã«æˆ»ã›ã‚‹ã‚ˆã† --use-db ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³

### ãƒªã‚¹ã‚¯2: æ—¥ä»˜ç¯„å›²ã®ãƒŸã‚¹ãƒãƒƒãƒ
- **å¯¾ç­–**: UseCase ã§çµ±ä¸€çš„ã«è¨ˆç®—ã—ã€å¼•æ•°ã¨ã—ã¦æ¸¡ã™
- **æ¤œè¨¼**: ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆã§å®Ÿéš›ã®SQLçµæœã‚’ç¢ºèª

### ãƒªã‚¹ã‚¯3: æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ç ´å£Š
- **å¯¾ç­–**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å¾“æ¥é€šã‚Šï¼ˆ--use-db ã¯æ˜ç¤ºæŒ‡å®šï¼‰
- **ãƒ†ã‚¹ãƒˆ**: CSVæ–¹å¼ã§ã‚‚å¼•ãç¶šãå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª

### ãƒªã‚¹ã‚¯4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹
- **å¯¾ç­–**: DBå´ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèªï¼ˆddate, slip_dateï¼‰
- **æ¸¬å®š**: å®Ÿè¡Œæ™‚é–“ã‚’ãƒ­ã‚°ã«è¨˜éŒ²

---

## ğŸ“š å‚è€ƒæƒ…å ±

### æ—¢å­˜å®Ÿè£…
- `app/backend/inbound_forecast_worker/app/adapters/forecast/inbound_actuals_exporter.py` (å“ç›®åˆ¥å®Ÿç¸¾ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼)
- `app/backend/inbound_forecast_worker/app/adapters/forecast/reserve_exporter.py` (äºˆç´„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼)
- `app/backend/inbound_forecast_worker/app/application/run_daily_tplus1_forecast_with_training.py` (UseCase)

### ã‚¹ã‚­ãƒ¼ãƒ
- `data/postgres/` (ãƒ­ãƒ¼ã‚«ãƒ«DB)
- `docs/database/` (ã‚¹ã‚­ãƒ¼ãƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ)

### å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `app/backend/inbound_forecast_worker/scripts/train_daily_model.py` (å­¦ç¿’)
- `app/backend/inbound_forecast_worker/scripts/serve_predict_model_v4_2_4.py` (æ¨è«–)
- `app/backend/inbound_forecast_worker/scripts/retrain_and_eval.py` (ãƒ©ãƒƒãƒ‘ãƒ¼)

---

## âœ… æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. **Phase 1å®Ÿè£…**: train_daily_model.py ã« --use-db ã‚’è¿½åŠ 
2. **Phase 2å®Ÿè£…**: serve_predict_model_v4_2_4.py ã« --use-db ã‚’è¿½åŠ 
3. **Phase 3å®Ÿè£…**: ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®å¼•æ•°è»¢é€
4. **Phase 4å®Ÿè£…**: UseCase ã®å¤‰æ›´ï¼ˆCSVå»ƒæ­¢ï¼‰
5. **Phase 5æ¤œè¨¼**: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ

---

**ä½œæˆæ—¥**: 2025-12-18  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: èª¿æŸ»å®Œäº†ãƒ»å®Ÿè£…å¾…ã¡
